\documentclass[Crown, sagev]{sagej}

\usepackage{amssymb, amsmath, amsfonts}


\include{GrandMacros}
\newcommand{\cdf}{{c.d.f.} }
\newcommand{\pdf}{{p.d.f.} }
\newcommand{\ind}{\mathbb{I}}

\begin{document}

\title{Infectious disease prediction with kernel conditional density
estimation}

\author{Evan L. Ray\affilnum{1},
Krzysztof Sakrejda\affilnum{1},
Stephen A. Lauer\affilnum{1},
Michael Johansen\affilnum{2} and
Nicholas G. Reich\affilnum{1}}

\affiliation{\affilnum{1}Department of Biostatistics and Epidemiology,
University of Massachusetts, Amherst\\
\affilnum{2}CDC, Puerto Rico}

\corrauth{Evan Ray, UMass Address Here}

\email{elray@umass.edu}

\begin{abstract}
Abstract
\end{abstract}
\keywords{copula, infectious disease, kernel conditional density estimation,
prediction}

\maketitle

<<knitrGlobalSetup, echo = FALSE>>=
library(reshape2)

opts_chunk$set(cache = TRUE, autodep = TRUE)
#opts_chunk$set(cache = FALSE)
@

\section{Introduction}
\label{sec:Intro}

Accurate prediction of infectious disease incidence is important for public
health officials planning disease prevention and control measures such as vector
control and increased use of personal protective equipment by medical
personnel during periods of high disease incidence (cite ***).  Several
quantities have emerged as being of particular utility in making these planning decisions (cite ***); in this article we focus
on measures of weekly incidence, the timing of the season peak, and incidence
in the peak week.  Predictive distributions for these quantities are preferred
to point predictions because they communicate uncertainty in the predictions and
give decision makers more information in cases where the predictive distribution
is skewed or has multiple modes.
In this work, we employ a non-parametric approach referred to as kernel
conditional density estimation (KCDE) to obtain separate predictive
distributions for disease incidence in each week of the season, and then combine
those marginal distributions using copulas to obtain joint predictive distributions for the
trajectory of incidence over the course of multiple weeks.  Predictive
distributions relating to the timing of and incidence at the peak week can be
obtained from this joint predictive distribution for the trajectory of
disease incidence.
In addition to the novel application of these methods to predicting disease incidence, our contributions include the use of a periodic kernel specification to capture
seasonality in disease incidence and a method for obtaining multivariate
kernel functions that handle discrete data while allowing for a fully
parameterized bandwidth matrix.

KCDE is a method for estimating the conditional distribution of
a random vector $\bY$ given observations of another vector $\bX$.  In our work,
$\bY$ is a measure of disease incidence at some future date (the prediction
target) and $\bX$ is a vector of predictive variables that we condition
on in order to make our prediction.  In our example applications,
$\bX$ includes observations of incidence over several recent time
points and variables indicating the time of year at which we are
making a prediction; in general, it would be possible to include other
variables such as weather covariates.

KCDE has not previously been applied to obtain predictive distributions in the
context of infectious disease, but it has been successfully used for prediction in other settings such as survival time of
lung cancer patients\cite{hall2004crossvalidationKCDE}, female labor force
participation\cite{hall2004crossvalidationKCDE}, bond yields and value at risk
in financial markets\cite{fan2004crossvalidationKCDE}, and wind
power\cite{jeon2012KCDEWindPower} among others.  Although KCDE has not
previously been applied to predicting infectious disease, closely related methods for obtaining point predictions have been employed for
diseases such as measles\cite{sugihara1990nonlinearForecasting} and
influenza\cite{viboud2003predictionInfluenzaMoA}.  In the infectious disease
literature these methods have been referred to as state space reconstruction and
the method of analogues, but they amount to an application of nearest neighbors
regression methods.  The point prediction obtained from nearest neighbors
regression is equal to the expected value of the predictive distribution
obtained from KCDE if a particular kernel function is used in the formulation of
KCDE\cite{HastieTibshiraniESL}.  However, KCDE offers the advantage of providing
a complete predictive distribution rather than only a point prediction.  Methods
similar to those we explore in this article can also be formulated in the
Bayesian framework.  One example along these lines is Zhou et
{al.}\cite{zhou2015DirichletProcessCopulaAmphibianDiseaseArrival}, who model the
time to arrival of a disease in amphibian populations using Dirichlet
processes and copulas.

There is also a long history of using other modeling approaches such as
compartmental models for infectious disease prediction.  A full discussion of
those methods is beyond the scope of this article; see *** for a recent review. 
KCDE is distinguished from these alternative approaches in that it makes
minimal assumptions about the data generating process.  This can be
either an advantage and a disadvantage of KCDE.
In general, flexible non-parametric methods such as KCDE exhibit low
bias but high variance.  If they are correctly specified, parametric
approaches may achieve reduced variance without introducing bias.  
On the other hand, because non-parametric approaches such as KCDE make fewer
assumptions they may outperform incorrectly specified parametric models.  An
evaluation of the benefits of an approach such as KCDE is therefore dependent on
the particular characteristics of the system being modeled, the data that are
available, and the quality of the more structured parametric models that are
considered as alternatives.  We will return to this point in our conclusions.

\lboxit{Need to find a review of prediction methods for infectious disease.}

%There is an extensive literature on KCDE, focusing mainly on estimation of
%continuous conditional densities.  Here we offer a brief overview emphasizing
%the case with mixed continuous and discrete variables; Li and
%Racine\cite{li2007nonparametricEconometrics} offer a detailed discussion of this
%case.

To our knowledge, all previous authors using kernel methods to estimate
multivariate densities involving discrete variables have employed a kernel
function that is a product of univariate kernel functions \cite{aitchison1976multivariateBinaryKernel,
wang1981SmoothEstDiscreteDistn,
li2003nonparametricEstDistnsCategoricalContinuous,
ouyang2006crossvalidationEstDistnCategorical}.
A variety of functional forms have been proposed for this purpose, ranging from
blah to blah to blah \cite{aitchison1976multivariateBinaryKernel,
wang1981SmoothEstDiscreteDistn, li2008nonparametricConditionalCDFQuantile,
li2003nonparametricEstDistnsCategoricalContinuous,
ouyang2006crossvalidationEstDistnCategorical}.

%\cite{wang1981SmoothEstDiscreteDistn},
%\cite{li2008nonparametricConditionalCDFQuantile}\cite{li2003nonparametricEstDistnsCategoricalContinuous},
% \cite{ouyang2006crossvalidationEstDistnCategorical}].

Using a product kernel simplifies the mathemetical formulation of the kernel
function when discrete variables are present, but has the
effect of forcing the kernel function to be orientied in line with the
coordinate axes.  In settings with only continuous variables, asymptotic
analysis and experience with applications have shown that using a multivariate
kernel function with a bandwidth parameterization that allows for
other orientations can result in improved density estimates
in many cases (cite ***).  We introduce an approach to allowing for discrete
kernels with orientation by discretizing an underlying continuous kernel
function.
%One possibility for introducing orientation
%to the kernel function is to use a fully parameterized
%bandwidth matrix, allowing the kernel to be oriented in any direction.  Another
%common alternative is to fix the orientation to be in the directions of the
%eigenvectors of the sample covariance matrix.

%Estimation.  Two main strategies:  cross validation and rule-based.  Targets
% for optimization in cross-validation.  For
%estimating joint densities without
%conditioning, \cite{hart1990bandwidthEstDependentData} have shown that with
%dependent data, but small gains in the mean integrated square error of the
%density estimate relative to the true conditional density can be achieved by
%leaving out observations adjacent to the time point whose density is being
%estimated.

A limitation of kernel-based density estimation methods is that they may not
scale well with the dimension of the vector whose distribution is being
estimated.  This is particularly relevant in our application, where it is
desired to obtain joint predictive distributions for disease incidence over the
course of many weeks.  Copulas present one strategy for estimating the joint
distribution of moderate to high dimensional random vectors, and work by
specifying a relatively simple parametric model for the dependence relations
among those variables.  Specifically, we model the joint distribution of $Y_1,
\ldots, Y_D$ by $F_{Y_1, \ldots, Y_D}(y_1, \ldots, y_D) = c(F_{Y_1}(y_1),
\ldots, F_{Y_D}(y_D) ; \bxi)$.  Here $c: [0,1]^D \rightarrow [0,1]$ is
the copula function depending on parameters $\bxi$ and mapping the vector
of marginal {c.d.f.} values to the joint {c.d.f.} value.

It would be possible to handle
this task using just the formulation of KCDE we discussed above, but a
direct application of this approach has some limitations.  First, the
performance of kernel-based density estimation methods scales poorly with the
dimension of the random vector whose density is being estimated (cite ***). 
Second, we have found that different information is available in the data at
different prediction horizons.  For example, we will demonstrate in our
applications below that recently observed incidence is important for
making short-term predictions, but terms capturing seasonality are more
important for making long-term predictions.

%We make several contributions in this article.  First, we apply KCDE to
%prediction of infectious disease (specifically, Dengue fever and Influenza), a
%novel application of the method which gives rise to several challenges and
%opportunities.  Among these challenges is the fact that infectious disease
%incidence can be quite noisy, with a lot of variation around a longer term
%trend; we will illustrate this in two real data sets in Section ***.  As we will
%see, this noise can cause difficulty for the method when applied to prediction
%of future incidence directly from recent observations of incidence.  Our
%solution is to introduce an initial low-pass filtering step on the observed
%incidence counts that are used as inputs to the predictions.

%Another challenge is in capturing seasonality in disease incidence.  In order
%to address this, we consider the use of periodic functions of the observation
%time as conditioning variables.  Effectively, this means that we can base our
%predictive density on previous observations that have been recorded at the time
%of year we are interested in.

%A third challenge is that for some applications, observations of disease
%incidence may take the form of discrete counts (i.e., the number of new cases
%in the last week).  If these incidence counts span a large range, it may be
%reasonable to approximate their predictive distribution with a continuous
%density function.  However, in our data for Dengue fever, the number of cases
%often falls within a limited range so that this continuous approximation is not
%reasonable.  We address this by discretizing an underlying continuous density
%function.  To our knowledge, this approach is novel in the KCDE literature.

The remainder of this article is organized as follows.  We:
 - describe how kernel density estimation with a non-diagonal bandwidth can be
 achieved using a partially discretized multivariate normal distribution for the
 kernel functions.

 - simulation study comparing product and non-product formulations for marginal
 and conditional density estimation
 
 - applications

\section{Method Description}
\label{sec:Methods}

In this Section, we give a detailed discussion of our methods.  Throughout, we
use the term density to refer to the Radon-Nikodym derivative of the
cumulative distribution function with respect to an appropriately defined measure.
In the case of random vectors where some
components are continuous random variables and other are discrete, we take this
measure to be a product of Lebesgue and counting measures for the corresponding
random variables.  We use bold letters to indicate column vectors or matrices;
capital letters are random variables and lower case letters are observations of those
random variables.

Suppose we observe a measure $z_t$ of disease incidence at each point in time $t
= 1, \ldots, T$.  At time $T$, our goal is to obtain a
predictive distribution $f(z_{T + 1}, \ldots, z_{T + H} | T, z_{1}, \ldots, z_{T})$
for the trajectory of disease incidence over a range of prediction horizons
from $1$ to $H$ weeks in the future given the time at which we are making the
predictions and observed incidence at previous times.

%We estimate this joint predictive density in two stages.  First, we
%use KCDE to obtain separate predictive distributions for each prediction
%horizon $h = 1, \ldots, H$: $f(z_{T + h} | z_{1}, \ldots, z_{T}, T)$.  Next, we
%use a copula to combine these 

Our model represents this density as follows:
\begin{align}
&f(z_{T + 1}, \ldots, z_{T + H} | T, z_{1}, \ldots, z_{T}) = f(z_{T + 1}, \ldots, z_{T + H} | T, z_{T - l} \text{, } l \in L) \label{eqn:ModelReducedLags} \\
&\qquad c^H\{f^{1}(z_{T + 1} | T+1, z_{T - l} \text{, } l \in L; \btheta^1), \ldots, f^{H}(z_{T + H} | T+H, z_{T - l} \text{, } l \in L; \btheta^H) ; \xi^H\}. \label{eqn:ModelKCDECopula}
\end{align}
Equation~\eqref{eqn:ModelReducedLags} formalizes an assumption that all of the
information about future incidence contained in the observed history of the
disease is captured by the incidence at a few recent time points, with lags in
the set $L$.  For now, we leave the set of lags included in $L$ unspecified; in
our applications to real disease data below, we will consider several candidate
sets of lags.  In Equation~\eqref{eqn:ModelKCDECopula}, each
$f^{h}(z_{T + h} | T + h, z_{T - l} \text{, } l \in L; \btheta^h)$
is a predictive density for one prediction horizon obtained through KCDE.  As
the notation suggests, we use a separate parameter vector $\btheta^h$ for each
prediction horizon, but the same set of lags $L$ at all horizons.  This may not
be the optimal setup, but exploration of other alternatives is beyond the scope
of this article.  The function $c^H(\cdot)$ is a copula used to tie these
marginal predictive densities together into a joint predictive density.

This model entails several sets of parameters: one set $\btheta^h$ for the KCDE fit at each
prediction horizon $h = 1, \ldots, H$, and another set $\bxi^H$ for the copula
used to obtain joint distributions for trajectories of length $H$.
Following Joe (cite ***), we pursue a two-stage estimation procedure for these
parameters.  In the first stage, we estimate the parameters for KCDE separately
for each horizon $h$.  Then in the second stage, we estimate the copula
parameters while holding the KCDE parameters fixed.  In the following
subsections, we describe our formulations of KCDE and the copula in more
detail and discuss parameter estimation for each of these stages in turn.

\subsection{KCDE for Predictive Densities at Individual Prediction Horizons}
\label{subsec:Methods:KCDE}

We now discuss the methods we use to obtain the predictive density
$f^{h}(z_{T + h} | T + h, z_{T - l} \text{, } l \in L; \btheta^h)$
for disease incidence at a particular horizon $h$.  In order to simplify the
notation we define two new variables.  $Y_t^{h} = Z_{t + h}$ represents
the prediction target relative to time $t$.  $\bX_t^{h} = (T + h, Z_{t -
l_1}, \ldots, Z_{t - l_M})$, where $l_1, \ldots, l_M$ are the elements of the
set $L$ of lags used for prediction, represents the vector of predictive
variables relative to time $t$.  We can use the observed quantities $z_t$ to
form the pair $(\bx_t^{h}, y_t^{h})$ for all $t = 1 + \max_m l_m, \ldots, T - h$;
for smaller values of $t$ there are not enough observations before $t$ to form
$\bx_t^{h}$ and for larger values of t there are not enough observations after $t$ to form
$y_t^{h}$.  With this notation, the distribution we wish to estimate is
$f^{h}(y_T^{h} | \bx_T^{h}; \btheta^h)$.

In order to do this, we regard $\{(\by_t, \bx_t), t = 1 + \max_m l_m, \ldots, T
- h\}$ as a (dependent) sample from the joint distribution of $(\bX, Y)$, and estimate
the conditional distribution of $Y | \bX$ via KCDE.  The KCDE estimate is given
by
\begin{equation}
\widehat{f}_{\bY|\bX}(\by | \bx) = \frac{\sum_{t \in \btau} K^{\bX, \bY}\left\{(\bx', \by')', (\bx'_t, \by'_t)'; \bH^{\bX,\bY}\right\}}{\sum_{t \in \btau}K^{\bX}(\bx, \bx_t ; \bH^{\bX})}. \label{eqn:KCDEDefinition}
\end{equation}
Here, $'$ is the transpose operator and $\btau \subseteq \{1, \ldots, T\}$
indexes the subset of observations used in obtaining the conditional density
estimate.  In the final density estimate, $\btau$ typically includes all
available time points, but proper subsets are used in the cross-validation
procedures we discuss later for parameter estimation.


%In order to do this, we employ kernel density estimation.  Let $K^{\bY}(\by, \by^*,
%H^{\bY})$ and $K^{\bX}(\bx, \bx^*, H^{\bX})$ be kernel functions centered at
%$\by^*$ and $\bx^*$ respectively and with bandwidth matrices $H^{\bY}$ and
%$H^{\bX}$.  We estimate the conditional distribution of $\bY | \bX$ as follows:
%\begin{align}
%&\widehat{f}_{\bY|\bX}(\by | \bX = \bx) = \frac{\widehat{f}_{\bY, \bX}(\by, \bx)}{\widehat{f}_{\bX}(\bx)} \label{eqn:KDECondDef} \\
%&\qquad = \frac{\sum_{t \in \tau} K^{\bY, \bX}\{(\by, \bx), (\by_t, \bx_t), H^{\bY, \bX}\}}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
%&\qquad = \frac{\sum_{t \in \tau} K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) K^{\bX}(\bx, \bx_t, H^{\bX})}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
%&\qquad = \sum_{t \in \tau} w_t K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) \text{, where} \label{eqn:KDEwt} \\
%&w_t = \frac{ K^{\bX}(\bx, \bx_t, H^{\bX}) }{\sum_{t^* \in \tau} K^{\bX}(\bx, \bx_{t^*}, H^{\bX}) } \label{eqn:KDEWeightsDef}
%\end{align}

%In Equation~\eqref{eqn:KDECondDef}, we are making use of the fact that the
% conditional density for $\bY | \bX$ can be written as the quotient of the joint density for $(\bY, \bX)$ and the marginal density for $\bX$.  In Equation~\eqref{eqn:KDESubKDEJtMarginal}, we obtain separate kernel density estimates for the joint and marginal densities in this quotient.  In Equation~\eqref{eqn:KDEwt}, we rewrite this quotient by passing the denominator of Equation~\eqref{eqn:KDESubKDEJtMarginal} into the summation in the numerator.  We can interpret the result as a weighted kernel density estimate, where each observation $t \in \tau$ contributes a different amount to the final conditional density estimate.  The amount of the contribution from observation $t$ is given by the weight $w_t$, which effectively measures how similar $\bx_t$ is to the point $\bx$ at which we are estimating the conditional density.  If $\bx_t^{(\bl^{max})}$ is similar to $\bx_{t^*}^{(\bl^{max})}$, a large weight is assigned to $t$; if $\bx_t^{(\bl^{max})}$ is different from $\bx_{t^*}^{(\bl^{max})}$, a small weight is assigned to $t$.

%In kernel density estimation, it is generally required that the kernel
% functions integrate to $1$ in order to obtain valid density estimates.  However, after conditioning on $\bX$, it is no longer necessary that $K^{\bX}(\bx, \bx_t, H^{\bX})$ integrate to $1$.  In fact, as can be seen from Equation~\eqref{eqn:KDEWeightsDef}, any multiplicative constants of proportionality will cancel out when we form the observation weights.  We can therefore regard $K^{\bX}(\bx, \bx_t, H^{\bX})$ as a more general weighting function that measures the similarity between $\bx$ and $\bx_t$.  As we will see, eliminating the constraint that $K^{\bX}$ integrates to $1$ is a useful expansion the space of functions that can be used in calculating the observation weights.  However, we still require that $K^{\bY}$ integrates to $1$.

%In Equations \eqref{eqn:KDECondDef} through \eqref{eqn:KDEWeightsDef}, $\tau$
% is an index set of time points used in obtaining the density estimate.  In most settings, we can take $\tau = \{1 + P + L, \ldots, T\}$.  These are the time points for which we can form the lagged observation vector $\bx_t$ and the prediction target vector $\by_t$.  However, we will place additional restrictions on the time points included in $\tau$ in the cross-validation procedure discussed in Section \ref{sec:Estimation}.

%In Equation~\eqref{eqn:KCDEDefinition}, if both $K^{\bX, \bY}$ and $K^{\bX}$
%integrate to 1 with respect to $\bx$ and $y$ then the numerator is a kernel
%density estimate of the joint density of $\bX$ and $\bY$ and the denominator
%is a kernel density estimate of the marginal density of $\bX$; forming the quotient yields an
%estimate of the conditional density of $\bY | \bX$.  However, it is not strictly
%required that 

We will work with a slightly restricted specification of
Equation~\eqref{eqn:KCDEDefinition} in which the kernel function $K^{\bX,\bY}$
can be written as the product of $K^{\bX}$ and a
``conditional kernel'' $K^{\bY|\bX}$:
\begin{equation}
K^{\bX,\bY}\left\{(\bx', \by')', (\bx'_t, \by'_t)'; \bH^{\bX,\bY}\right\} = K^{\bX}\left\{\bx, \bx_t; \bH^{\bX}\right\} K^{\bY | \bX}\left\{\by, \by_t | \bx, \bx_t; \bH^{\bX,\bY}\right\}.
\end{equation}

Make clear that $H^X$ is a sub-matrix of $H^{Y,X}$.  Hall, Racine, and
Li \cite{hall2004crossvalidationKCDE}
say that this "does not adversely affect
the rate of convergence of estimators..."

With this restriction, we can rearrange Equation~\eqref{eqn:KCDEDefinition} to
obtain
\begin{align}
\widehat{f}_{\bY|\bX}(\by | \bx) &= \sum_{t \in \btau} w_t K^{\bY | \bX}\left\{\by, \by_t | \bx, \bx_t; \bH^{\bX,\bY}\right\}, \text{ where} \label{eqn:KCDEDefinitionWeighted} \\
w_t &= \frac{K^{\bX}\left\{\bx, \bx_t; \bH^{\bX}\right\}}{\sum_{t^* \in \btau} K^{\bX}\left\{\bx, \bx_{t^*}; \bH^{\bX}\right\}}.
\end{align}

We can now interpret $K^{\bX}$ as a weighting function determining how much each
observation $(\bx_t, \by_t)$ contributes to our final density estimate according to how similar
$\bx_t$ is to the value $\bx$ that we are conditioning on.
$K^{\bY | \bX}$ is a density function that contributes
mass to the final density estimate near the observed value $\by_t$; we require
that for any values of $\by_t$, $\bx_t$, and $\bx$, the integral of $K^{\bY |
\bX}$ with respect to $\by$ must equal $1$.  Since the weights $w_t$ sum to $1$,
this condition ensures that the combined density estimate integrates to 1.  The
bandwidth matrices $\bH^{\bX}$ and $\bH^{\bX, \bY}$ are parameterized by
$\btheta^h$ and control the locality and orientation of the weighting function
and the contributions to the density estimate from each observation.  In
practice, we have used the Cholesky decomposition of 

In order to complete the formulation of the estimator given in
Equation~\eqref{eqn:KCDEDefinition}, we must specify the kernel functions.

We obtain the discretized kernel function by integrating an underlying
continuous kernel function over the dimensions corresponding to discrete
random variables.  To make this concrete, consider a $J$-dimensional random
vector $\bZ = (\bZ^{d'}, \bZ^{c'})'$ that is partitioned into a
$J^d$-dimensional subvector $\bZ^d$ of discrete random variables and a
$J^c$-dimensional subvector $\bZ^c$ of continuous random variables.  Without
loss of generality, we assume that the discrete variables are the first $J^d$
variables.  For each component random variable $Z_j$, $j = 1, \ldots, J$, we
denote the set of values that $Z_j$ may take by $\mathcal{D}^j \subseteq
\mathbb{R}$.  In the continuous cases, we take $\mathcal{D}^j = \mathbb{R}$.  In
the discrete cases, $\mathcal{D}^j$ is a discrete set such as the positive
integers.  Our definitions could be modified to handle a component random
variable whose distribution comprised a combination of discrete and continuous
parts; however, this is not required for our applications so we have not pursued
that line here.

Let $L(\bz, \bz^{*} ; \bH)$ be a continuous multivariate kernel function defined
on $\prod_{j = 1}^{J} \mathcal{E}^j$.  For each discrete variable indexed by $j
= 1, \ldots, J^d$, we associate lower and upper bounds of integration $a_{z_j}$
and $b_{z_j}$ with each value $z_j \in \mathcal{D}^j$.  In order to ensure that
our final density estimate integrates to $1$, we require that these integration
bounds form a disjoint cover of $\mathcal{E}^j$ in the sense that $\cup_{z_j \in
\mathcal{D}^j} [a_{z_j}, b_{z_j}) = \mathcal{E}^j$ and $\cap_{z_j \in
\mathcal{D}^j} [a_{z_j}, b_{z_j}) = \emptyset$, the empty set.  At a vector of
values $\bz \in \prod_{j = 1}^J \mathcal{D}^j$, we define the partially
discretized kernel as follows:
\begin{equation*}
K(\bz, \bz^{*} ; \bH) = \int_{a_{z_1}}^{b_{z_1}} \cdots
\int_{a_{z_{J^d}}}^{b_{z_{J^d}}} L(\bz, \bz^{*} ; \bH) \, d z_{1} \cdots d
z_{J^d}
\end{equation*}

This approach can be used to discretize any underlying continuous kernel
function.  In the applications in Section~\ref{sec:Applications} below, we have
used a multivariate log-normal kernel function.  This is similar to the
suggestion of *** of using log-transformed data, but allows us to work with the
data on the original scale.  Advantages of this kernel specification include
automatic handling of the restriction that counts are non-negative, and approximately
capturing the long tail in disease incidence that we will illustrate in the
applications Section below.

Seasonality is an important characteristic of many infectious disease time
series, as we will illustrate in the applications below. Here we describe one
approach to capturing this seasonality within the KCDE framework by using a
kernel function that is periodic in the observation time $t$.  This periodic
kernel function was originally developed in the literature on Gaussian Processes
for the purposes of specifying a periodic covariance
function\cite{mackay1998introductionGP}, and is defined by
\begin{equation}
K(t, t^*; \rho, h) = \exp\left[- \frac{\sin^2\{\rho (t - t^*)\}}{2h^2} \right]. \label{eqn:PeriodicKernel}
\end{equation}
This functional form can also be obtained by mapping $t$ to the bivariate pair
$\{cos(\rho t), sin(\rho t)\}$ and then applying a bivariate Gaussian kernel to
that pair.  We picture this periodic kernel function in Figure
\ref{fig:PeriodicKernelPlot}.

This kernel function does not integrate to $1$, but this does not preclude us
from using it for the conditioning variables $\bX_t$ that are used to calculate
the observation weights $w_t$ in Equation~\eqref{eqn:KCDEDefinitionWeighted}. 
The result is that observations are weighted according to the similarity of the
time of year that an observation was collected with the time of year of the
analysis time from which we are predicting forward.  Effectively, this allows
the approach to set the weights $w_t$ to be small when the observation time does
not match the observation time from which we are predicting.  The strength of
this weighting is determined by the bandwidth parameter $h$.

%\begin{figure}[height=2in]
\begin{figure}
\caption{The periodic kernel function illustrated as a function of time in
weeks with $\rho = \pi / 52$ and three possible values for the bandwidth
parameter $h$.}
\label{fig:PeriodicKernelPlot}
<<PeriodicKernelPlot, echo = FALSE, fig.height = 2>>=
plot_df <- data.frame(t=seq_len(5 * 52))

kernel_center <- plot_df$t[nrow(plot_df)]
rho <- pi / 52

h <- 0.1
plot_df$kernel_h0.1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 1
plot_df$kernel_h1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 10
plot_df$kernel_h10 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

plot_df <- melt(plot_df, id.vars = "t")
plot_df$variable <- as.character(plot_df$variable)
plot_df$bandwidth <- "0.1"
plot_df$bandwidth[plot_df$variable == "kernel_h1"] <- "1"
plot_df$bandwidth[plot_df$variable == "kernel_h10"] <- "10"

ggplot(plot_df) +
    geom_line(aes(x = t, y = value, linetype = bandwidth, colour = bandwidth)) +
    geom_vline(xintercept = kernel_center) +
    scale_colour_manual("Bandwidth",
        breaks = c("0.1", "1", "10"),
        labels = c("0.1", "1", "10"),
        values = c("#E69F00", "#56B4E9", "#009E73")
    ) +
    scale_linetype("Bandwidth") +
    ylab("Kernel Function Value") +
    xlab("Time in Weeks") +
#    ggtitle("The Periodic Kernel") +
    theme_bw(base_size = 11)
@
\end{figure}

We use cross-validation to select the variables that are used in the model and estimate the corresponding bandwidth parameters by (approximately) minimizing a cross-validation measure of the quality of the predictions obtained from the model.  Formally,
\begin{align}
&(\widehat{\bu}, \widehat{H}^{\bX}, \widehat{H}^{\bY}) \approx \argmin{(\bu, H^{\bX}, H^{\bY})} \sum_{t^* = 1 + P + L}^T Q[ \by_{t^*}, \widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \}) ] \label{eqn:ParamEst}
\end{align}
Here, $Q$ is a loss function that measures the quality of the estimated density $\widehat{f}$ given an observation $\by_{t^*}$.  We have made the dependence of this estimated density on the the parameters $\bu$, $H^{\bx}$, and $H^{\bY}$, as well as on the data $\{ (\by_t, \bx_t): t \in \tau_{t^*} \}$, explicit in the notation.  In order to reduce the potential for our parameter estimates to be affected by local correlation in the time series, we eliminate all time points that fall within one year of $t^*$ from the index set $\tau_{t^*}$ used to form the conditional density estimate $\widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \})$.

Hart and Vieu\cite{hart1990bandwidthEstDependentData} show that when kernel density estimation is used to estimate
a marginal density with dependent observations, leaving out multiple time points
around the target time point in cross validation can yield small improvements in
the ISE under certain assumptions about the form of the dependence.

\lboxit{Talk about proper scoring rules and our particular choice of $Q$.}


\subsection{Combining Marginal Predictive Distributions with Copulas}
\label{subsec:Methods:Copulas}

The approach we take for some of the prediction targets we examine in our
applications is to obtain a joint predictive distribution for disease incidence
over a sequence of multiple prediction horizons.  We do this by
using a copula to combine marginal predictive densities for each of those
prediction horizons.  Specifically, we us the isotropic Gaussian copula
implemented in the {\tt R}\cite{RCoreLanguage} package {\tt copula}
\cite{HofertRCopulaPackage}.

This copula function is given by
\begin{equation}
c(u_1, \ldots, u_J ; \btheta_c) = \Phi_{\Sigma}(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_J)),
\end{equation}
where $\Phi^{-1}$ is the inverse {c.d.f.} of a standard univariate Gaussian
distribution and $\Phi_{\Sigma}$ is the {c.d.f.} of a multivariate Gaussian
distribution with mean $\b0$ and covariance matrix $\Sigma$.  We set $\Sigma =
[\sigma_{i,j}^2]$, where 
\begin{equation}
\sigma_{i,j}^2 = \begin{cases} 1 \text{ if $i = j$,} \\ \rho_d \text{ if $\vert i - j \vert = d$} \end{cases}
\end{equation}
Intuitively, $\rho_d$ captures the correlation of incidence at future times that
are $d$ weeks apart.

Estimation by maximum likelihood.

\section{Simulation Study}
\label{sec:SimStudy}

In this Section, we conduct two sets of simulation studies designed to answer
two separate questions:
\begin{enumerate}
\item How much does using a kernel function with a non-diagonal bandwidth matrix
contribute to the quality of conditional density estimates relative to density
estimates obtained through KCDE using diagonal bandwidth matrices?
\item How does our method perform in the context of seasonal time series data? 
Specifically, how does the method perform relative to common alternatives, and
how much do each of our three contributions (non-diagonal bandwidth matrices for
discrete data, using a periodic function of time as predictive variable, and
use of low band-pass filtered observatiosn as predictive variables) contribute
to predictive performance?
\end{enumerate}

\subsection{Comparison of KCDE approaches}
\label{sec:SimStudiesKCDEComparison}

Our first set of simulation studies is based closely on those conducted in
\cite{duong2005crossvalidationBandwidthMultivariateKDE}; their examples
demonstrate the utility of using a fully parameterized bandwidth matrix in
kernel density estimation of continuous distributions.  We modify their
simulation study to examine the benefits of fully parameterized bandwidth
matrices in the context of conditional density estimation with discrete
variables.

We simulate observations from each of seven distributions.  The first five of
these are plotted in Figure ***.


<<SimStudyDistributionsDiscretizedDuongHazelton>>=
library(ggplot2)
library(grid)
library(plyr)
library(dplyr)
library(tidyr)
library(pdtmvn)
library(kcde)
source("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/code/sim-densities-sim-study-discretized-Duong-Hazelton.R")

## Density family bivariate-A
n_sim <- 10000
discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-A-discretized") %>%
    as.data.frame()
continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-A") %>%
    as.data.frame()
discrete_sample_counts <- discrete_sample %>%
    count(X1, X2)

pa <- ggplot() +
    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
pa

## Density family bivariate-B
n_sim <- 10000
discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-B-discretized") %>%
    as.data.frame()
continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-B") %>%
    as.data.frame()
discrete_sample_counts <- discrete_sample %>%
    count(X1, X2)

pb <- ggplot() +
    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
pb

## Density family bivariate-C
n_sim <- 10000
discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-C-discretized") %>%
    as.data.frame()
continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-C") %>%
    as.data.frame()
discrete_sample_counts <- discrete_sample %>%
    count(X1, X2)

pc <- ggplot() +
    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
pc

## Density family bivariate-D
n_sim <- 10000
discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-D-discretized") %>%
    as.data.frame()
continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-D") %>%
    as.data.frame()
discrete_sample_counts <- discrete_sample %>%
    count(X1, X2)

pd <- ggplot() +
    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
pd

## Density family multivariate-2d
n_sim <- 10000
discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "multivariate-2d-discretized") %>%
    as.data.frame()
continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "multivariate-2d") %>%
    as.data.frame()
discrete_sample_counts <- discrete_sample %>%
    count(X1, X2)

pd <- ggplot() +
    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
pd

@

\section{Applications}
\label{sec:Applications}

In this Section, we illustrate the methods through applications to prediction in
examples with several real time series data sets.

\subsection{Example 1: Influenza Prediction}

In our first and simplest example, we apply the method for prediction of
influenza with prediction horizons of 1 through 4 weeks.  Data on influenza
incidence are available through {\tt R}'s {\tt cdcfluview} package.  Here we
create a data set with a nationally aggregated measure of flu incidence

<<FluDataLoadData, echo = FALSE>>=
library(cdcfluview)
library(plyr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(grid)
library(kcde)

usflu<-get_flu_data("national", "ilinet", years=1997:2015)
ili_national <- transmute(usflu,
    region.type = REGION.TYPE,
    region = REGION,
    year = YEAR,
    week = WEEK,
    weighted_ili = as.numeric(X..WEIGHTED.ILI))
ili_national$time <- ymd(paste(ili_national$year, "01", "01", sep = "-"))
week(ili_national$time) <- ili_national$week
ili_national$time_index <- seq_len(nrow(ili_national))

str(ili_national)
@

We plot the {\tt total\_cases} measure over time, representing missing values
with vertical grey lines.  The low season was not measured in the first few
years.

<<FluDataInitialPlotTotalCases, echo = FALSE>>=
ggplot() +
    geom_line(aes(x = as.Date(time), y = weighted_ili), data =
ili_national) +
    geom_vline(aes(xintercept = as.numeric(as.Date(time))),
        colour = "grey",
        data = ili_national[is.na(ili_national$weighted_ili), ]) +
    scale_x_date() +
    xlab("Time") +
    ylab("Weighted ILI") +
    theme_bw()
@

There are several methods that we could employ to handle these missing data:
\begin{enumerate}
\item Impute the missing values.  They are all in the low season, so this should be relatively easy to do.
\item Drop all data up through the last NA.
\item Use the data that are available.
\end{enumerate}
Of these approaches, the first is probably preferred.  The concern with the second
is that we are not making use of all of the available data.  The potential concern with the
third is that in the data used in estimation, there will be more examples of prediction of values in the high season
using values in the high season and middle of the season than of prediction of values in the high season using values in the low season.
This could potentially affect our inference.  However, we do not expect this effect to be large,
so we proceed with this option for the purposes of this example.

%We also plot histograms of the observed total cases on the original scale and on the log scale.
%
%<<FluDataHistogramPlotTotalCases, echo = FALSE>>=
%hist_df <- rbind(
%	data.frame(value = ili_national$weighted_ili,
%    	variable = "Weighted ILI"),
%    data.frame(value = log(ili_national$weighted_ili),
%    	variable = "log(Weighted ILI)")
%)
%
%ggplot(aes(x = value), data = hist_df) +
%    geom_histogram() +
%    facet_wrap( ~ variable, ncol = 2) +
%    xlab("Weighted ILI") +
%    theme_bw()
%@
%
%These plots demonstrate that total cases follows an approximately log-normal
%distribution.  In the application below, we will consider modeling these data on
%both the original scale and the log scale.  Intuitively, since we are using a
%kernel that is obtained from a Gaussian, modeling the data on the log scale
%should yield better performance.  On the other hand, the performance gain may be
%negligible if we have enough data.

%Finally, we plot the autocorrelation function:
%
%<<FluDataACFPlotTotalCases, echo = FALSE>>=
%last_na_ind <- max(which(is.na(ili_national$weighted_ili)))
%non_na_inds <- seq(from = last_na_ind + 1, to=nrow(ili_national))
%acf(ili_national$weighted_ili[non_na_inds],
%  lag.max = 52 * 4)
%@
%
%This plot illustrates the annual periodicity that was also visible in the
%initial data plot above.  There is no apparent evidence of longer term annual
%cycles.  We therefore include a periodic kernel acting on the time index with a
%period of 52.2 weeks (the length of the period is motivated by the fact that
%in our data, there is a year with 53 weeks once every 5 or 6 years).


%<<FluDataKernelComponentsSetup, echo = TRUE>>=
%## Definitions of kernel components.  A couple of notes:
%##   1) In the current implementation, it is required that separate kernel
%##      components be used for lagged (predictive) variables and for leading
%##      (prediction target) variables.
%##   2) The current syntax is verbose; in a future version of the package,
%##      convenience functions may be provided.
%
%## Define kernel components -- 3 pieces:
%##   1) Periodic kernel acting on time index
%##   2) pdtmvn kernel acting on lagged total cases (predictive) -- all continuous
%##   3) pdtmvn kernel acting on lead total cases (prediction target) -- all continuous
%kernel_components <- list(
%    list(
%        vars_and_offsets = data.frame(var_name = "time_index",
%            offset_value = 0L,
%            offset_type = "lag",
%            combined_name = "time_index_lag0",
%            stringsAsFactors = FALSE),
%        kernel_fn = periodic_kernel,
%        theta_fixed = list(period=pi / 52.2),
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_periodic_kernel,
%        initialize_kernel_params_args = NULL,
%        vectorize_kernel_params_fn = vectorize_params_periodic_kernel,
%        vectorize_kernel_params_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_periodic_kernel,
%        update_theta_from_vectorized_theta_est_args = NULL
%    ),
%    list(
%        vars_and_offsets = data.frame(var_name = "weighted_ili",
%            offset_value = 1L,
%            offset_type = "horizon",
%            combined_name = "time_index_horizon1",
%            stringsAsFactors = FALSE),
%        kernel_fn = pdtmvn_kernel,
%        rkernel_fn = rpdtmvn_kernel,
%        theta_fixed = list(
%            parameterization = "bw-diagonalized-est-eigenvalues",
%            continuous_vars = "weighted_ili_horizon1",
%            discrete_vars = NULL,
%            discrete_var_range_fns = NULL,
%            lower = -Inf,
%            upper = Inf
%        ),
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
%        initialize_kernel_params_args = NULL,
%        vectorize_kernel_params_fn = vectorize_params_pdtmvn_kernel,
%        vectorize_kernel_params_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
%        update_theta_from_vectorized_theta_est_args = NULL
%    ))#,
%    list(
%        vars_and_lags = vars_and_lags[3:5, ],
%        kernel_fn = pdtmvn_kernel,
%        rkernel_fn = rpdtmvn_kernel,
%        theta_fixed = NULL,
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
%        initialize_kernel_params_args = list(
%            continuous_vars = vars_and_lags$combined_name[3:4],
%            discrete_vars = vars_and_lags$combined_name[5],
%            discrete_var_range_fns = list(
%                c_lag2 = list(a = pdtmvn::floor_x_minus_1, b = floor, in_range = pdtmvn::equals_integer, discretizer = round_up_.5))
%        ),
%        vectorize_theta_est_fn = vectorize_params_pdtmvn_kernel,
%        vectorize_theta_est_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
%        update_theta_from_vectorized_theta_est_args = list(
%            parameterization = "bw-diagonalized-est-eigenvalues"
%        )
%    ))
%@

<<FluDataMergePredictionResults, echo = FALSE>>=
library(plyr)
library(dplyr)
library(tidyr)

ili_prediction_results_sarima <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/prediction-results/sarima-predictions.rds")
ili_prediction_results_kcde <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/prediction-results/kcde-predictions.rds")
ili_prediction_results <- rbind.fill(ili_prediction_results_sarima[!is.na(ili_prediction_results_sarima$log_score), ],
    ili_prediction_results_kcde)
ili_prediction_results$AE <- unlist(ili_prediction_results$AE)

ili_prediction_results$full_model_descriptor <- paste0(ili_prediction_results$model,
    "-seasonal_lag_", ili_prediction_results$max_seasonal_lag,
#    "-filtering_", ili_prediction_results$filtering,
    "-differencing_", ili_prediction_results$differencing,
    "-periodic_", ili_prediction_results$seasonality,
    "-bw_", ili_prediction_results$bw_parameterization)
@

%<<FluDataRibbonsPredictionPlot50Intervals, echo = FALSE>>=
%ribbons_df <- ili_prediction_results %>%
%    select(prediction_time,
%        prediction_horizon,
%        full_model_descriptor,
%        model,
%        interval_pred_lb_95:interval_pred_ub_50) %>%
%    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
%    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
%        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
%    spread(bound_type, predictive_value)
%
%phs_used <- c(1, 6, 13, 26, 52)
%models_used <- c("SARIMA-filtering_NA-periodic_NA-bw_NA", "kcde-filtering_FALSE-periodic_TRUE-bw_full")
%
%ggplot() +
%    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model),
%        alpha = 0.4,
%        size = 0,
%        data = ribbons_df[ribbons_df$prediction_horizon %in% phs_used &
%                ribbons_df$full_model_descriptor %in% models_used &
%                ribbons_df$interval_type == "50", ]) +
%    geom_line(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2010:2014, ]) +
%#    geom_point(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2010:2014, ]) +
%    geom_line(aes(x = prediction_time, y = pt_pred, colour = model),
%        data = ili_prediction_results[ili_prediction_results$prediction_horizon %in% phs_used &
%                ili_prediction_results$full_model_descriptor %in% models_used, ]) +
%#    scale_alpha_discrete("Prediction\nInterval\nCoverage",
%#        labels = c("50 Percent", "95 Percent"),
%#        limits = c("50", "95"),
%#        range = c(0.4, 0.2)) +
%    facet_wrap( ~ prediction_horizon, ncol = 1) +
%    ggtitle("Point and 50% Interval Predictions") +
%    theme_bw()
%@
%
%
%<<FluDataRibbonsPredictionPlot95Intervals, echo = FALSE>>=
%ggplot() +
%    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model),
%        alpha = 0.4,
%        size = 0,
%        data = ribbons_df[ribbons_df$prediction_horizon %in% phs_used &
%                ribbons_df$full_model_descriptor %in% models_used &
%                ribbons_df$interval_type == "95", ]) +
%    geom_line(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2010:2014, ]) +
%#    geom_point(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2010:2014, ]) +
%    geom_line(aes(x = prediction_time, y = pt_pred, colour = model),
%        data = ili_prediction_results[ili_prediction_results$prediction_horizon %in% phs_used &
%                ili_prediction_results$full_model_descriptor %in% models_used, ]) +
%#    scale_alpha_discrete("Prediction\nInterval\nCoverage",
%#        labels = c("50 Percent", "95 Percent"),
%#        limits = c("50", "95"),
%#        range = c(0.4, 0.2)) +
%    facet_wrap( ~ prediction_horizon, ncol = 1) +
%    ggtitle("Point and 95% Interval Predictions") +
%    theme_bw()
%@


%<<FluDataPredictionsPlotViolinLogScore>>=
%library(ggplot2)
%
%ggplot() +
%    geom_violin(aes(x = factor(full_model_descriptor), y = log_score), data =
%     ili_prediction_results) + theme_bw()
%@

<<FluDataPredictionsPlotViolinLogScoreDifference, fig.height = 8, echo = FALSE>>=
ili_prediction_log_score_diffs_wide <- ili_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

ili_prediction_log_score_diffs_wide[, unique(ili_prediction_results$full_model_descriptor)] <-
    ili_prediction_log_score_diffs_wide[, unique(ili_prediction_results$full_model_descriptor)] -
    ili_prediction_log_score_diffs_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
#    ili_prediction_log_score_diffs_wide[, "SARIMA-seasonal_lag_NA-filtering_NA-differencing_NA-periodic_NA-bw_NA"]

ili_prediction_log_score_diffs_long <- ili_prediction_log_score_diffs_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor))

ggplot() +
    geom_violin(aes(x = factor(model), y = log_score_difference), data = ili_prediction_log_score_diffs_long) +
    theme_bw() +
    theme(axis.text.x=element_text(angle = -90, hjust = 0))
@


<<FluDataPredictionsPlotViolinLogScoreDifferenceZoomedIn, fig.height = 8, echo = FALSE>>=
ggplot() +
    geom_violin(aes(x = factor(model), y = log_score_difference), data = ili_prediction_log_score_diffs_long) +
    theme_bw() +
    ylim(c(-6, 6)) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0))
@

<<FluDataPredictionsPlotBoxplotLogScoreDifferenceZoomedIn, fig.height = 8, echo = FALSE>>=
ggplot() +
    geom_boxplot(aes(x = factor(model), y = log_score_difference), data = ili_prediction_log_score_diffs_long) +
    theme_bw() +
    ylim(c(-6, 6)) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0))
@


<<FluDataPredictionsPlotStripLogScoreDifference, fig.height = 9, echo = FALSE>>=
ggplot() +
    geom_point(aes(x = factor(model), y = log_score_difference), size = 0.1, position = position_jitter(w = 0.2, h = 0), alpha = 0.01, data = ili_prediction_log_score_diffs_long) +
    theme_bw() +
    ylim(c(-6, 6)) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0))
@


<<FluDataPredictionsPlotViolinLogScoreDifferencePeakSeasonOnly, echo = FALSE>>=
peak_season_test_inds_in_ili_national <- c(
    693:704, # 2010/2011 season, starting Jan 2011
    744:755, # 2011/2012 season
    791:808, # 2012/2013 season
    844:857, # 2013/2014 season
    895:901  # 2014/2015 season, ending end of Dec 2014
)
peak_season_test_times <- ili_national$time[peak_season_test_inds_in_ili_national]



ili_prediction_log_score_diffs_wide <- ili_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

ili_prediction_log_score_diffs_wide[, unique(ili_prediction_results$full_model_descriptor)] <-
    ili_prediction_log_score_diffs_wide[, unique(ili_prediction_results$full_model_descriptor)] -
    ili_prediction_log_score_diffs_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
#    ili_prediction_log_score_diffs_wide[, "SARIMA-seasonal_lag_NA-filtering_NA-differencing_NA-periodic_NA-bw_NA"]

ili_prediction_log_score_diffs_long <- ili_prediction_log_score_diffs_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor))

ggplot() +
    geom_violin(aes(x = factor(model), y = log_score_difference),
        data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$prediction_time %in% peak_season_test_times, ]) +
    theme_bw() +
    theme(axis.text.x=element_text(angle = -60, hjust = 0))
@



<<FluDataPredictionsPlotViolinLogScoreDifferenceOnlyModelsWithoutFiltering, echo = FALSE>>=
models_used <- c("kcde-filtering_FALSE-periodic_FALSE-bw_diagonal",
    "kcde-filtering_FALSE-periodic_FALSE-bw_full",
    "kcde-filtering_FALSE-periodic_TRUE-bw_diagonal",
    "kcde-filtering_FALSE-periodic_TRUE-bw_full")

central_stat_log_score_diff_by_model <- ili_prediction_log_score_diffs_long %>%
    group_by(model) %>%
    summarize(median = median(log_score_difference),
        mean = mean(log_score_difference)) %>%
    gather_("statistic", "value", c("median", "mean"))

ggplot() +
    geom_violin(aes(x = factor(model), y = log_score_difference),
        data = ili_prediction_log_score_diffs[ili_prediction_log_score_diffs$model %in% models_used, ]) +
    geom_hline(aes(yintercept = 0), colour = "red") +
    geom_point(aes(x = factor(model), y = value, colour = statistic),
        data = central_stat_log_score_diff_by_model[central_stat_log_score_diff_by_model$model %in% models_used, ]) +
    theme_bw() +
    theme(axis.text.x=element_text(angle = -60, hjust = 0))
@


%<<FluDataPredictionsPlotViolinLogScoreDifferenceByHorizon, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%
%
%median_log_score_diff_by_model_and_horizon <- ili_prediction_log_score_diffs_long %>%
%    group_by(model, prediction_horizon) %>%
%    summarize(median = median(log_score_difference))
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_violin(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    geom_point(aes(x = factor(prediction_horizon), y = median), colour = "blue", data = median_log_score_diff_by_model_and_horizon[median_log_score_diff_by_model_and_horizon$model %in% models_used, ]) +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@


%<<FluDataPredictionsPlotLineLogScoreDifferenceByPredictionTimeAndHorizon, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%
%phs_used <- c(1, 6, 13, 26, 39, 52)
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used & ili_prediction_log_score_diffs_long$prediction_horizon %in% phs_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_point(aes(x = prediction_time, y = log_score_difference, colour = factor(prediction_horizon), shape = factor(prediction_horizon))) +
%    geom_line(aes(x = prediction_time, y = log_score_difference, colour = factor(prediction_horizon))) +
%    geom_hline(aes(yintercept = 0), colour = "black") +
%#    geom_point(aes(x = factor(prediction_horizon), y = median), colour = "blue", data = median_log_score_diff_by_model_and_horizon[median_log_score_diff_by_model_and_horizon$model %in% models_used, ]) +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw()# +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%<<FluDataPredictionsPlotBoxplotLogScoreDifferenceByHorizonOnlyModelsWithDifferencing, fig.height = 9, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full",
%    "kcde-seasonal_lag_0-differencing_TRUE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_1-differencing_TRUE-periodic_FALSE-bw_diagonal",
%    "kcde-seasonal_lag_1-differencing_TRUE-periodic_TRUE-bw_diagonal")
%
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_boxplot(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%%<<FluDataPredictionsPlotBoxplotScoreRatioByHorizonOnlyModelsWithDifferencing, fig.height = 9, echo = FALSE>>=
%%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%%#    facet_wrap( ~ model, ncol = 1) +
%%#    theme_bw() +
%%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%%
%%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full")
%%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full",
%%    "kcde-seasonal_lag_0-differencing_TRUE-periodic_TRUE-bw_full",
%%    "kcde-seasonal_lag_1-differencing_TRUE-periodic_FALSE-bw_diagonal",
%%    "kcde-seasonal_lag_1-differencing_TRUE-periodic_TRUE-bw_diagonal")
%%
%%
%%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%%    geom_boxplot(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%%    geom_hline(aes(yintercept = 0), colour = "red") +
%%    facet_wrap( ~ model, ncol = 1) +
%%    theme_bw() +
%%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%%@
%
%
%<<FluDataPredictionsPlotBoxplotLogScoreDifferenceByHorizonOnlyModelsWithoutDifferencing, fig.height = 9, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full")
%
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_boxplot(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%
%<<FluDataPredictionsPlotBoxplotScoreRatioByHorizonOnlyModelsWithoutDifferencing, fig.height = 9, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-FALSE-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_TRUE-periodic_FALSE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_FALSE-bw_full",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full")
%
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_boxplot(aes(x = factor(prediction_horizon), y = exp(log_score_difference))) +
%    geom_hline(aes(yintercept = 1), colour = "red") +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%<<FluDataPredictionsPlotViolinLogScoreDifferenceByHorizonPeakSeasonOnly, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%
%median_log_score_diff_by_model_and_horizon <- ili_prediction_log_score_diffs_long %>%
%    group_by(model, prediction_horizon) %>%
%    summarize(median = median(log_score_difference))
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used & ili_prediction_log_score_diffs_long$prediction_time %in% peak_season_test_times, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_violin(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    geom_point(aes(x = factor(prediction_horizon), y = median), colour = "blue", data = median_log_score_diff_by_model_and_horizon[median_log_score_diff_by_model_and_horizon$model %in% models_used, ]) +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%<<FluDataPredictionsPlotStripLogScoreDifferenceByHorizonPeakSeasonOnly, echo = FALSE>>=
%#ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% unique(ili_prediction_results$full_model_descriptor)[2:5], ]) +
%##    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%#    geom_point(aes(x = prediction_horizon, y = log_score_difference)) +
%#    facet_wrap( ~ model, ncol = 1) +
%#    theme_bw() +
%#    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
%models_used <- c("kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%
%models_used <- c("kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_1-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "kcde-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")
%
%median_log_score_diff_by_model_and_horizon <- ili_prediction_log_score_diffs_long %>%
%    group_by(model, prediction_horizon) %>%
%    summarize(median = median(log_score_difference))
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used & ili_prediction_log_score_diffs_long$prediction_time %in% peak_season_test_times, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%    geom_point(aes(x = factor(prediction_horizon), y = log_score_difference),
%        position = position_jitter(width = 0.1, height = 0)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    geom_point(aes(x = factor(prediction_horizon), y = median), colour = "blue", data = median_log_score_diff_by_model_and_horizon[median_log_score_diff_by_model_and_horizon$model %in% models_used, ]) +
%    facet_wrap( ~ model, ncol = 1) +
%    theme_bw() +
%    theme(axis.text.x=element_text(angle = -60, hjust = 0))
%@
%
%
%<<FluDataResultsPredictionHorizonSplinesPlot, fig.height = 9, echo = FALSE>>=
%library(splines)
%
%prediction_horizon_spline_basis_for_plot <- bs(1:52, df = 26, intercept = TRUE)
%prediction_horizon_spline_basis_plot_df <- prediction_horizon_spline_basis_for_plot %>%
%    as.data.frame() %>%
%    mutate(prediction_horizon = 1:52) %>%
%    gather_("spline", "value", as.character(1:26))
%
%ggplot(prediction_horizon_spline_basis_plot_df) +
%    geom_line(aes(x = prediction_horizon, y = value, colour = spline)) +
%    theme_bw()
%@


%<<FluDataResultsModelIndepBoxplot1, fig.height = 9, echo = FALSE>>=
%ili_prediction_log_score_diffs_long <- ili_prediction_log_score_diffs_long %>%
%    filter(model != "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA")
%
%ili_prediction_log_score_diffs_long$model_indicator <- as.integer(as.factor(ili_prediction_log_score_diffs_long$model))
%
%ili_results_fit_samples <- readRDS(
%    file = "/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/analysis-results/influenza-results-model-samples-spline-indep-fit-chains_2-iter_1000.rds")
%
%N <- nrow(ili_prediction_log_score_diffs_long) 
%M <- 16
%B_spline <- 26
%B <- B_spline + 1
%
%
%## Make boxplot by prediction horizon for one model with corresponding spline fits superimposed
%plots <- lapply(seq_len(16), function(models_used_inds) {
%#models_used_inds <- 1L
%        models_used_inds_in_log_score_diffs_long <- which(ili_prediction_log_score_diffs_long$model_indicator %in% models_used_inds)
%        models_used <- unique(ili_prediction_log_score_diffs_long$model[models_used_inds_in_log_score_diffs_long])
%        
%        
%        beta_inds <- seq_len(B) + (models_used_inds - 1) * B
%        spline_fits <-
%            data.frame(
%                prediction_horizon = seq_len(52),
%                spline_fit = cbind(
%                    matrix(1, nrow = 52),
%                    bs(seq_len(52),
%                        df = B_spline,
%                        intercept = TRUE)
%                ) %*% t(as.matrix(ili_results_fit_samples[, paste0("beta[", beta_inds, "]")]))
%            ) %>%
%            gather_("spline_fit", "fit_val", paste0("spline_fit.", seq_len(1000)))
%        
%        p <- ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]) +
%#    geom_violin(aes(x = prediction_horizon, y = log_score_difference)) +
%            geom_boxplot(aes(x = factor(prediction_horizon), y = log_score_difference)) +
%            geom_line(aes(x = factor(prediction_horizon), y = fit_val, group = spline_fit), alpha = 0.01, data = spline_fits) +
%            geom_hline(aes(yintercept = 0), colour = "red") +
%            facet_wrap( ~ model, ncol = 1) +
%            theme_bw() +
%            theme(axis.text.x=element_text(angle = -60, hjust = 0))
%        return(p)
%    })
%
%print(plots[[1]])
%@
%
%<<FluDataResultsModelIndepBoxplot2, fig.height = 9, echo = FALSE>>=
%print(plots[[2]])
%@
%
%<<FluDataResultsModelIndepBoxplot3, fig.height = 9, echo = FALSE>>=
%print(plots[[3]])
%@
%
%<<FluDataResultsModelIndepBoxplot4, fig.height = 9, echo = FALSE>>=
%print(plots[[4]])
%@
%
%<<FluDataResultsModelIndepBoxplot5, fig.height = 9, echo = FALSE>>=
%print(plots[[5]])
%@
%
%<<FluDataResultsModelIndepBoxplot6, fig.height = 9, echo = FALSE>>=
%print(plots[[6]])
%@
%
%<<FluDataResultsModelIndepBoxplot7, fig.height = 9, echo = FALSE>>=
%print(plots[[7]])
%@
%
%<<FluDataResultsModelIndepBoxplot8, fig.height = 9, echo = FALSE>>=
%print(plots[[8]])
%@
%
%<<FluDataResultsModelIndepBoxplot9, fig.height = 9, echo = FALSE>>=
%print(plots[[9]])
%@
%
%<<FluDataResultsModelIndepBoxplot10, fig.height = 9, echo = FALSE>>=
%print(plots[[10]])
%@
%
%<<FluDataResultsModelIndepBoxplot11, fig.height = 9, echo = FALSE>>=
%print(plots[[11]])
%@
%
%<<FluDataResultsModelIndepBoxplot12, fig.height = 9, echo = FALSE>>=
%print(plots[[12]])
%@
%
%<<FluDataResultsModelIndepBoxplot13, fig.height = 9, echo = FALSE>>=
%print(plots[[13]])
%@
%
%<<FluDataResultsModelIndepBoxplot14, fig.height = 9, echo = FALSE>>=
%print(plots[[14]])
%@
%
%<<FluDataResultsModelIndepBoxplot15, fig.height = 9, echo = FALSE>>=
%print(plots[[15]])
%@
%
%<<FluDataResultsModelIndepBoxplot16, fig.height = 9, echo = FALSE>>=
%print(plots[[16]])
%@
%
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime1, echo = FALSE>>=
%## Get median within model and prediction horizon
%ili_prediction_median_log_score_diffs_by_model_and_ph <-
%    ili_prediction_log_score_diffs_long %>%
%    group_by(model, prediction_horizon) %>%
%    summarize(median_by_model_and_ph = median(log_score_difference))
%
%ili_prediction_log_score_diffs_long <- 
%    ili_prediction_log_score_diffs_long %>%
%    left_join(ili_prediction_median_log_score_diffs_by_model_and_ph,
%        by = c("model", "prediction_horizon")) %>%
%    mutate(log_score_difference_residual_from_median = log_score_difference - median_by_model_and_ph)
%
%#temp <- ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model == unique(ili_prediction_log_score_diffs_long$model)[4], ]
%#temp <- temp[order(temp$prediction_horizon), ]
%
%## For each model, ...
%plots <- lapply(seq_len(16), function(models_used_inds) {
%        ## 
%        models_used_inds_in_log_score_diffs_long <- which(ili_prediction_log_score_diffs_long$model_indicator %in% models_used_inds)
%        models_used <- unique(ili_prediction_log_score_diffs_long$model[models_used_inds_in_log_score_diffs_long])
%        
%        reduced_ili_prediction_log_score_diffs_long <-
%            ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]
%        ## For each time point, get acf wrt prediction horizon
%        
%        time_names <- paste0("time_",
%            seq_along(unique(reduced_ili_prediction_log_score_diffs_long$prediction_time))
%        )
%        plot_data <- sapply(
%            unique(reduced_ili_prediction_log_score_diffs_long$prediction_time),
%            function(prediction_time) {
%                temp <- reduced_ili_prediction_log_score_diffs_long[
%                    reduced_ili_prediction_log_score_diffs_long$prediction_time == prediction_time, ]
%                temp <- temp[order(temp$prediction_horizon), ]
%                acf(temp$log_score_difference_residual_from_median, plot = FALSE)$acf
%            }) %>%
%            as.data.frame() %>%
%            `colnames<-`(time_names) %>%
%            mutate(lag_in_horizon_time_fixed = seq(from = 0, to = 17)) %>%
%            gather_("time", "residual_acf", time_names)
%        
%        acf_null_limits <- qnorm((1 + 0.95)/2)/sqrt(52)
%        
%        p <- ggplot(plot_data) +
%            geom_point(aes(x = lag_in_horizon_time_fixed, y = residual_acf), colour = "grey") +
%            geom_line(aes(x = lag_in_horizon_time_fixed, y = residual_acf, group = time), colour = "grey") +
%            geom_hline(yintercept = 0) +
%            geom_hline(yintercept = acf_null_limits, colour = "blue", linetype = 2) +
%            geom_hline(yintercept = -acf_null_limits, colour = "blue", linetype = 2) +
%            ggtitle(paste0(models_used, "\nPrediction Time Fixed")) +
%            theme_bw()
%        
%        
%        return(p)
%    })
%
%print(plots[[1]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime2, fig.height = 9, echo = FALSE>>=
%print(plots[[2]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime3, fig.height = 9, echo = FALSE>>=
%print(plots[[3]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime4, fig.height = 9, echo = FALSE>>=
%print(plots[[4]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime5, fig.height = 9, echo = FALSE>>=
%print(plots[[5]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime6, fig.height = 9, echo = FALSE>>=
%print(plots[[6]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime7, fig.height = 9, echo = FALSE>>=
%print(plots[[7]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime8, fig.height = 9, echo = FALSE>>=
%print(plots[[8]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime9, fig.height = 9, echo = FALSE>>=
%print(plots[[9]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime10, fig.height = 9, echo = FALSE>>=
%print(plots[[10]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime11, fig.height = 9, echo = FALSE>>=
%print(plots[[11]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime12, fig.height = 9, echo = FALSE>>=
%print(plots[[12]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime13, fig.height = 9, echo = FALSE>>=
%print(plots[[13]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime14, fig.height = 9, echo = FALSE>>=
%print(plots[[14]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime15, fig.height = 9, echo = FALSE>>=
%print(plots[[15]])
%@
%
%<<FluDataResultsResidualsACFByPHWithinModelAndTime16, fig.height = 9, echo = FALSE>>=
%print(plots[[16]])
%@
%
%
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH1, echo = FALSE>>=
%## Get median within model and prediction horizon
%
%#temp <- ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model == unique(ili_prediction_log_score_diffs_long$model)[4], ]
%#temp <- temp[order(temp$prediction_horizon), ]
%
%## For each model, ...
%plots <- lapply(seq_len(16), function(models_used_inds) {
%        ## 
%        models_used_inds_in_log_score_diffs_long <- which(ili_prediction_log_score_diffs_long$model_indicator %in% models_used_inds)
%        models_used <- unique(ili_prediction_log_score_diffs_long$model[models_used_inds_in_log_score_diffs_long])
%        
%        reduced_ili_prediction_log_score_diffs_long <-
%            ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used, ]
%        ## For each time point, get acf wrt prediction horizon
%        
%        ph_names <- paste0("ph_", seq_len(52))
%        plot_data <- sapply(
%            seq_len(52),
%            function(prediction_horizon) {
%                temp <- reduced_ili_prediction_log_score_diffs_long[
%                    reduced_ili_prediction_log_score_diffs_long$prediction_horizon == prediction_horizon, ]
%                temp <- temp[order(temp$prediction_time), ]
%                acf(temp$log_score_difference_residual_from_median, plot = FALSE)$acf
%            }) %>%
%            as.data.frame() %>%
%            `colnames<-`(ph_names)
%        plot_data$lag_in_time_horizon_fixed <- seq(from = 0, to = nrow(plot_data) - 1)
%        plot_data <- gather_(plot_data, "prediction_horizon", "residual_acf", time_names)
%        
%        acf_null_limits <- qnorm((1 + 0.95)/2)/sqrt(length(unique(reduced_ili_prediction_log_score_diffs_long$prediction_time)))
%        
%        p <- ggplot(plot_data) +
%            geom_point(aes(x = lag_in_time_horizon_fixed, y = residual_acf), colour = "grey") +
%            geom_line(aes(x = lag_in_time_horizon_fixed, y = residual_acf, group = prediction_horizon), colour = "grey") +
%            geom_hline(yintercept = 0) +
%            geom_hline(yintercept = acf_null_limits, colour = "blue", linetype = 2) +
%            geom_hline(yintercept = -acf_null_limits, colour = "blue", linetype = 2) +
%            ggtitle(paste0(models_used, "\nPrediction Horizon Fixed")) +
%            theme_bw()
%        
%        
%        return(p)
%    })
%
%print(plots[[1]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH2, fig.height = 9, echo = FALSE>>=
%print(plots[[2]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH3, fig.height = 9, echo = FALSE>>=
%print(plots[[3]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH4, fig.height = 9, echo = FALSE>>=
%print(plots[[4]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH5, fig.height = 9, echo = FALSE>>=
%print(plots[[5]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH6, fig.height = 9, echo = FALSE>>=
%print(plots[[6]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH7, fig.height = 9, echo = FALSE>>=
%print(plots[[7]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH8, fig.height = 9, echo = FALSE>>=
%print(plots[[8]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH9, fig.height = 9, echo = FALSE>>=
%print(plots[[9]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH10, fig.height = 9, echo = FALSE>>=
%print(plots[[10]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH11, fig.height = 9, echo = FALSE>>=
%print(plots[[11]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH12, fig.height = 9, echo = FALSE>>=
%print(plots[[12]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH13, fig.height = 9, echo = FALSE>>=
%print(plots[[13]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH14, fig.height = 9, echo = FALSE>>=
%print(plots[[14]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH15, fig.height = 9, echo = FALSE>>=
%print(plots[[15]])
%@
%
%<<FluDataResultsResidualsACFByTimeWithinModelAndPH16, fig.height = 9, echo = FALSE>>=
%print(plots[[16]])
%@

<<FluDataResultsEmpiricalContrastsSeasonalLag, fig.height = 9, echo = FALSE>>=
ili_prediction_log_score_diffs_long <- ili_prediction_log_score_diffs_long %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )

ili_contrast_seasonal_lag <- ili_prediction_log_score_diffs_long %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
    spread_("seasonal_lag",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`,
        fixed_values = paste0("differencing_", differencing, "-periodic_", periodic, "-bw_full_", bw_full))

ggplot(ili_contrast_seasonal_lag) +
    geom_boxplot(aes(x = factor(prediction_horizon), y = contrast_value)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Seasonal Lag to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsMedianSeasonalLag, fig.height = 9, echo = FALSE>>=
ili_contrast_seasonal_lag_medians <- ili_contrast_seasonal_lag %>%
    group_by(prediction_horizon, fixed_values) %>%
    summarize(median_contrast = median(contrast_value))

ggplot(ili_contrast_seasonal_lag_medians) +
    geom_point(aes(x = factor(prediction_horizon), y = median_contrast)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Seasonal Lag to Model") +
    theme_bw()
@


<<FluDataResultsEmpiricalContrastsDifferencing, fig.height = 9, echo = FALSE>>=
ili_contrast_differencing <- ili_prediction_log_score_diffs_long %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
    spread_("differencing",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`,
        fixed_values = paste0("seasonal_lag_", seasonal_lag, "-periodic_", periodic, "-bw_full_", bw_full))

ggplot(ili_contrast_differencing) +
    geom_boxplot(aes(x = factor(prediction_horizon), y = contrast_value)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Seasonal Differencing to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsMedianDifferencing, fig.height = 9, echo = FALSE>>=
ili_contrast_differencing_medians <- ili_contrast_differencing %>%
    group_by(prediction_horizon, fixed_values) %>%
    summarize(median_contrast = median(contrast_value))

ggplot(ili_contrast_differencing_medians) +
    geom_point(aes(x = factor(prediction_horizon), y = median_contrast)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Seasonal Differencing to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsPeriodic, fig.height = 9, echo = FALSE>>=
ili_contrast_periodic <- ili_prediction_log_score_diffs_long %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
    spread_("periodic",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`,
        fixed_values = paste0("seasonal_lag_", seasonal_lag, "-differencing_", differencing, "-bw_full_", bw_full))

ggplot(ili_contrast_periodic) +
    geom_boxplot(aes(x = factor(prediction_horizon), y = contrast_value)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Periodic Kernel to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsMedianPeriodic, fig.height = 9, echo = FALSE>>=
ili_contrast_periodic_medians <- ili_contrast_periodic %>%
    group_by(prediction_horizon, fixed_values) %>%
    summarize(median_contrast = median(contrast_value))

ggplot(ili_contrast_periodic_medians) +
    geom_point(aes(x = factor(prediction_horizon), y = median_contrast)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Periodic Kernel to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsBWFull, fig.height = 9, echo = FALSE>>=
ili_contrast_bw_full <- ili_prediction_log_score_diffs_long %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
    spread_("bw_full",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`,
        fixed_values = paste0("seasonal_lag_", seasonal_lag, "-differencing_", differencing, "-periodic_", periodic))

ggplot(ili_contrast_bw_full) +
    geom_boxplot(aes(x = factor(prediction_horizon), y = contrast_value)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Full BW Parameterization to Model") +
    theme_bw()
@

<<FluDataResultsEmpiricalContrastsMedianBWFull, fig.height = 9, echo = FALSE>>=
ili_contrast_bw_full_medians <- ili_contrast_bw_full %>%
    group_by(prediction_horizon, fixed_values) %>%
    summarize(median_contrast = median(contrast_value))

ggplot(ili_contrast_bw_full_medians) +
    geom_point(aes(x = factor(prediction_horizon), y = median_contrast)) +
    geom_hline(yintercept = 0, colour = "red") +
    facet_wrap( ~ fixed_values, ncol = 1) +
    ggtitle("Effect of Adding Full BW Parameterization to Model") +
    theme_bw()
@


%<<FluDataResultsGAMM>>=
%## Outcome is y_{t, h, s, d, p, b}, where
%## t = week for which y_{t, h, s, d, p, b} is a prediction.
%##     Predictions are made each week for the 4 years in the test set.
%## h = prediction horizon.  The prediction is made from h weeks before t.
%##     For each target week, we make predictions for every horizon between 1 and 52 weeks.
%## s = seasonal lag included in model.  Either 0 or 1.
%## d = differencing used?  True/False
%## p = periodic kernel used?  True/False
%## b = bandwidth parameterization:  Diagonal or Full
%## 
%## Mean structure:
%##     E(y_{t, h, s, d, p, b}) = mu_{s, d, p, b} + spline(h)_{s, d, p, b}
%##     The spline term is in the prediction horizon, with a separate spline
%##     fit for each combination of s, d, p, and b
%## 
%## Covariance structure:
%##     We want autocorrelation in t.
%##     We may want correlation in nearby h?
%##     We want different variances for different s, d, p, b.
%## y_{t, h, s, d, p, b} = E(y_{t, h, s, d, p, b}) + alpha_ + epsilon_{t, h, s, d, p, b}
%## 
%##     Var(y_{t, h, s, d, p, b}) = blah
%##     Cov(y_{t, h, s, d, p, b}, y_{t, h, s, d, p, b}) = blah
%flu_data_results_gamm_fit <- gamm4(
%#    formula = log_score_difference ~ model + s(prediction_horizon, model, bs = "fs"), # the s term gives a separate smooth for each level of model
%    formula = log_score_difference ~ model + s(prediction_horizon, by = model), # the s term gives a separate smooth for each level of model
%    random = ,
%    data = ili_prediction_log_score_diffs_long
%)
%
%plot(flu_data_results_gamm_fit$gam,pages=1)
%summary(flu_data_results_gamm_fit$gam)
%@


%<<FluDataLogScoreDiffVsTimePlot, echo = FALSE>>=
%phs_used <- c(1, 6, 13, 26, 52)
%models_used <- c("kcde-filtering_FALSE-periodic_TRUE-bw_full")
%
%ggplot(data = ili_prediction_log_score_diffs_long[ili_prediction_log_score_diffs_long$model %in% models_used &
%                ili_prediction_log_score_diffs_long$prediction_horizon %in% phs_used, ]) +
%    geom_line(aes(x = prediction_time, y = log_score_difference)) +
%    geom_point(aes(x = prediction_time, y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    facet_wrap( ~ prediction_horizon, ncol = 1) +
%    ylim(c(-4, 4)) +
%    theme_bw()
%    
%@
%
%<<FluDataLogScoreDiffVsObsIncidencePlot, echo = FALSE>>=
%ili_prediction_log_score_diffs_long_with_obs_incidence <- ili_prediction_log_score_diffs_long %>%
%    left_join(ili_national, by = c("prediction_time" = "time"))
%
%ggplot(data = ili_prediction_log_score_diffs_long_with_obs_incidence[
%            ili_prediction_log_score_diffs_long_with_obs_incidence$model %in% models_used &
%                ili_prediction_log_score_diffs_long_with_obs_incidence$prediction_horizon %in% phs_used, ]) +
%    geom_line(aes(x = weighted_ili, y = log_score_difference)) +
%    geom_point(aes(x = weighted_ili, y = log_score_difference)) +
%    geom_hline(aes(yintercept = 0), colour = "red") +
%    facet_wrap( ~ prediction_horizon, ncol = 1) +
%    ylim(c(-4, 4)) +
%    theme_bw()
%@



<<ACFBySeasonInfluenza>>=
pit_inc_traj_acf_by_season <- rbind.fill(lapply(
        seq_len(11),
        function(season_row_ind) {
            temp <- acf(pit_incidence_trajectories[season_row_ind, ], plot = FALSE)
            return(data.frame(
                    season = rownames(pit_incidence_trajectories)[season_row_ind],
                    acf = temp$acf,
                    lag = temp$lag
                ))
        }
    ))

acf_null_limits <- qnorm((1 + 0.95)/2)/sqrt(max_prediction_horizon)

ggplot(pit_inc_traj_acf_by_season) +
    geom_point(aes(x = lag, y = acf), colour = "grey") +
    geom_line(aes(x = lag, y = acf, group = season), colour = "grey") +
    geom_hline(yintercept = 0) +
    geom_hline(yintercept = acf_null_limits, colour = "blue", linetype = 2) +
    geom_hline(yintercept = -acf_null_limits, colour = "blue", linetype = 2) +
    theme_bw()
@


<<ACFBySeasonVsFromCopulaSimInfluenza>>=
pit_inc_traj_acf_by_season <- rbind.fill(lapply(
        seq_len(11),
        function(season_row_ind) {
            temp <- acf(pit_incidence_trajectories[season_row_ind, ], plot = FALSE)
            return(data.frame(
                    season = rownames(pit_incidence_trajectories)[season_row_ind],
                    acf = temp$acf,
                    lag = temp$lag,
                    type = "observed",
                    stringsAsFactors = FALSE
                ))
        }
    ))

acf_null_limits <- qnorm((1 + 0.95)/2)/sqrt(max_prediction_horizon)


ggplot(pit_inc_traj_acf_by_season) +
    geom_point(aes(x = lag, y = acf), colour = "grey") +
    geom_line(aes(x = lag, y = acf, group = season), colour = "grey") +
    geom_hline(yintercept = 0) +
    geom_hline(yintercept = acf_null_limits, colour = "blue", linetype = 2) +
    geom_hline(yintercept = -acf_null_limits, colour = "blue", linetype = 2) +
    theme_bw()

n_sim <- 11L

clayton_copula_fit <- fitCopula(
    copula = claytonCopula(dim = max_prediction_horizon),
    data = pit_incidence_trajectories,
    method = "ml")
frank_copula_fit <- fitCopula(
    copula = frankCopula(dim = max_prediction_horizon),
    data = pit_incidence_trajectories,
    method = "ml")
t_copula_fit <- fitCopula(
    copula = tCopula(dim = max_prediction_horizon, df.fixed = TRUE),
    data = pit_incidence_trajectories,
    method = "ml")
ar1_normal_copula_fit <- fitCopula(
    copula = normalCopula(dim = max_prediction_horizon, dispstr = "ar1"),
    data = pit_incidence_trajectories,
    method = "ml")
toep_normal_copula_fit <- fitCopula(
    copula = normalCopula(dim = max_prediction_horizon, dispstr = "toep"),
    data = pit_incidence_trajectories,
    method = "ml")

sim_pit_seq_acf <- rbind.fill(lapply(
    c(frank_copula_fit, clayton_copula_fit, t_copula_fit, toep_normal_copula_fit),
    function(copula_fit) {
        rbind.fill(lapply(
            seq_len(n_sim),
            function(sim_ind) {
                predictive_copula <- copula_fit@copula
                ## Note that the variance estimate for parameters is low; at least we're accounting for some uncertainty though...
                orig_params <- predictive_copula@parameters
                predictive_copula@parameters <- rmvnorm(1, copula_fit@estimate, sigma = copula_fit@var.est)[1, ]
                random_params <- predictive_copula@parameters
                if(identical(class(predictive_copula)[1], "normalCopula")) {
                    ## randomly generated parameters above may not yield a positive definite correlation matrix; correct
                    while(min(eigen(getSigma(predictive_copula))$values) < 0.00001) {
                        predictive_copula@parameters <- copula:::makePosDef(getSigma(predictive_copula))[1, 1 + seq_along(predictive_copula@parameters)]
                    }
                }
                
                sim_sequence <- rCopula(1, predictive_copula)[1, ]
                
                temp <- acf(sim_sequence, plot = FALSE)
                return(data.frame(
                    sim_ind = sim_ind,
                    acf = temp$acf,
                    lag = temp$lag,
                    type = class(copula_fit@copula)[[1]],
                    stringsAsFactors = FALSE
                ))
            }
        ))
    }
))

pit_inc_traj_acf_by_season$sim_ind <- pit_inc_traj_acf_by_season$season
combined_acfs <- rbind.fill(pit_inc_traj_acf_by_season, sim_pit_seq_acf)

ggplot(combined_acfs) +
    geom_point(aes(x = lag, y = acf), colour = "grey") +
    geom_line(aes(x = lag, y = acf, group = sim_ind), colour = "grey") +
    geom_hline(yintercept = 0) +
    geom_hline(yintercept = acf_null_limits, colour = "blue", linetype = 2) +
    geom_hline(yintercept = -acf_null_limits, colour = "blue", linetype = 2) +
    facet_wrap(~type, ncol = 1) +
    theme_bw()
@


\section{Future Work}


Hall, Racine, and Li\cite{hall2004crossvalidationKCDE} show that when
cross-validation is used to select the bandwidth parameters in KCDE using product kernels, the
estimated bandwidths corresponding to irrelevant conditioning variables tend to
infinity asymptotically as the sample size increases.  They discuss the fact
that similar results could be obtained for linear combinations of
continuous variables if a full bandwidth matrix were used.  Our approach for
obtaining kernels that can be used with mixed discrete and continuous variables
opens up an opportunity to extend this analysis to that case; we have not
pursued this mathematical analysis here.

The above results regarding the inclusion of irrelevant conditioning variables
hold asymptotically as the sample size increases.  However, in practice, data
set sizes are often limited.  In other modeling settings where some conditioning
variables may not be informative, shrinkage methods are often helpful.  These
methods could be incorporated into a kernel-based approach by imposing a penalty
on the elements of the bandwidth matrix; in particular, we suggest that a
penalty on the inverse of the bandwidth matrix encouraging it to have small
eigenvalues could be helpful.  Another alternative would be to pursue the
Bayesian framework, using Dirichlet process mixtures with an informative prior
on the mixture component covariance matrices.

We could also make some tweaks to our implementation of KCDE.  Locally linear --
help address edge effects.

Ensembles -- either ensembles of KCDE and/or include as a component in an
ensemble.  Also Bayesian model averaging.  Return to discussion of bias/variance
trade-off?

%\bibliographystyle{plainnat}
\bibliography{kde-bib}


\end{document}