\documentclass[times, doublespace]{simauth}
%\documentclass[times]{simauth}
%\documentclass[]{article}

%\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}

\usepackage[colorlinks=true, urlcolor=citecolor, linkcolor=citecolor, citecolor=citecolor]{hyperref}

% Add history information for the article if required
%\history{Received August 1, 2010;
%revised October 1, 2010;
%accepted for publication November 1, 2010}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{color}
\usepackage[list=off]{caption} % list=off option prevents errors when using math environments within captions
%\usepackage{natbib}
%\usepackage{cite}

\include{GrandMacros}
\newcommand{\cdf}{{c.d.f.} }
\newcommand{\pdf}{{p.d.f.} }
\newcommand{\ind}{\mathbb{I}}

\begin{document}
%\documentclass[Crown, sagev]{sagej}

\runninghead{E. L. Ray et al.}

\title{Infectious disease prediction with\\
kernel conditional density estimation}

%\author{Evan L. Ray\affilnum{1},
%Krzysztof Sakrejda\affilnum{1},
%Stephen A. Lauer\affilnum{1} and
%%Michael Johansen\affilnum{2} and
%Nicholas G. Reich\affilnum{1}}

% List of authors, with corresponding author marked by asterisk
\author{Evan L. Ray\affil{a}\corrauth,
Krzysztof Sakrejda\affil{a},
Stephen A. Lauer\affil{a},
Michael A. Johansson\affil{b}\ and
Nicholas G. Reich\affil{a}}

\address{
\affilnum{a}Department of Biostatistics and Epidemiology, School of Public Health and Health Sciences, University of Massachusetts, Amherst, 415 Arnold House, 715 N. Pleasant Street, Amherst, MA 01003, USA\\
\affilnum{b}Dengue Branch, Division of Vector-Borne Infectious Diseases, Centers for Disease Control and Prevention, San Juan, Puerto Rico, USA
}

\corraddr{Department of Biostatistics and Epidemiology, School of Public Health
and Health Sciences, University of Massachusetts, Amherst, 415 Arnold House, 715
N. Pleasant Street, Amherst, MA 01003, USA.  Email: elray@umass.edu}


\begin{abstract}
Creating statistical models that generate accurate predictions of 
infectious disease incidence over multiple time points is a challenging
problem whose solution could benefit public health decision makers.
We develop a new approach to this problem using
kernel conditional density estimation (KCDE) and copulas.  We obtain
predictive distributions for incidence in individual weeks using KCDE and tie those
distributions together into joint distributions using copulas. This strategy enables us to create
predictions for the timing of and incidence in the peak week of the season.
Our implementation of KCDE incorporates two novel kernel components: a periodic
component that captures seasonality in disease incidence, and a component that
allows for a full parameterization of the bandwidth matrix with discrete
variables.  We demonstrate via simulation that a fully parameterized
bandwidth matrix can be beneficial for estimating conditional densities.
We apply the method to predicting dengue fever and influenza, and compare to
a seasonal autoregressive integrated moving average
(SARIMA) model and a previously published generalized linear model for infectious disease
incidence known as HHH4.  KCDE outperforms the baseline methods for
predictions of dengue incidence in individual weeks.  KCDE also offers more
consistent performance than the baseline models for predictions of
incidence in the peak week, and is comparable to the baseline models on the
other prediction targets.  Using the periodic kernel function led to better
predictions of incidence.  Our approach and extensions of
it could yield improved predictions for public health decision makers,
particularly in diseases with heterogeneous seasonal dynamics such as dengue fever.
\end{abstract}

\keywords{copula, dengue fever, infectious disease, influenza, kernel
conditional density estimation, prediction}

\maketitle

<<knitrGlobalSetup, echo = FALSE>>=
library(cdcfluview)

library(stringr)

library(reshape2)
library(plyr)
suppressMessages(library(dplyr))
library(tidyr)
suppressMessages(library(lubridate))

suppressMessages(library(ggplot2))
library(grid)

suppressMessages(library(forecast))
library(kcde)
suppressMessages(suppressWarnings(library(pdtmvn)))

get_legend_grob <- function(x) {
  data <- ggplot2:::ggplot_build(x)
  
  plot <- data$plot
  panel <- data$panel
  data <- data$data
  theme <- ggplot2:::plot_theme(plot)
  position <- theme$legend.position
  if (length(position) == 2) {
    position <- "manual"
  }
  
  legend_box <- if (position != "none") {
    ggplot2:::build_guides(plot$scales, plot$layers, plot$mapping, 
      position, theme, plot$guides, plot$labels)
  } else {
    ggplot2:::zeroGrob()
  }
  if (ggplot2:::is.zero(legend_box)) {
    position <- "none"
  }
  else {
    legend_width <- gtable:::gtable_width(legend_box) + theme$legend.margin
    legend_height <- gtable:::gtable_height(legend_box) + theme$legend.margin
    just <- valid.just(theme$legend.justification)
    xjust <- just[1]
    yjust <- just[2]
    if (position == "manual") {
      xpos <- theme$legend.position[1]
      ypos <- theme$legend.position[2]
      legend_box <- editGrob(legend_box, vp = viewport(x = xpos, 
        y = ypos, just = c(xjust, yjust), height = legend_height, 
        width = legend_width))
    }
    else {
      legend_box <- editGrob(legend_box, vp = viewport(x = xjust, 
        y = yjust, just = c(xjust, yjust)))
    }
  }
  return(legend_box)
}

opts_chunk$set(cache = TRUE)
#opts_chunk$set(cache = TRUE, autodep = TRUE)
#opts_chunk$set(cache = FALSE)
@

<<LoadFluData, echo = FALSE>>=
# junk <- capture.output({
#    usflu <- suppressMessages(get_flu_data("national", "ilinet", years=1997:2015))
# })
usflu <- read.csv("../../data-raw/usflu.csv", stringsAsFactors = FALSE)
ili_national <- suppressWarnings(transmute(usflu,
    region.type = REGION.TYPE,
    region = REGION,
    year = YEAR,
    week = WEEK,
    weighted_ili = as.numeric(X..WEIGHTED.ILI)))
ili_national$time <- ymd(paste(ili_national$year, "01", "01", sep = "-"))
week(ili_national$time) <- ili_national$week
ili_national$time_index <- seq_len(nrow(ili_national))

## Season column: for example, weeks of 2010 up through and including week 30 get season 2009/2010;
## weeks after week 30 get season 2010/2011
ili_national$season <- ifelse(
    ili_national$week <= 30,
    paste0(ili_national$year - 1, "/", ili_national$year),
    paste0(ili_national$year, "/", ili_national$year + 1)
)

## Season week column: week number within season
ili_national$season_week <- sapply(seq_len(nrow(ili_national)), function(row_ind) {
        sum(ili_national$season == ili_national$season[row_ind] &
                ili_national$time_index <= ili_national$time_index[row_ind])
    })


## Subset to data actually used in this analysis -- up through end of 2014.
ili_national <- ili_national[ili_national$year <= 2014, , drop = FALSE]

## cutoff time for training data
ili_train_cutoff_time <- ili_national$time[max(which(ili_national$year == 2010))]
@

<<LoadDengueData, echo = FALSE>>=
## The following relative path assumes the working directory is inst/article/
dengue_sj <- read.csv("../../data-raw/San_Juan_Testing_Data.csv")

## convert dates
dengue_sj$time <- ymd(dengue_sj$week_start_date)

## cutoff time for training data
dengue_train_cutoff_time <- dengue_sj$time[max(which(dengue_sj$season == "2008/2009"))]
@


\section{Introduction}
\label{sec:Intro}

With the maturation of digital disease surveillance systems in recent years, 
accurate and real-time infectious disease prediction has become an achieveable goal in many contexts.
These predictions provide valuable information to
public health officials planning disease prevention and control measures
\cite{Manheim:2016ur}.
For example, interventions designed to reduce person-to-person transmission of disease
have been associated with diminished outbreak intensity 
\cite{hatchett2007interventionsIntensity1918flu}.  Accurate 
predictions can help target such interventions more effectively.

In this work, we use a semi-parametric approach that combines a non-parametric
method for conditional density estimation referred to as kernel conditional
density estimation (KCDE) with a parametric method for modeling joint dependence
structures known as copulas.  We apply this method to make predictions for three targets
chosen by the CDC as relevant to public health:

\begin{enumerate}
  \item Incidence $h$ time steps in the future (at ``prediction horizon'' $h$).  
  \item Timing of the peak week of the current season, $w^* \in \{1, \ldots, W\}$, where $W$ is the total number of weeks in the season.
  \item Binned incidence in the peak week of the current season.
\end{enumerate}

These quantities have emerged as being targets of particular utility in making planning decisions 
\cite{PandemicPredictionandForecastingScienceandTechnologyInteragencyWorkingGroup2015Announcement,
EpidemicPredictionInitiative2015Index}.

We model the first of these prediction targets directly; predictions for the second and third prediction targets are derived from a joint predictive distribution of incidence in each remaining week of the season.  Using data available up through time $t^*$, we employ KCDE
to obtain separate predictive distributions for disease incidence in each subsequent week of the season.  
We then combine those marginal distributions using copulas to obtain joint predictive distributions for the
trajectory of incidence over the following weeks.  Without a techinque
like copulas to introduce correlation among week-specific predictions
the predictions would not realistically represent the time-series nature
of infectious disease dynamics.  Predictive
distributions relating to the timing of, and incidence at, the peak week can be
obtained from this joint predictive distribution.  Methods combining non-parametric
estimates of marginal densities with copulas have been considered previously for
other applications such as economic time series
\cite{patton2012reviewCopulaEconomicTimeSeries}.

In addition to the novel application of these methods to predicting
disease incidence, our contributions include the use of a periodic kernel
specification to capture seasonality in disease incidence and a method for
obtaining multivariate kernel functions that handle discrete data while allowing
for a fully parameterized bandwidth matrix.  
Previous implementations of kernel methods involving discrete variables have
employed a kernel function that is a product of univariate kernel functions
\cite{aitchison1976multivariateBinaryKernel,
bowman1980ConsistencyKernelCategorical,
grund1993kernelCellProb,
hall2004crossvalidationKCDE,
hall2007nonparametricRegressionIrrelevantRegressors,
li2003nonparametricEstDistnsCategoricalContinuous,
li2008nonparametricConditionalCDFQuantile,
ouyang2006crossvalidationEstDistnCategorical,
racine2004kernelEstConditionalDistns}.
This approach forces the
kernel function to be oriented in line with the coordinate axes.  
Motivated by results showing that multivariate
kernel functions with a bandwidth parameterization allowing for
flexible orientations can result in improved continuous density estimates
\cite{duong2005crossvalidationBandwidthMultivariateKDE}, we introduce an
approach that allows for flexible orientation of discrete kernels by
discretizing an underlying continuous kernel function.

%Copulas present an attractive strategy
%for estimating the joint distribution of moderate to high dimensional random
%vectors.  Using a parametric dependence model, a copula
%ties separate marginal distribution estimates (in our case, obtained by KCDE) together into a joint
%distribution.  
%This approach addresses the limitation of local methods such as KCDE that their performance may not
%scale well with the dimension of the vector whose distribution is being estimated (\cite{HastieTibshiraniESL}).  This
%is particularly relevant in our application, where we wish to obtain joint
%predictive distributions for disease incidence over the course of many weeks.

In a time-series context, KCDE is a local method in the sense that the density
estimate for observations at future time points conditional on covariates is a weighted
combination of contributions from previous observations with
similar covariate values.  Using such local methods is a natural idea in predicting
nonlinear systems because it imposes little structure on the assumed
relationship between conditioning and outcome variables. 
The covariates we condition on could include historical observations from the time
series we are predicting as well as other variables such as weather or
the time of the season.

Applications range from
similar infectious disease settings where nearest neighbors regression has been used to make point predictions for incidence of
measles \cite{sugihara1990nonlinearForecasting} and
influenza \cite{viboud2003predictionInfluenzaMoA} to sports analytics
where a version of nearest neighbors regression predicts the career trajectories
of current NBA players \cite{SilverCARMELOPrediction}.
We note that KCDE can be seen as a distribution-based counterpart of nearest neighbors regression. For example, the point prediction
obtained from nearest neighbors regression is equal to the expected value of the predictive distribution
obtained from KCDE if a particular kernel function is used in the formulation of
KCDE ({e.g.}, Hastie \etal \cite{HastieTibshiraniESL} discuss the connection
between nearest neighbors and kernel methods for regression).

KCDE has not previously been applied to obtain predictive distributions for
infectious disease incidence, but it has been successfully used for prediction in other settings such as survival
time of lung cancer patients \cite{hall2004crossvalidationKCDE}, female
labor force participation \cite{hall2004crossvalidationKCDE}, bond yields
and value at risk in financial markets \cite{fan2004crossvalidationKCDE},
and wind power \cite{jeon2012KCDEWindPower}, among others.  Similar methods
can also be formulated in the Bayesian framework.  For example, Zhou \etal
\cite{zhou2015DirichletProcessCopulaAmphibianDiseaseArrival} model the time to
arrival of a disease in amphibian populations using Dirichlet processes and copulas.

There is also a long history of using other modeling approaches for infectious
disease prediction, including agent-based models, compartmental models and
more generic regression-based time series models such as seasonal autoregressive
integrated moving average (SARIMA) models among others.  Brown \etal [in preparation] and
Unkel \etal \cite{unkel2012statisticalInfectiousDiseasePredictionReview} are
recent reviews of work on forecasting infectious disease, and describe these alternative
approaches in more detail.

Little research has been done comparing the predictive performance of more detailed and 
disease-mechanistic modeling approaches (agent-based or compartmental
models) to more generic models (regression or SARIMA).  One difficulty in making
comparisons to agent-based models is that these models are often 
highly parameterized and difficult to independently reproduce or replicate.
An additional challenge with agent-based and compartmental models is that
expert knowledge is required to tailor them to the specific disease being
modeled, and details of the assumed model specification can
have a large impact on the quality of predictions
\cite{grad2012cholera}.

One of the most well-developed modern statistical frameworks for infectious
disease prediction is the ``HHH4'' model
\cite{Held2005HHHIntro, Paul2008MultivariateHHH4, Held2012HHHSeasonality, meyeretal2016SpatioTemporalAnalysisSurveillance},
a specific variation of a generalized linear model developed for infectious
disease. Another commonly used and widely studied approach is the seasonal
autoregressive integrated moving average (SARIMA) model. However, both of these approaches 
have limitations that also hamper generalizability. The HHH4 model specifies a
discrete distribution for the observed incidence measure, an appropriate 
assumption for some data sets, but not for others. The standard SARIMA
specification is based on continuous distributions which means that it cannot be
directly applied to modeling discrete case count data if low case counts are
observed \cite{unkel2012statisticalInfectiousDiseasePredictionReview}.


% Each of these classes of models has advantages and disadvantages.
% Approaches such as agent-based and compartmental models attempt to represent
% the disease process by tracking the disease status of
% individuals or groups of individuals, possibly in addition to disease reservoirs
% and vectors such as the water supply or mosquitoes.  
% This potential for fidelity to the disease process makes these models powerful tools, but comes with drawbacks.
% One disadvantage of these approaches is that they require expert knowledge to tailor them to the disease at hand: the
% same compartmental or agent-based model specification will not be applicable across a wide variety of
% diseases.  The details of the assumed model specification are important and can
% have a large impact on the quality of predictions obtained from these models
% (\cite{grad2012cholera}).
% Additionally, inference in fine-grained agent-based and compartmental models
% can entail considerable computational costs.  For example, recent
% realistic compartmental models for dengue fever strain the limits of modern
% computational resources (cite something).

% A variety of more generic regression-based and time series modeling frameworks
% have also been applied to infectious disease.  These approaches sacrifice some of the
% representative power of agent-based and compartmental models, but may
% offer advantages in terms of ease of use.  One of the most well-developed
% approaches in this category is the HHH4 model (\cite{Held2005HHHIntro,
% Paul2008MultivariateHHH4, Held2012HHHSeasonality}), a specific variation of a
% generalized linear model developed for infectious disease.
% Another commonly used approach in this category is the seasonal autoregressive
% integrated moving average (SARIMA) model.  While approaches along these lines
% have been used with a variety of different diseases, details of the model
% specification are still important.  For example, the HHH4 model specifies a
% discrete distribution for the observed incidence measure.  This is appropriate
% for some data sets, but not for others.  On the other hand, the standard SARIMA
% specification is based on continuous distributions which means that it cannot be
% directly applied to modeling discrete case count data if low case counts are
% observed (\cite{unkel2012statisticalInfectiousDiseasePredictionReview}).
% As we will see, the mechanisms used in these models to capture seasonality may also be more or less appropriate for
% some diseases.

Several key features distinguish our approach from existing methods commonly
used for predicting infectious disease incidence.
First, we generate full predictive distributions to fully  
characterize uncertainty in the predictions.
Compared to point predictions, this gives decision makers additional information in
situations where the predictive distribution is skewed or has multiple modes.
Second, unlike many methods common in the infectious disease literature,
KCDE makes minimal assumptions about the
underlying system governing disease dynamics. This flexibility makes KCDE
suitable for application to a wide variety of time series, including diseases
with different latent dynamics.  
Third, the method can easily be used with either discrete or continuous data 
by substituting one kernel function specification for another.

One of the few previous methods that shares these characteristics is in Brooks \etal
\cite{brooks2015empiricalBayes}, who propose an Empirical Bayes method that also gives
a joint predictive distribution for incidence in each remaining week in the season.
Their approach contrasts with ours in that it takes a ``top-down'' approach to
constructing that predictive distribution, saying that the general trend in
incidence over the course of the season will look like a modified version of the
season-long trend in incidence from a previous season.  On the other hand, the approach
we discuss in the present article is a ``bottom-up'' method that first constructs
predictive distributions for incidence in individual weeks and then ties those
marginal distributions together to obtain a joint distribution for incidence in all
weeks of the season.  It seems likely that both of these approaches have something to
offer in predicting disease incidence; we will return to this point in the conclusions.

The remainder of this article is organized as follows.  First, we describe our
approach to prediction using KCDE and copulas.  Next, we present the
results of a simulation study comparing the performance of KCDE for
estimating discrete conditional distributions using a fully parameterized
bandwidth matrix and a diagonal bandwidth matrix.  We then illustrate our methods by applying them to predicting
disease incidence in two data sets: one with a discrete measure
of weekly incidence of dengue fever in San Juan, Puerto Rico and a
second with a continuous measure of weekly incidence of influenza in the United
States.  We conclude with a discussion of these results.

\section{Method Description}
\label{sec:Methods}

Suppose we observe a measure $z_t$ of disease incidence at evenly spaced times
indexed by $t = 1, \ldots, T$.  Our goal is to obtain predictions relating to
incidence after time $T$ using time series of incidence up to time $T$
as well as time series of covariates up to time $T$.  

We allow the incidence measure to be either
continuous or discrete and use the term density to refer to either the probability density function or probability mass function as appropriate.  We will use a colon notation to
specify vectors: for example, $\bz_{s:t} = (z_s, \ldots, z_t)$.  The
variable $t^* \in \{1, \ldots, T\}$ will be used to represent a time at which we
desire to form a predictive distribution, using observed data up through $t^*$
to predict incidence after $t^*$.  When we apply the method to
perform prediction for incidence after time $T$, $t^*$ is equal to $T$; however, $t^*$
takes other values in the estimation procedure we describe below.

We introduce the overall structure of our model here and describe its
components and parameter estimation in more detail in the
following Subsections.
At time $t^*$, our model approximates $f(\bz_{(t^* + 1):(t^* + H_{t^*})} \mid t^*, \bz_{1:t^*})$
by conditioning only on the time at which we are making the predictions and
observed incidence at a few recent time points with lags given by the non-negative integers $l_1, \ldots, l_M$:
$f(\bz_{(t^* + 1):(t^* + H_{t^*})} \mid t^*, z_{t^* - l_1}, \ldots, z_{t^* -
l_M})$.  For notational simplicity, we take $l_M$ to be the largest of these
lags.  The model represents this density as follows:
\begin{align}
&f(z_{(t^* + 1):(t^* + H_{t^*})} \mid t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}) = \nonumber \\
&\qquad c^{H_{t^*}}\{f^{1}(z_{t^* + 1} \mid t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^1), \ldots, f^{H_{t^*}}(z_{t^* + H_{t^*}} \mid t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^{H_{t^*}}) ; \bxi^{H_{t^*}}\}. \label{eqn:ModelKCDECopula}
\end{align}
Here, each $f^{h}(z_{t^* + h} \mid t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M};
\btheta^h)$ is a predictive density for one prediction horizon obtained through KCDE.  The
distribution for each prediction horizon depends on a separate parameter vector $\btheta^h$.
The function $c^{H_{t^*}}(\cdot)$ is a copula
used to tie these marginal predictive densities together into a joint predictive
density, and depends on parameters $\bxi^{H_{t^*}}$.  In our
applications, we will obtain a separate copula fit for each trajectory length
$H_{t^*}$ of interest for the prediction task.

Let $W$ denote the number of time
points in a disease season ({e.g.}, $W = 52$ if we have weekly data).
For each time $t^*$, let $S_{t^*}$ denote the time index of the last time
point in the \textit{previous} season, so that the times in the same season
as $t^*$ are indexed by $S_{t^*} + 1, \ldots, S_{t^*} + W$.
Finally, let $H_{t^*} = W - (t^* - S_{t^*})$ denote the number of time points
after $t^*$ that are in the same season as $t^*$.  $H_{t^*}$ gives the largest
prediction horizon for which we need to make a prediction in order to obtain
predictions for all remaining time points in the season.

We obtain predictive distributions for each of three prediction targets.
We will model the first of these prediction targets directly and frame the
second and third as suitable integrals of a predictive
distribution $f(\bz_{({t^*} + 1):({t^*} + H_{t^*})} \mid {t^*}, \bz_{1:{t^*}})$
for the trajectory of incidence over all remaining weeks in the season:
\begin{enumerate}
  \item Incidence in a single future week with prediction horizon $h \in \{1, ..., W\}$:
    \begin{align}
    &f(z_{{t^*} + h} \mid {t^*}, \bz_{1:{t^*}}) \nonumber
    \end{align} %    &\quad = \int \cdots \int f(\bz_{(T + 1):(T + H_T)} \mid T, \bz_{1:T}) \, d z_{T + 1} \cdots \mathrm{d} z_{T + h - 1} \, \mathrm{d} z_{T + h + 1} \cdots \mathrm{d} z_{T + H_T}
  \item Timing of the peak week of the current season, $w^* \in \{1, \ldots, W\}$:
    \begin{align}
    &P(\text{Peak Week} = w^*) = P(Z_{S_{t^*} + w^*} = \max_{w} Z_{S_{t^*} + w} \mid {t^*}, \bz_{1:{t^*}}) \nonumber \\
    &\quad = \int_{\{\bz_{({t^*} + 1):({t^*} + H_{t^*})}: z_{S_{t^*} + w^*} = \max_{w} z_{S_{t^*} + w} \}} f(\bz_{({t^*} + 1):({t^*} + H_{t^*})} \mid {t^*}, \bz_{1:{t^*}}) \, \mathrm{d} \bz_{({t^*} + 1):({t^*} + H_{t^*})}. \label{eqn:PeakPredTimingIntegral}
    \end{align}
  \item Binned incidence in the peak week of the current season:
    \begin{align}
    &P(\text{Incidence in Peak Week} \in [a, b)) = P(a \leq \max_{w} Z_{S_{t^*} + w} < b \mid {t^*}, \bz_{1:{t^*}}) \nonumber \\
    &\quad = \int_{\{(\bz_{({t^*} + 1):({t^*} + H_{t^*})}): a \leq \max_{w} z_{S_{t^*} + w} < b\}} f(\bz_{({t^*} + 1):({t^*} + H_{t^*})} \mid {t^*}, \bz_{1:{t^*}}) \, \mathrm{d} \bz_{({t^*} + 1):({t^*} + H_{t^*})}. \label{eqn:PeakPredIncidenceIntegral}
    \end{align}
\end{enumerate}
In practice, we use Monte Carlo integration to evaluate
the integrals in Equations \eqref{eqn:PeakPredTimingIntegral} and
\eqref{eqn:PeakPredIncidenceIntegral} by sampling incidence trajectories from
the joint predictive distribution.

\subsection{KCDE for Predictive Densities at Individual Prediction Horizons}
\label{subsec:Methods:KCDE}

We now discuss the use of KCDE to obtain $f^{h}(z_{t^* + h} \mid t^*, z_{t^* -
l_1}, \ldots, z_{t^* - l_M}; \btheta^h)$, the predictive density for disease
incidence at a particular horizon $h$ after time $t^*$.  To simplify the
notation, we define two new variables: $Y_t^{h} = Z_{t + h}$ represents the
prediction target relative to time $t$, and $\bX_t = (t, Z_{t - l_1}, \ldots,
Z_{t - l_M})$ represents the vector of predictive variables relative to time
$t$.  With this notation, the distribution we wish to estimate is
$f^{h}(y_{t^*}^{h} \mid \bx_{t^*}; \btheta^h)$.

To estimate this distribution, we use the observed data to form the
pairs $(\bx_t, y_t^{h})$ for all $t = 1 + l_M, \ldots, T - h$
(for smaller values of $t$ there are not enough observations before $t$ to form
$\bx_t$ and for larger values of t there are not enough observations after $t$ to form
$y_t^{h}$).  We then regard these pairs as a (dependent) sample from the joint
distribution of $(\bX, Y^h)$ and estimate the conditional distribution of $Y^h \mid \bX$ via KCDE:
\begin{align}
\hat{f}^h(y^h_{t^*} \mid \bx_{t^*}) &= \frac{\sum_{t \in \btau} K^{\bX, Y}\left\{(\bx_{t^*}, y^h_{t^*}), (\bx_t, y^h_t); \btheta^h \right\}}{\sum_{t \in \btau}K^{\bX}(\bx_{t^*}, \bx_t ; \btheta^h)} \label{eqn:KCDEDefinition} \\
\quad &= \sum_{t \in \btau} \zeta^h_{t^*, t} K^{Y \mid \bX}(y^h_{t^*}, y^h_t \mid \bx_{t^*}, \bx_t; \btheta^h) \text{, where} \label{eqn:KDEwt} \\
\zeta^h_{t^*, t} &= \frac{ K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h) }{\sum_{s \in \btau} K^{\bX}(\bx_{t^*}, \bx_{s}; \btheta^h) }. \label{eqn:KCDEWeightsDef}
\end{align}

%&\quad = \frac{\sum_{t \in \btau} K^{Y \mid \bX}(y^h_{t^*}, y^h_t \mid \bx_{t^*},
% \bx_t; \btheta^h) K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h)}{\sum_{t \in \btau} K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h) } \label{eqn:KDESubKDEJtMarginal} \\

Here we are working with a slightly restricted specification in which
the kernel function $K^{\bX, Y}$ can be written as the product of $K^{\bX}$ and $K^{Y\mid\bX}$.
With this restriction, we can
interpret $K^{\bX}$ as a weighting function determining how much each observation
$(\bx_t, y^h_t)$ contributes to our final density estimate according to how
similar $\bx_t$ is to the value $\bx_{t^*}$ that we are conditioning on.
These weights are the $\zeta^h_{t^*, t}$ in Equations~\eqref{eqn:KDEwt} and
\eqref{eqn:KCDEWeightsDef}.
$K^{Y \mid \bX}$ is a density function that contributes
mass to the final density estimate near $y^h_t$.  The
parameters $\btheta^h$ control the locality and orientation of the weighting
function and the contributions to the density estimate from each observation.
In Equations \eqref{eqn:KCDEDefinition} through \eqref{eqn:KCDEWeightsDef},
$\btau \subseteq \{(1 + l_M), \ldots, (T - h)\}$ indexes the subset of
observations used in obtaining the conditional density estimate; we return to
how this subset of observations is defined in the discussion of estimation
below.

We take the kernel function $K^{Y, \bX}$ to be a product kernel with one
component being a periodic kernel in time and the other component capturing the
remaining covariates, which are measures of disease incidence:
\begin{align}
&K^{\bX, Y}\left\{(\bx_{t^*}, y^h_{t^*}), (\bx_t, y^h_t); \btheta^h \right\} \nonumber \\
&\quad = K^{\text{per}}(t^*, t; \btheta^h_{\text{per}}) K^{\text{inc}}\{(z_{t^* - l_1}, \ldots, z_{t^* - l_M}, z_{t^* + h}), (z_{t - l_1}, \ldots, z_{t - l_M}, z_{t + h}); \btheta^h_{\text{inc}}\}. \nonumber
\end{align}
Here we have set $\btheta^h = (\btheta^h_{\text{per}}, \btheta^h_{\text{inc}})$ where $\btheta^h$ encompasses parameters both about the periodicity and incidence.
%&\quad = K^{\bX, Y}\left\{(t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}, z_{t^* +
% h}), (t, z_{t - l_1}, \ldots, z_{t - l_M}, z_{t + h}); \btheta^h \right\} \nonumber \\

The periodic kernel function was originally developed in the
literature on Gaussian Processes \cite{mackay1998introductionGP}, and is
defined by
\begin{equation}
K^{\text{per}}(t^*, t; \rho^h, \eta^h) = \exp\left[- \frac{\sin^2\{\rho^h (t^* - t)\}}{2(\eta^h)^2} \right]. \label{eqn:PeriodicKernel}
\end{equation}
We illustrate this kernel function in Figure \ref{fig:PeriodicKernelPlot}. 
It has two parameters: $\btheta^h_{\text{per}} = (\rho^h, \eta^h)$, where $\rho^h$ determines the length of the
periodicity and $\eta^h$ determines the strength and locality
of this periodic component in computing the observation weights $\zeta_{t^*, t}^h$.
In our applications, we have fixed $\rho^h = \pi / 52$, so that the kernel has
period of length 1 year with weekly data.  Using this periodic kernel provides a
mechanism to capture seasonality in disease incidence by allowing the
observation weights to depend on the similarity of the time of year that an
observation was collected and the time of year at which we are making a prediction.

The second component of our kernel is a multivariate kernel incorporating
all of the other variables in $\bx_t$ and $y_t^h$.  In our
applications, these variables are measures of incidence; for brevity of
notation, we collect them in the column vector
$\tilde{\bz}_t = (z_{t - l_1}, \ldots, z_{t - l_M}, z_{t + h})'$.
These incidence measures are continuous in the application to influenza and
discrete case counts in the application to dengue fever.  In the continuous
case, we have used a multivariate log-normal kernel function parameterized in
terms of its mode rather than its mean (Figure~\ref{fig:PeriodicKernelPlot}).
Using the mode ensures that the contribution to the conditional density
is largest near $z_{t + h}$.
This kernel specification automatically handles the restriction that counts are
non-negative, and approximately captures the long tail in disease incidence that
we will illustrate in the applications Section below.  This kernel function has
the following functional form:
\begin{equation}
K^{\text{inc}}_{\text{cont}}(\tilde{\bz}_{t^*}, \tilde{\bz}_{t}; \bB) = \frac{\exp\left[ -\frac{1}{2} \{\log(\tilde{\bz}_{t^*}) - \log(\tilde{\bz}_t) - \bB \b1\}' \bB^{-1} \{\log(\tilde{\bz}_{t^*}) - \log(\tilde{\bz}_t) - \bB \b1\} \right]}{(2 \pi)^{\frac{M+1}{2}} \vert \bB \vert^{\frac{1}{2}} z_{t^* + h} \prod_{m = 1}^M z_{t^* - l_m} }
\end{equation}

In this expression, $\b1$ is a column vector of ones.
The matrix $\bB$ is a bandwidth matrix that
controls the orientation and scale of the kernel function.  Subtracting $\bB \b1$ in the numerator has the effect of placing the mode of the kernel function at $z_{t + h}$.  This bandwidth
matrix is parameterized by $\btheta^h_{\text{inc}}$.  In this work we have considered
two parameterizations: a diagonal bandwidth matrix, and a fully parameterized
bandwidth based on the Cholesky decomposition.  To obtain the
discrete kernel (Figure~\ref{fig:PeriodicKernelPlot}), we integrate an
underlying continuous kernel function over hyper-rectangles containing the
points in the range of the discrete random variable (see supplement for details).

We estimate the bandwidth parameters $\btheta^h$ by numerically maximizing the
cross-validated log score of the predictive distributions for the observations
in the training data.  For a random variable $Y$ with observed value $y$ the
log score of the predictive distribution $f_Y$ is $\log\{f_Y(y)\}$.  A larger log
score indicates better model performance.  In obtaining the cross-validated log score for the
predictive distribution at time $t^*$, we leave the year
of training data before and after the time $t^*$ out of the set $\btau$ in
Equations~\eqref{eqn:KCDEDefinition} through \eqref{eqn:KCDEWeightsDef}.
Our primary motivation for using the log score as the optimization target during
estimation is that this is the criteria that has been used to evaluate and
compare prediction methods in two recent government-sponsored infectious disease
prediction contests
\cite{PandemicPredictionandForecastingScienceandTechnologyInteragencyWorkingGroup2015Announcement,
EpidemicPredictionInitiative2015Index}.
We apply our method to the data sets
from those competitions in the applications section below, and report log
scores to facilitate comparisons with other results from those
competitions that may be published in the future.
In general, the log score is a strictly proper scoring rule; i.e., its
expectation is uniquely maximized by the true predictive
distribution \cite{gneiting2007strictlyProperScoringRules}.
However, its use as an optimization criterion has been criticised for being
sensitive to outliers \cite{gneiting2007strictlyProperScoringRules}.  In
the kernel density estimation literature, this approach to estimation is referred to
as likelihood cross-validation, and similar criticisms have been made regarding
its performance in handling outliers and estimating heavy-tailed distributions
\cite{schuster1981nonconsistencyLCVforKDE,
scott1981MonteCarloStudyNonparDensityEstimators}.  This is relevant to application
of the method to infectious disease prediction, as the distribution of disease
incidence tends to be skewed right with a long upper tail.  It is possible that
the use of cross-validated log scores in estimation could lead to too-large
bandwidth estimates, in turn inflating the width of the predictive distribution.
We will return to this possibility in our conclusions.

\subsection{Combining Marginal Predictive Distributions with Copulas}
\label{subsec:Methods:Copulas}

We use copulas \cite{nelsen2007introductionCopulas} to tie the marginal
predictive distributions for individual prediction horizons obtained from KCDE together into a joint predictive
distribution for the trajectory of incidence over multiple time points. 
The copula is a parametric function that captures the dependence relations among
a collection of random variables and allows us to compute the joint distribution
from the marginal distributions.  Figure 16 in the
supplement shows that the copula induces positive correlation
in the predictive distributions for incidence in nearby weeks, so that high
incidence in one week is more likely to be followed by high incidence in
weeks soon after.

To describe our methods for both continuous and discrete distributions, it is
most convenient to frame the discussion in this Subsection in terms of
cumulative distribution functions (CDF) instead of density functions.
We will use a capital $C$ to denote the copula function for CDFs and a
lower case $c$ to denote the copula function for densities.  Similarly, the predictive
densities $f^{h}(y_{t^*}^h \mid \bx_{t^*}; \btheta^h)$ we obtained in the previous
Subsection naturally yield corresponding predictive CDFs
$F^{h}(y_{t^*}^h \mid \bx_{t^*}; \btheta^h)$.

Our model specifies the joint CDF for $(Y_{t^*}^1, \ldots, Y_{t^*}^{H_{t^*}})$
as follows:
\begin{align}
&F^{H_{t^*}}(y_{t^*}^1, \ldots, y_{t^*}^{H_{t^*}} \mid \bx_{t^*}; \btheta^1, \ldots, \btheta^{H_{t^*}}, \bxi^{H_{t^*}}) = \nonumber \\
&\quad C\{F^{1}(y_{t^*}^1 \mid \bx_{t^*}; \btheta^1), \ldots, F^{H_{t^*}}(y_{t^*}^{H_{t^*}} \mid \bx_{t^*}; \btheta^{H_{t^*}}); \bxi^{H_{t^*}}\}
\end{align}

The copula function $C$ maps the marginal CDF values to the joint
CDF value.  We use the isotropic normal
copula implemented in the {\tt R} \cite{RCoreLanguage} package {\tt copula}
\cite{HofertRCopulaPackage}.  The copula function is given by
\begin{equation}
C(u_1, \ldots, u_H ; \bxi^H) = \Phi_{\Sigma^H}(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_H)),
\end{equation}
where $\Phi^{-1}$ is the inverse CDF of a univariate normal
distribution with mean $0$ and variance $1$ and $\Phi_{\Sigma^H}$ is the CDF
of a multivariate normal distribution with mean $\b0$ and covariance matrix
$\Sigma^H$.  The isotropic specification sets $\Sigma^H = [{\sigma^H_{i,j}}]$, where 
\begin{equation}
{\sigma^H_{i,j}} = \begin{cases} 1 \text{ if $i = j$,} \\ \xi^H_d \text{ if $\vert i - j \vert = d$} \end{cases}
\end{equation}
Intuitively, $\xi^H_d$ captures the amount of dependence between incidence
levels at future times that are $d$ weeks apart.

We obtain a separate copula fit for each value of $H$ from 2 to $W$ (note that a
copula is not required for ``trajectories'' of length $H = 1$).
Broadly, estimation for the model parameters proceeds in two stages:
first we estimate the parameters for KCDE separately for each prediction
horizon $h = 1, \ldots, H$ as described in the previous Section, and second
we estimate the copula parameters while holding the KCDE parameters fixed.
We give a more detailed description of this estimation
procedure in the supplement.  In general the two-stage approach may result in
some loss of efficiency relative to one-stage methods, but this efficiency 
loss is small for some model specifications
\cite{joe2005asymptoticEfficiencyTwoStageCopula}.
Also, it results in a large reduction in the computational cost of parameter
estimation.

\section{Simulation Study}
\label{sec:SimStudy}

One component of the KCDE model specification outlined in
Subsection~\ref{subsec:Methods:KCDE} is the parameterization of the bandwidth matrix.
We conducted a simulation study to examine the utility of using a
fully parameterized matrix specification instead of a diagonal bandwidth matrix when
estimating discrete conditional distributions with KCDE.  The simulation study
is motivated by the simplest case of predicting
incidence in a single week using KCDE: predicting incidence at time $t + h$ given
incidence at time $t$.  A central characteristic of the disease
incidence data we analyze in the next Section is the presence of positive
correlation between incidence in nearby time points
(Supplemental Figure 2).  In this simulation study we demonstrate that
in the presence of such correlation, using fully parameterized bandwidth
matrices can improve conditional density estimates over using a diagonal
bandwidth.

There are many factors that determine the relative performance of KCDE
estimators with different bandwidth parameterizations.  In this simulation
study, we vary just one of these factors:
the sample size ($N = 100$ or $N = 1000$).  These sample sizes are roughly similar to
the number of observations in the training data sets used in the applications in Section~\ref{sec:Applications} (where we have training sets of size 692 in the application to influenza and 988 in the application to dengue fever).

%We simulate data from a discretized bivariate normal
%distribution.  To define this
%distribution, let $\bU \sim MVN(\b0, \Sigma)$ where $\Sigma$ is a $2 \times 2$ matrix with $1$ on
%the diagonal and $0.9$ off of the diagonal.  We treat $\bU$
%as a latent variable and discretize it to obtain the random variable $\bX$ using
%the approach described in the supplement.

We conducted 500 simulation trials for each sample size.
In each trial, we simulated $N$ observations of
a discretized bivariate normal random variable $\bX$ with mean $\b0$ and
covariance matrix $\Sigma$ where $\Sigma$ has $1$ on
the diagonal and $0.9$ off of the diagonal (see Supplement for further detail).
Using these observations as a training data set, we estimated the bandwidth
parameters for two variations on a KCDE model for the conditional distribution
of $X_1 \mid X_2$: one with a diagonal bandwidth matrix specification
and one with a fully parameterized bandwidth matrix.  In this simulation study,
the kernel function was obtained by discretizing a multivariate normal kernel
function rather than a log-normal kernel function as in the applications below. 
Otherwise, the method is as described previously.

We evaluated the conditional density estimates by an importance
sampling approximation of the Hellinger distance of the conditional
density estimate from the true conditional density, integrated over the range of
the covariates (see supplement). 
The Hellinger distance lies between 0 and 1, with smaller values indicating that the
density estimate is better.  It has been argued that the Hellinger distance is
preferred to other measures of the quality of kernel density estimates such as
integrated squared error \cite{kanazawa1993hellingerDistanceKDE}. For each
combination of the training set sample size, dimension, and simulation trial, we
compute the difference between the Hellinger distance from the true conditional
distribution achieved with a diagonal bandwidth matrix and with a fully parameterized bandwidth matrix.

The results indicate that in the presence of correlation between the
conditioning variable and the density estimation target, using a fully
parameterized bandwidth matrix instead of a diagonal bandwidth generally yields improved
density estimates as measured by the integrated Hellinger distance (Figure~\ref{fig:SimStudyResultsPlot}).
The average improvement from using a fully parameterized bandwidth matrix
is larger with a sample size of $N = 100$ instead of $N = 1000$, but there is
also more variation in performance with the smaller sample size.  This suggests
that using a fully parameterized bandwidth may be helpful in applications
similar to infectious disease prediction where there is correlation between the
quantity being predicted (e.g., future incidence) and the quantities that we
condition on in order to make the predictions.

\section{Applications}
\label{sec:Applications}

In this Section, we illustrate our methods through applications to prediction
of infectious disease incidence in two examples with real disease incidence data
sets.  We begin with a discussion of the data, then we describe the models we
compare and the evaluation procedures before discussing the results.

\subsection{Data}

We apply our methods to two infectious disease data sets (Figure~\ref{fig:IntialDataPlots}).
The first data set consists of a weekly count of reported cases of dengue fever in San
Juan, Puerto Rico. The second data set consists of a composite indicator
of flu activity generated by the Centers for Disease Control (CDC) and
referred to as the weighted influenza-like illness (wILI) index.  The
wILI is calculated as the of a proportion of doctor visits with influenza-like
illness to clinics participating in the U.S. Outpatient Influenza-like
Illness Surveillance Network (ILINet).  The measured proportions are
weighted by state population and combined into region-level scores.  We
did not attempt to replicate this weighting scheme and instead used wILI
directly in our models.
These data sets were used in two recent
prediction competitions sponsored by the United States federal government
\cite{PandemicPredictionandForecastingScienceandTechnologyInteragencyWorkingGroup2015Announcement,
EpidemicPredictionInitiative2015Index}.

An important feature of both of these time series is that they exhibit fairly regular seasonal trends: incidence of dengue fever usually reaches a peak during the summer months and a nadir during the winter months, while influenza typically peaks during the winter and reaches a nadir during the summer months.  For the purposes of making predictions of seasonal targets with the dengue data, we have used the definition of a season used by the competition administrators: the season begins in the week starting on April 29th or April 30th (depending on the year); historically, this has been the week of the year with lowest dengue incidence.  For the influenza data, we define the season as beginning in the 30th week of the year; which is the week starting on July 29th or July 30th.  Again, historically the lowest incidence of influenza has tended to occur near that time.

\subsection{Prediction targets and evaluation criteria}

We use the three prediction targets described in Section~\ref{sec:Methods}
(Supplemental Figure 3).
Following the precedent set in the
competitions, we make predictions for \textit{binned} incidence in the peak
week.  For the dengue data set, the bins are $[0, 50), [50, 100), \ldots, [500, \infty)$.
For the influenza data set, the bins are $[0, 0.5), [0.5, 1), \ldots, [13, \infty)$.
Our predictions for incidence in individual weeks are for the raw, unbinned,
incidence measure.

We divided each data set into two subsets.  The last four years of each data set are
reserved as a test set for evaluating model performance.  The size of the test set was determined by
the dengue prediction competition administrators.  In the influenza data set, the last four years of data included
only observations for three full seasons.  The first period is used as a training set in
estimating the model parameters.  For the influenza data, we had 14 years of training data (1997 through 2010);
for the dengue data, we had 19 seasons of training data (1990/1991 through 2008/2009).
All predictions are made as though in real time, assuming that
once cases are reported they are never revised and that there are no delays in
reporting.  Specifically, we use only data up through a given week to
make predictions for incidence after that week.

We evaluated model performance using log scores for predictions in the test phase for each data set 
(log scores were defined previously in Section \ref{subsec:Methods:KCDE}).
For each season in the testing period, we examined the log scores for
predictions made in all weeks of the season, as well as for smaller subsets of those weeks
that are most relevant to decision makers using predictions to set
public health policy.  Specifically, for incidence in individual weeks, we examined
model performance for predictions of incidence in the weeks where the eventually observed
incidence was at least 2/3 of the maximum incidence observed in the testing phase.
For predictions of incidence in the peak week and the timing of the peak week,
we evaluated performance of predictions made before the peak actually occurred.
Additionally, for predictions of incidence in individual weeks, we considered the
coverage rate of predictive intervals obtained from each method.

\subsection{Models}

Our applications evaluate four variations on KCDE model specifications:
\begin{enumerate}
\item The ``Null KCDE'' model omits the periodic component of the kernel
function and uses a diagonal bandwidth matrix specification for the incidence
kernel.
\item The ``Full Bandwidth KCDE'' model omits the periodic component of the
kernel function and uses a fully parameterized bandwidth matrix specification for the incidence
kernel.
\item The ``Periodic KCDE'' model includes the periodic component of the kernel
function and uses a diagonal bandwidth matrix specification for the incidence
kernel.
\item The ``Periodic, Full Bandwidth KCDE'' model includes the periodic
component of the kernel function and uses a fully parameterized bandwidth
matrix specification for the incidence
kernel.
\end{enumerate}

We include three baseline models for comparison to our methods.  The first is a
seasonal autoregressive integrated moving average (SARIMA) model.  In fitting this model, we first
transformed the observed incidence measure to the log scale (after adding $1$ in
the dengue data set, which included some observations of $0$ cases); this
transformation makes the normality assumptions of the SARIMA model more plausible.
We then performed first-order seasonal differencing, and obtained the final
model fits using the {\tt auto.arima} function in {\tt R}'s {\tt forecast}
package \cite{hyndmanRForecastPackage}; this function uses a stepwise
procedure to determine the terms to include in the model.
This procedure resulted in a 
<<ILISarimaModelFitSummary, echo = FALSE, results = "asis">>=
## The following relative path assumes the working directory is inst/article/
ili_sarima_fit <- readRDS("../results/ili_national/estimation-results/sarima-fit.rds")
temp <- capture.output(summary(ili_sarima_fit))
cat(paste0("SARIMA(", substr(temp[2], 7, 15), as.integer(substr(temp[2], 16,16)) + 1, substr(temp[2], 17, 18), ")$_{52}$")) 
@
model for the influenza data and a 
<<DengueSarimaModelFitSummary, echo = FALSE, results = "asis">>=
## The following relative path assumes the working directory is inst/article/
dengue_sarima_fit <- readRDS("../results/dengue_sj/estimation-results/sarima-fit.rds")
temp <- capture.output(summary(dengue_sarima_fit))
cat(paste0("SARIMA(", substr(temp[2], 7, 15), as.integer(substr(temp[2], 16, 16)) + 1, substr(temp[2], 17, 18), ")$_{52}$")) 
@
model for the dengue data.  In applying this model to the dengue data, we have
discretized the predictive distributions obtained from SARIMA using the same
methods that we used for KCDE.  This discretization was not used in model
estimation since it is not available in the standard estimation software.

The second baseline model is the ``HHH4'' model for infectious disease incidence
\cite{Held2005HHHIntro, Paul2008MultivariateHHH4, Held2012HHHSeasonality},
available in the {\tt surveillance} \cite{Hohle:R:surveillance} package in R.
This is a generalized linear model with either a Poisson or Negative Binomial family.
The mean is a linear combination of autoregressive and sinusoidal components. 
We followed the model selection and estimation procedures outlined in \cite{Held2012HHHSeasonality}
(see Supplement for details).  The prediction target for dengue data is
discrete case counts and easily implemented in the HHH4 software
(Figure~\ref{fig:DengueRibbonsPredictions}).  The prediction target for
flu requested by the CDC is proportion of doctor visits with flu-like
illness weighted by state population.  Implementing this in HHH4
would require weighting state-level predictions.  As we did not attempt
to make state level predictions we did not use HHH4 as a reference
model for the flu data.

For predictions of peak timing and binned peak incidence, we considered a third naive baseline
model that assigned equal probability to all bins (where for peak timing, there is one bin for
each week in the season).

\subsection{Results}

\subsubsection{For predictions of incidence in individual weeks, KCDE outperforms the baseline models (Table~\ref{tbl:IndWeeksIncidenceResults}).}  KDCE specifications including a periodic kernel component consistently had the highest or close to the highest mean log scores for both data sets whether aggregating across all weeks or only high incidence weeks.  Additionally, the worst-case performance of the HHH4 and SARIMA models was much lower than the worst-case performance of any of the KCDE specifications for all combinations of the data set and the subset of weeks considered.

<<DengueDataMergePredictionResults, echo = FALSE>>=
## The following relative paths assume the working directory is inst/article/
dengue_prediction_results_sarima <- readRDS("../results/dengue_sj/prediction-results/sarima-predictions.rds")
dengue_prediction_results_hhh4 <- readRDS("../results/dengue_sj/prediction-results/surveillance-predictions.rds")
dengue_prediction_results_kcde <- readRDS("../results/dengue_sj/prediction-results/kcde-predictions.rds")
dengue_prediction_results_kcde$model <- "KCDE"
dengue_prediction_results <- rbind.fill(dengue_prediction_results_sarima[!is.na(dengue_prediction_results_sarima$log_score), ],
    dengue_prediction_results_hhh4,
    dengue_prediction_results_kcde)
dengue_prediction_results$AE <- unlist(dengue_prediction_results$AE)

dengue_prediction_results$full_model_descriptor <- paste0(dengue_prediction_results$model,
    "-seasonal_lag_", dengue_prediction_results$max_seasonal_lag,
#    "-filtering_", dengue_prediction_results$filtering,
    "-differencing_", dengue_prediction_results$differencing,
    "-periodic_", dengue_prediction_results$seasonality,
    "-bw_", dengue_prediction_results$bw_parameterization)

dengue_prediction_log_score_diffs_from_sarima_wide <- dengue_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

dengue_prediction_log_score_diffs_from_sarima_wide[, unique(dengue_prediction_results$full_model_descriptor)] <-
    dengue_prediction_log_score_diffs_from_sarima_wide[, unique(dengue_prediction_results$full_model_descriptor)] -
    dengue_prediction_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

dengue_prediction_log_score_diffs_from_sarima_long <- dengue_prediction_log_score_diffs_from_sarima_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null KCDE"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$periodic & !dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic KCDE"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    !dengue_prediction_log_score_diffs_from_sarima_long$periodic & dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$periodic & dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
    ] <- "HHH4"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
    factor(dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

dengue_prediction_log_score_diffs_from_hhh4_wide <- dengue_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

dengue_prediction_log_score_diffs_from_hhh4_wide[, unique(dengue_prediction_results$full_model_descriptor)] <-
    dengue_prediction_log_score_diffs_from_hhh4_wide[, unique(dengue_prediction_results$full_model_descriptor)] -
    dengue_prediction_log_score_diffs_from_hhh4_wide[, "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

dengue_prediction_log_score_diffs_from_hhh4_long <- dengue_prediction_log_score_diffs_from_hhh4_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor <- "Null KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$periodic & !dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Periodic KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    !dengue_prediction_log_score_diffs_from_hhh4_long$periodic & dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$periodic & dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor <-
    factor(dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))
@

<<FluDataMergePredictionResults, echo = FALSE>>=
## The following relative paths assume the working directory is inst/article/
ili_prediction_results_sarima <- readRDS("../results/ili_national/prediction-results/sarima-predictions.rds")
ili_prediction_results_kcde <- readRDS("../results/ili_national/prediction-results/kcde-predictions.rds")
ili_prediction_results_kcde$model <- "KCDE"
ili_prediction_results <- rbind.fill(ili_prediction_results_sarima[!is.na(ili_prediction_results_sarima$log_score), ],
    ili_prediction_results_kcde)
ili_prediction_results$AE <- unlist(ili_prediction_results$AE)

ili_prediction_results$full_model_descriptor <- paste0(ili_prediction_results$model,
    "-seasonal_lag_", ili_prediction_results$max_seasonal_lag,
#    "-filtering_", ili_prediction_results$filtering,
    "-differencing_", ili_prediction_results$differencing,
    "-periodic_", ili_prediction_results$seasonality,
    "-bw_", ili_prediction_results$bw_parameterization)

ili_prediction_log_score_diffs_from_sarima_wide <- ili_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

ili_prediction_log_score_diffs_from_sarima_wide[, unique(ili_prediction_results$full_model_descriptor)] <-
    ili_prediction_log_score_diffs_from_sarima_wide[, unique(ili_prediction_results$full_model_descriptor)] -
    ili_prediction_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

ili_prediction_log_score_diffs_from_sarima_long <- ili_prediction_log_score_diffs_from_sarima_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null KCDE"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    ili_prediction_log_score_diffs_from_sarima_long$periodic & !ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic KCDE"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    !ili_prediction_log_score_diffs_from_sarima_long$periodic & ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Full Bandwidth KCDE"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    ili_prediction_log_score_diffs_from_sarima_long$periodic & ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    ili_prediction_log_score_diffs_from_sarima_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
    factor(ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "SARIMA"))



temp <- dengue_sj[, c("time", "total_cases", "season")]
colnames(temp) <- c("prediction_time", "total_cases", "season")

dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars <-
    dengue_prediction_log_score_diffs_from_sarima_long %>%
    left_join(temp, by = "prediction_time", copy = TRUE)
dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars <-
    dengue_prediction_log_score_diffs_from_hhh4_long %>%
    left_join(temp, by = "prediction_time", copy = TRUE)

dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$baseline_model = "SARIMA"
dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$baseline_model = "HHH4"
@


<<GetLogScoreDiffMeanSDInLowerUpperTertiles, echo = FALSE>>=
lower_cutoff <- max(dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$total_cases) / 3
upper_cutoff <- 2 * lower_cutoff

temp <- dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
        dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$total_cases <= lower_cutoff,
    "log_score_difference"]

kcde_sarima_lower_tertile_mean <- mean(temp)
kcde_sarima_lower_tertile_sd <- sd(temp)
kcde_sarima_lower_tertile_median <- median(temp)
kcde_sarima_lower_tertile_quantiles <- quantile(temp, probs = c(0.25, 0.75))

temp <- dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
        dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$total_cases >= upper_cutoff,
    "log_score_difference"]

kcde_sarima_upper_tertile_mean <- mean(temp)
kcde_sarima_upper_tertile_sd <- sd(temp)
kcde_sarima_upper_tertile_median <- median(temp)
kcde_sarima_upper_tertile_quantiles <- quantile(temp, probs = c(0.25, 0.75))

kcde_sarima_min <- min(
  dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "log_score_difference"]
)
kcde_sarima_max <- max(
  dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "log_score_difference"]
)




lower_cutoff <- max(dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$total_cases) / 3
upper_cutoff <- 2 * lower_cutoff

temp <- dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
        dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$total_cases <= lower_cutoff,
    "log_score_difference"]

kcde_hhh4_lower_tertile_mean <- mean(temp)
kcde_hhh4_lower_tertile_sd <- sd(temp)
kcde_hhh4_lower_tertile_median <- median(temp)
kcde_hhh4_lower_tertile_quantiles <- quantile(temp, probs = c(0.25, 0.75))

temp <- dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
        dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$total_cases >= upper_cutoff,
    "log_score_difference"]

kcde_hhh4_upper_tertile_mean <- mean(temp)
kcde_hhh4_upper_tertile_sd <- sd(temp)
kcde_hhh4_upper_tertile_median <- median(temp)
kcde_hhh4_upper_tertile_quantiles <- quantile(temp, probs = c(0.25, 0.75))

kcde_hhh4_min <- min(
  dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "log_score_difference"]
)
kcde_hhh4_max <- max(
  dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars[
    dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "log_score_difference"]
)

@

In the application to dengue fever, KCDE offered the largest improvements relative to the baseline models for predictions in weeks with high incidence near
the season peaks (Table~\ref{tbl:IndWeeksIncidenceResults}, Figure~\ref{fig:DengueLogScoreDiffVsReportedCasesCombined}).
For example, in weeks with more than 184 reported cases (two thirds of
the maximum weekly case count in the testing period), the median log score
difference between the predictions from the Periodic, Full Bandwidth KCDE model and SARIMA was about
<<KCDESARIMALogScoreDiffsUpperTertile, echo = FALSE, results = "asis">>=
cat(paste0(
    sprintf("%.2f", round(kcde_sarima_upper_tertile_median, 2)),
    " ($Q_1$ = ",
    sprintf("%.2f", round(kcde_sarima_upper_tertile_quantiles[1], 2)),
    ", $Q_3$ = ",
    sprintf("%.2f", round(kcde_sarima_upper_tertile_quantiles[2], 2)), 
    ")"
))
@
where values greater than zero show KCDE making more accurate predictions than SARIMA (Supplemental Table [[TODO: table number here]]).
The median log score difference relative to HHH4 for these weeks was about
<<KCDEHHH4LogScoreDiffsUpperTertile, echo = FALSE, results = "asis">>=
cat(paste0(
    sprintf("%.2f", round(kcde_hhh4_upper_tertile_median, 2)),
    " ($Q_1$ = ",
    sprintf("%.2f", round(kcde_hhh4_upper_tertile_quantiles[1], 2)),
    ", $Q_3$ = ",
    sprintf("%.2f", round(kcde_hhh4_upper_tertile_quantiles[2], 2)), 
    ")."
))
@
Translating to a probability scale, in these periods of high incidence this KCDE
specification assigned about 5 times higher probability to the observed outcome
as SARIMA on average and about 1.25 times higher probability as HHH4 on average.
Moreover, there were cases where the KCDE model assigned up to about 450 times as much
probability to the realized outcome as SARIMA, and over 1300 times as much probability as HHH4.  Across all weeks in the test period and all prediction horizons, neither baseline model ever outperformed this KCDE specification by a factor of more than 9.
Similar patterns also hold with the other KCDE specifications.
For the application to influenza, there were not consistent trends in the relative performance of
the models in low and high incidence weeks.

In both applications, the predictive intervals for incidence in individual weeks are quite wide for all of the methods we considered (Figure~\ref{fig:DengueRibbonsPredictions}).  However, for dengue fever the coverage rates in the test phase were actually lower than the nominal coverage rate for all methods (Table~\ref{tbl:CoverageResults}).  The KCDE models were generally closer to the target coverage rates than the baseline models, indicating that the width of the predictive intervals from KCDE give an appropriate representation of uncertainty about future dengue incidence.  For predictions of influenza, the coverage rates for all KCDE specifications as well as the baseline SARIMA model were too large.  For this application, none of the models had consistently better or worse performance than the others as measured by coverage rates of the predictive intervals.

\subsubsection{For predictions of peak incidence the KCDE models with periodic kernel components had better mean performance than the baseline models in the application to dengue fever, and in both applications the KCDE models had more consistent performance across seasons than the baseline models (Table~\ref{tbl:PeakIncidenceResults}, Figure~\ref{fig:CombinedPeakWeekTimingPredictionLogScores}).}  In the application to dengue fever, the HHH4 model struggled to predict peak incidence in the two test phase seasons with the highest peak, generally performing worse than a naive approach using equal bin probabilities in those seasons.  The SARIMA model did well at predicting peak incidence for dengue, with overall performance that only slightly lower than the Periodic, Full Bandwidth KCDE specification and similar performance in all four test phase seasons.  However, in the application to influenza the SARIMA model struggled in the test phase season with highest incidence, with performance levels generally falling below the approach using equal bin probabilities.  Meanwhile, the KCDE specifications had much more consistent performance across all test phase seasons, and never did much worse than using equal bin probabilities.  For dengue fever the Periodic, Full Bandwidth KCDE specification had the highest average log scores and best worst-case log scores for predictions of peak incidence (Table), while in the application to influenza the KCDE models did only a little worse than SARIMA overall, and did much better in the influenza season with highest incidence.

\subsubsection{For predictions of peak week timing made before the peak actually occurred, the KCDE models without periodic kernel components had the best performance in the application to dengue fever, but the SARIMA model had the best performance in the application to influenza.}  For dengue fever, both the SARIMA and HHH4 models consistently underperformed relative
to the naive approach using equal bin probabilities for predictions of peak timing.  On the other hand, the KCDE models, and particularly those that did not use a periodic kernel component, generally outperformed this baseline.  There was quite a bit of variability in the timing of peak dengue incidence between test phase seasons, and the models including seasonal terms sometimes failed badly when the peak occurred relatively early or late (Figure~\ref{fig:CombinedPeakWeekTimingPredictionLogScores}).  The KCDE specifications without periodic terms were more robust to this variation in season timing.  There was less variability in the timing of the season peak in the three complete test phase seasons in our influenza data, and the SARIMA model was the overall best performing model in that application.  However, the SARIMA model was still outperformed by the KCDE models in the influenza season with the latest peak.  All methods we evaluated tend to converge rapidly on the truth once the peak week has passed.


<<FluDataMergePeakWeekPredictionResults, echo = FALSE>>=
data_set <- "ili_national"

## The following relative path assumes the working directory is inst/article/
prediction_save_path <- file.path("../results",
    data_set,
    "prediction-results")

all_max_lags <- as.character(c(1L))
#all_max_seasonal_lags <- as.character(c(0L, 1L))
all_max_seasonal_lags <- as.character(c(0L))
all_filtering_values <- c("FALSE")
#all_differencing_values <- c("FALSE", "TRUE")
all_differencing_values <- c("FALSE")
all_seasonality_values <- c("FALSE", "TRUE")
all_bw_parameterizations <- c("diagonal", "full")

case_definitions <- expand.grid(
        data_set,
        all_max_lags,
        all_max_seasonal_lags,
        all_filtering_values,
        all_differencing_values,
        all_seasonality_values,
        all_bw_parameterizations,
        stringsAsFactors = FALSE) %>%
    `colnames<-`(c("data_set",
            "max_lag",
            "max_seasonal_lag",
            "filtering",
            "differencing",
            "seasonality",
            "bw_parameterization"))
 
ili_peak_week_results <- rbind.fill(
    c(
        list(
            readRDS(file.path(prediction_save_path,
                        paste0("peak-week-sarima-", data_set, ".rds"))) %>%
                `[[<-`("model", value = "SARIMA")
        ),
        lapply(seq_len(nrow(case_definitions)), function(case_row_ind) {
                max_lag <- case_definitions$max_lag[case_row_ind]
                max_seasonal_lag <- case_definitions$max_seasonal_lag[case_row_ind]
                filtering <- case_definitions$filtering[case_row_ind]
                differencing <- case_definitions$differencing[case_row_ind]
                seasonality <- case_definitions$seasonality[case_row_ind]
                bw_parameterization <- case_definitions$bw_parameterization[case_row_ind]
                
                case_descriptor <- paste0(
                    data_set,
                    "-max_lag_", max_lag,
                    "-max_seasonal_lag_", max_seasonal_lag,
                    "-filtering_", filtering,
                    "-differencing_", differencing,
                    "-seasonality_", seasonality,
                    "-bw_parameterization_", bw_parameterization
                )
                
                temp <- readRDS(file.path(prediction_save_path,
                            paste0("peak-week-", case_descriptor, ".rds")))
                temp$model <- "KCDE"
                temp$max_lag <- max_lag
                temp$max_seasonal_lag <- max_seasonal_lag
                temp$filtering <- filtering
                temp$differencing <- differencing
                temp$seasonality <- seasonality
                temp$bw_parameterization <- bw_parameterization
                
                return(temp)
            })
    )
)

ili_peak_week_results$full_model_descriptor <- paste0(ili_peak_week_results$model,
    "-seasonal_lag_", ili_peak_week_results$max_seasonal_lag,
#    "-filtering_", ili_prediction_results$filtering,
    "-differencing_", ili_peak_week_results$differencing,
    "-periodic_", ili_peak_week_results$seasonality,
    "-bw_", ili_peak_week_results$bw_parameterization)

ili_peak_week_results$peak_week_log_score[ili_peak_week_results$peak_week_log_score < -50] <- -50
ili_peak_week_results$peak_height_log_score[ili_peak_week_results$peak_height_log_score < -50] <- -50

ili_peak_week_results$reduced_model_descriptor <- "Null KCDE"
ili_peak_week_results$reduced_model_descriptor[
    as.logical(ili_peak_week_results$seasonality) & !(ili_peak_week_results$bw_parameterization == "full")] <-
    "Periodic KCDE"
ili_peak_week_results$reduced_model_descriptor[
    !as.logical(ili_peak_week_results$seasonality) & (ili_peak_week_results$bw_parameterization == "full")] <-
    "Full Bandwidth KCDE"
ili_peak_week_results$reduced_model_descriptor[
    as.logical(ili_peak_week_results$seasonality) & (ili_peak_week_results$bw_parameterization == "full")] <-
    "Periodic, Full Bandwidth KCDE"
ili_peak_week_results$reduced_model_descriptor[
    ili_peak_week_results$model == "SARIMA"] <-
    "SARIMA"

num_analysis_time_season_values <- length(unique(ili_peak_week_results$analysis_time_season))
num_analysis_time_season_week_values <- length(unique(ili_peak_week_results$analysis_time_season_week))
ili_peak_week_results <- rbind.fill(ili_peak_week_results,
    data.frame(
        full_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        reduced_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        analysis_time_season = rep(unique(ili_peak_week_results$analysis_time_season), each = num_analysis_time_season_week_values),
        analysis_time_season_week = rep(unique(ili_peak_week_results$analysis_time_season_week), times = num_analysis_time_season_values),
        peak_week_log_score = rep(log(1/52), num_analysis_time_season_week_values * num_analysis_time_season_values),
        peak_height_log_score = rep(log(1/27), num_analysis_time_season_week_values * num_analysis_time_season_values)
    ))


ili_peak_week_results$reduced_model_descriptor <-
    factor(ili_peak_week_results$reduced_model_descriptor,
        levels = c("SARIMA", "Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE",
            "Periodic, Full Bandwidth KCDE",
            "Equal Bin Probabilities"
        ))

#geom_hline(yintercept = log(1/31), colour = "grey", linetype = 2)

ili_peak_week_times <- data.frame(
    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
    peak_week = sapply(unique(ili_peak_week_results$analysis_time_season),
        function(season_val) {
            max_incidence_in_season <-
                max(ili_national$weighted_ili[ili_national$season == season_val])
            return(ili_national$season_week[ili_national$season == season_val &
                        ili_national$weighted_ili == max_incidence_in_season])
        })
)

ili_peak_week_heights <- data.frame(
    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
    peak_height = sapply(unique(ili_peak_week_results$analysis_time_season),
        function(season_val) {
            return(max(ili_national$weighted_ili[ili_national$season == season_val]))
        })
)
        
ili_peak_week_results$peak_week_log_score[ili_peak_week_results$peak_week_log_score == -50] <- NA
ili_peak_week_results$peak_height_log_score[ili_peak_week_results$peak_height_log_score == -50] <- NA
@






<<DengueDataMergePeakWeekPredictionResults, echo = FALSE>>=
data_set <- "dengue_sj"
    
prediction_save_path <- file.path("../results",
    data_set,
    "prediction-results")

all_max_lags <- as.character(c(1L))
#all_max_seasonal_lags <- as.character(c(0L, 1L))
all_max_seasonal_lags <- as.character(c(0L))
all_filtering_values <- c("FALSE")
#all_differencing_values <- c("FALSE", "TRUE")
all_differencing_values <- "FALSE"
all_seasonality_values <- c("FALSE", "TRUE")
all_bw_parameterizations <- c("diagonal", "full")

case_definitions <- expand.grid(
        data_set,
        all_max_lags,
        all_max_seasonal_lags,
        all_filtering_values,
        all_differencing_values,
        all_seasonality_values,
        all_bw_parameterizations,
        stringsAsFactors = FALSE) %>%
    `colnames<-`(c("data_set",
            "max_lag",
            "max_seasonal_lag",
            "filtering",
            "differencing",
            "seasonality",
            "bw_parameterization"))
 
dengue_peak_week_results <- rbind.fill(
    c(
        list(
            readRDS(file.path(prediction_save_path,
                        paste0("peak-week-sarima-", data_set, ".rds"))) %>%
                `[[<-`("model", value = "SARIMA")
        ),
        list(
            readRDS(file.path(prediction_save_path,
                        paste0("peak-week-surveillance-", data_set, ".rds"))) %>%
                `[[<-`("model", value = "HHH4")
        ),
        lapply(seq_len(nrow(case_definitions)), function(case_row_ind) {
                max_lag <- case_definitions$max_lag[case_row_ind]
                max_seasonal_lag <- case_definitions$max_seasonal_lag[case_row_ind]
                filtering <- case_definitions$filtering[case_row_ind]
                differencing <- case_definitions$differencing[case_row_ind]
                seasonality <- case_definitions$seasonality[case_row_ind]
                bw_parameterization <- case_definitions$bw_parameterization[case_row_ind]
                
                case_descriptor <- paste0(
                    data_set,
                    "-max_lag_", max_lag,
                    "-max_seasonal_lag_", max_seasonal_lag,
                    "-filtering_", filtering,
                    "-differencing_", differencing,
                    "-seasonality_", seasonality,
                    "-bw_parameterization_", bw_parameterization
                )
                
                temp <- readRDS(file.path(prediction_save_path,
                            paste0("peak-week-", case_descriptor, ".rds")))
                
                temp$model <- "KCDE"
                temp$max_lag = max_lag
                temp$max_seasonal_lag = max_seasonal_lag
                temp$filtering = filtering
                temp$differencing = differencing
                temp$seasonality = seasonality
                temp$bw_parameterization = bw_parameterization
                
                return(temp)
            })
    )
)

dengue_peak_week_results$full_model_descriptor <- paste0(dengue_peak_week_results$model,
    "-seasonal_lag_", dengue_peak_week_results$max_seasonal_lag,
#    "-filtering_", dengue_prediction_results$filtering,
    "-differencing_", dengue_peak_week_results$differencing,
    "-periodic_", dengue_peak_week_results$seasonality,
    "-bw_", dengue_peak_week_results$bw_parameterization)

dengue_peak_week_results$reduced_model_descriptor <- "Null KCDE"
dengue_peak_week_results$reduced_model_descriptor[
    as.logical(dengue_peak_week_results$seasonality) & !(dengue_peak_week_results$bw_parameterization == "full")] <-
    "Periodic KCDE"
dengue_peak_week_results$reduced_model_descriptor[
    !as.logical(dengue_peak_week_results$seasonality) & (dengue_peak_week_results$bw_parameterization == "full")] <-
    "Full Bandwidth KCDE"
dengue_peak_week_results$reduced_model_descriptor[
    as.logical(dengue_peak_week_results$seasonality) & (dengue_peak_week_results$bw_parameterization == "full")] <-
    "Periodic, Full Bandwidth KCDE"
dengue_peak_week_results$reduced_model_descriptor[
    dengue_peak_week_results$model == "SARIMA"] <-
    "SARIMA"
dengue_peak_week_results$reduced_model_descriptor[
    dengue_peak_week_results$model == "HHH4"] <-
    "HHH4"

num_analysis_time_season_values <- length(unique(dengue_peak_week_results$analysis_time_season))
num_analysis_time_season_week_values <- length(unique(dengue_peak_week_results$analysis_time_season_week))
dengue_peak_week_results <- rbind.fill(dengue_peak_week_results,
    data.frame(
        full_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        reduced_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        analysis_time_season = rep(unique(dengue_peak_week_results$analysis_time_season), each = num_analysis_time_season_week_values),
        analysis_time_season_week = rep(unique(dengue_peak_week_results$analysis_time_season_week), times = num_analysis_time_season_values),
        peak_week_log_score = rep(log(1/52), num_analysis_time_season_week_values * num_analysis_time_season_values),
        peak_height_log_score = rep(log(1/11), num_analysis_time_season_week_values * num_analysis_time_season_values)
    ))


dengue_peak_week_results$reduced_model_descriptor <-
    factor(dengue_peak_week_results$reduced_model_descriptor,
        levels = c("HHH4", "SARIMA", "Null KCDE", "Full Bandwidth KCDE",
        "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
            "Equal Bin Probabilities"
        ))

#geom_hline(yintercept = log(1/31), colour = "grey", linetype = 2)

dengue_peak_week_times <- data.frame(
    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
    peak_week = sapply(unique(dengue_peak_week_results$analysis_time_season),
        function(season_val) {
            max_incidence_in_season <-
                max(dengue_sj$total_cases[dengue_sj$season == season_val])
            return(dengue_sj$season_week[dengue_sj$season == season_val &
                        dengue_sj$total_cases == max_incidence_in_season])
        })
)

dengue_peak_week_heights <- data.frame(
    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
    peak_height = sapply(unique(dengue_peak_week_results$analysis_time_season),
        function(season_val) {
            return(max(dengue_sj$total_cases[dengue_sj$season == season_val]))
        })
)

dengue_peak_week_results$peak_week_log_score[dengue_peak_week_results$peak_week_log_score < -50] <- -50
dengue_peak_week_results$peak_height_log_score[dengue_peak_week_results$peak_height_log_score < -50] <- -50
dengue_peak_week_results$peak_week_log_score[dengue_peak_week_results$peak_week_log_score == -50] <- NA
dengue_peak_week_results$peak_height_log_score[dengue_peak_week_results$peak_height_log_score == -50] <- NA
@

<<PeakIncidenceLogScoreDifferencesFromEqual, echo = FALSE>>=
dengue_peak_incidence_log_score_diffs_from_equal_wide <- dengue_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
    spread(full_model_descriptor, peak_height_log_score)

dengue_peak_incidence_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
    dengue_peak_incidence_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
    dengue_peak_incidence_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

dengue_peak_incidence_log_score_diffs_from_equal_long <- dengue_peak_incidence_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & !dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)) {
    dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week[
        dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season == season &
            dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}



ili_peak_incidence_log_score_diffs_from_equal_wide <- ili_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
    spread(full_model_descriptor, peak_height_log_score)

ili_peak_incidence_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
    ili_peak_incidence_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
    ili_peak_incidence_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

ili_peak_incidence_log_score_diffs_from_equal_long <- ili_peak_incidence_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$periodic & !ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !ili_peak_incidence_log_score_diffs_from_equal_long$periodic & ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$periodic & ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)) {
    ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week[
        ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season == season &
            ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}



combined_peak_incidence_diffs_from_equal <- rbind.fill(
        ili_peak_incidence_log_score_diffs_from_equal_long[ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ],
        dengue_peak_incidence_log_score_diffs_from_equal_long[dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ]
    )
#peak_incidence_log_score_diffs_from_equal_means <- tapply(
#    combined_peak_incidence_diffs_from_equal$log_score_difference,
#    combined_peak_incidence_diffs_from_equal[, c("reduced_model_descriptor", "data_set_and_season")],
#    mean
#)
#peak_incidence_log_score_diffs_from_equal_sds <- tapply(
#    combined_peak_incidence_diffs_from_equal$log_score_difference,
#    combined_peak_incidence_diffs_from_equal[, c("reduced_model_descriptor", "data_set_and_season")],
#    sd
#)
@



<<PeakTimingLogScoreDifferencesFromEqual, echo = FALSE>>=
dengue_peak_timing_log_score_diffs_from_equal_wide <- dengue_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
    spread(full_model_descriptor, peak_week_log_score)

dengue_peak_timing_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
    dengue_peak_timing_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
    dengue_peak_timing_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

dengue_peak_timing_log_score_diffs_from_equal_long <- dengue_peak_timing_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$periodic & !dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !dengue_peak_timing_log_score_diffs_from_equal_long$periodic & dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$periodic & dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)) {
    dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week[
        dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season == season &
            dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}



ili_peak_timing_log_score_diffs_from_equal_wide <- ili_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
    spread(full_model_descriptor, peak_week_log_score)

ili_peak_timing_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
    ili_peak_timing_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
    ili_peak_timing_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

ili_peak_timing_log_score_diffs_from_equal_long <- ili_peak_timing_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$periodic & !ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !ili_peak_timing_log_score_diffs_from_equal_long$periodic & ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$periodic & ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)) {
    ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week[
        ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season == season &
            ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}



combined_peak_timing_diffs_from_equal <- rbind.fill(
        ili_peak_timing_log_score_diffs_from_equal_long[ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week, ],
        dengue_peak_timing_log_score_diffs_from_equal_long[dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week, ]
    )
#peak_timing_log_score_diffs_from_equal_means <- tapply(
#    combined_peak_timing_diffs_from_equal$log_score_difference,
#    combined_peak_timing_diffs_from_equal[, c("reduced_model_descriptor", "data_set_and_season")],
#    mean
#)
#peak_timing_log_score_diffs_from_equal_sds <- tapply(
#    combined_peak_timing_diffs_from_equal$log_score_difference,
#    combined_peak_timing_diffs_from_equal[, c("reduced_model_descriptor", "data_set_and_season")],
#    sd
#)
@

\subsubsection{In most cases, including a periodic kernel component in the KCDE specification led to improved predictions}.  KCDE specifications including a periodic kernel component had better average performance than the corresponding KCDE specification without a periodic kernel component for predicting incidence in individual weeks in all combinations of the data sets and the subset of weeks considered.  The periodic kernel also led to better predictions of peak incidence in every case except for early season predictions in the application to influenza when the bandwidth matrix for the incidence kernel component was fully parameterized.  For predictions of peak timing, including the periodic kernel component was helpful in the application to influenza, but led to worse performance for early-season predictions of the peak timing for dengue incidence.

\subsubsection{We have seen that the KCDE model outperformed the baseline models in the application to dengue, but the SARIMA model generally had higher mean performance than the KCDE models in the application to influenza (although SARIMA had less consistent performance for predictions of incidence in individual weeks or at the peak week in both applications).}
We believe that the difference in relative performance of KCDE and the baseline
models for prediction in the dengue and influenza data sets can be explained to
a great extent by differences in the underlying disease processes and how they relate to
the model specifications.  The most salient difference between the two
time series is the much greater season-to-season variablility in the dengue 
data set relative to the influenza data set (Figure~\ref{fig:IntialDataPlots}).  
For dengue, the peak incidence in the largest season is about 30
times larger than the peak incidence in the smallest season; this ratio is only
about 3 for influenza.  It may be the case that the restrictive
structure of the SARIMA and HHH4 models means that they are not able to capture
the dynamics of dengue incidence accurately.  For example, Held and Paul
\cite{Held2012HHHSeasonality} discuss the fact that the seasonal structure in
the HHH4 model does not explicitly allow for different amplitudes in different seasons.
Relaxing that structure by using a non-parametric approach such as KCDE may yield
improved capability to represent the disease dynamics.  This is less of an issue in predicting influenza where
there is much more consistency across different seasons -- but even in that case, SARIMA was outperformed by KCDE for predicting peak incidence in the season with the highest incidence and for predicting peak timing in the season with the latest peak (Figure~\ref{fig:CombinedPeakWeekTimingPredictionLogScores}).



%For the influenza data, the SARIMA method had average log scores 
%<<PeakIncSARIMAInfluenza20122013LogScoreDiff, echo = FALSE, results = "asis">>=
%cat(paste0(sprintf("%.3f", round(peak_incidence_log_score_diffs_from_equal_means["SARIMA", "Influenza: 2012/2013"], 3)), " (sd = ", sprintf("%.3f", round(peak_incidence_log_score_diffs_from_equal_sds["SARIMA", "Influenza: 2012/2013"], 3)), ")"))
%@
%in the 2012/2013 season, but the average performance of all four KCDE
%specifications was better than the naive model for all three seasons in the
%influenza data.  The SARIMA model did outperform KCDE 


% Figure~\ref{fig:CombinedPeakWeekIncidencePredictionLogScores} displays the log
% score of the predictive distributions for incidence in the peak week
% obtained from SARIMA and KCDE models over the course of each season in the test data sets, and
% Figure 5 in the supplement displays log scores for predictions of
% peak week timing.  Figures 6 through 9 of the supplement give the full
% predictive distributions for peak week timing and incidence.


% Overall, the
% performance of early-season predictions of peak week timing and incidence from
% KCDE is more stable than the predictions from either of the baseline models.
% Generally speaking, performance of both the SARIMA and HHH4 models
% is more variable; the methods underperform (occasionally drastically) relative to KCDE in some seasons but
% do better than KCDE at specific times in other seasons.  
% In the application to dengue, HHH4 is outperformed by KCDE and SARIMA for
% predictions of peak incidence made early in the season.  In the two dengue
% seasons with low incidence, HHH4 offered small gains in predictions for peak
% week incidence relative to KCDE and SARIMA, but these are offset by very large 
% drops in performance for predictions made in the first 10 to 20
% weeks of the two seasons with higher peaks.  The SARIMA and KCDE models performed very
% similarly in five of the seven seasons included in the testing period, including
% all four dengue seasons.  In the application to influenza, SARIMA performed
% better than KCDE in predicting peak incidence in the season with low incidence,
% but was outperformed by KCDE in the season with high incidence.  However, the
% SARIMA model was quite a bit worse than the naive prediction of assigning equal
% probability to each incidence bin in the season with high incidence, whereas KCDE never did much worse than using equal bin
% probabilities.  For predictions of peak week timing, performance of HHH4 and
% KCDE is similar in the application to Dengue.  




<<FluObtainPeakWeekTimingPredictiveDistributionsByAnalysisTime, echo = FALSE, dependson = c("FluDataMergePeakWeekPredictionResults")>>=
ili_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 13, by = 0.5),
    upper = c(seq(from = 0.5, to = 13, by = 0.5), Inf))

for(bin_num in seq(from = 9, to = 41)) {
   ili_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_timing_pred_dist_by_analysis_time <- ili_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq(from = 9, to = 41)))
peak_timing_pred_dist_by_analysis_time$bin <-
    as.integer(substr(peak_timing_pred_dist_by_analysis_time$bin, 14, 15))


peak_timing_and_height_pred_dist_means_by_analysis_time <- 
    ili_peak_week_results %>%
    select(full_model_descriptor,
        analysis_time_season,
        analysis_time_season_week,
        starts_with("est_prob_bin_")) %>%
    mutate(
        mean_peak_week = apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            mean),
        median_peak_week = apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            median),
        mean_peak_height = apply(ili_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            mean),
        median_peak_height = apply(ili_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            median)
    )
@

<<FluObtainPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE, cache = TRUE, dependson = c("FluDataMergePeakWeekPredictionResults")>>=
ili_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 13, by = 0.5),
    upper = c(seq(from = 0.5, to = 13, by = 0.5), Inf),
    center = seq(from = 0.25, to = 13.25, by = 0.5))
for(bin_num in seq_len(nrow(ili_incidence_bins))) {
    ili_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(ili_peak_week_results[, paste0("peak_height_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_height_pred_dist_by_analysis_time <- ili_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq_len(nrow(ili_incidence_bins))))
peak_height_pred_dist_by_analysis_time$bin <-
    as.integer(substr(peak_height_pred_dist_by_analysis_time$bin, 14, 15))
peak_height_pred_dist_by_analysis_time$bin_center <-
    ili_incidence_bins$center[peak_height_pred_dist_by_analysis_time$bin]
@

<<DengueObtainPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
dengue_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 500, by = 50),
    upper = c(seq(from = 50, to = 500, by = 50), Inf),
    center = seq(from = 25, to = 525, by = 50))

for(bin_num in seq_len(nrow(dengue_incidence_bins))) {
    dengue_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(dengue_peak_week_results[, paste0("peak_height_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_height_pred_dist_by_analysis_time_dengue <- dengue_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq_len(nrow(dengue_incidence_bins))))
peak_height_pred_dist_by_analysis_time_dengue$bin <-
    as.integer(substr(peak_height_pred_dist_by_analysis_time_dengue$bin, 14, 15))
peak_height_pred_dist_by_analysis_time_dengue$bin_center <-
    dengue_incidence_bins$center[peak_height_pred_dist_by_analysis_time_dengue$bin]
@

\section{Conclusions}

Prediction of infectious disease incidence at horizons of more than a few weeks
is a challenging task.  We have presented a semi-parametric approach to doing
this based on KCDE and copulas and found that it is a viable method that
can yield improved predictions relative to commonly employed methods in this
field.  In predicting incidence of dengue fever in individual weeks,
our approach offered consistent and substantial performance gains relative to a
SARIMA model and the HHH4 model.  These improvements were particularly
concentrated in the times that are of most interest to public health decision 
makers: periods of high incidence near the season peak.   In the application to influenza,
our method did about as well as SARIMA when predicting incidence in individual
weeks.

Overall, across both data sets our method offered more consistency than the
baseline models in predictions for incidence in the peak week.  Both baseline
models suffered in one or more seasons with high incidence where they made
substantially worse predictions than a naive model
assigning equal probability to each incidence bin, whereas KCDE never did much
worse than this naive model.  For pre-peak predictions of peak week timing,
there were multiple seasons where the SARIMA model consistently underperformed
relative to the naive approach of assigning equal probability to each week of
the year; KCDE and HHH4 were more consistently at or above the level of this
naive approach.  These improvements in the reliability of early-season
predictions are valuable to public health officials planning interventions several weeks or
months before the peak of the disease season.  However, since year-to-year
variation is substantial, continued evaluation of these methods on datasets with
longer prospective testing phases could provide better information about
long-run performance of all of these methods.

Our implementation of KCDE offers two main methodological contributions.  Most
importantly in the context of modeling infectious disease, we have introduced the use of a periodic kernel
component that captures seasonality.  In both of our applications, including
this periodic kernel component in the KCDE specification led to substantial 
improvements in the predictive distributions for incidence in individual weeks.
We also introduced a method for obtaining kernel functions that are appropriate
for use with discrete data while allowing for a fully parameterized bandwidth
matrix.  In our applications, using a fully parameterized bandwidth matrix did
not lead to consistent improvements in predictions.  However, we have
demonstrated through a simulation study that the fully parameterized bandwidth
can be helpful in some conditional density estimation tasks.  This general
method for obtaining discrete kernel functions may be beneficial in other
applications of KCDE.

%dengue_max_by_season <- tapply(dengue_sj$total_cases, dengue_sj$season, max)
%max(dengue_max_by_season) / min(dengue_max_by_season)
%ili_max_by_season <- tapply(ili_national$weighted_ili,
%as.factor(ili_national$season), max, na.rm = TRUE)
%max(ili_max_by_season) / min(ili_max_by_season)

%Another more subtle effect is present in the influenza data: there is a
%consistent short-term peak in influenza incidence on Christmas week.  This is
%visible in Figure~\ref{fig:IntialDataPlots}, and is highlighted in Figure 11 in
%the supplement.  This ``Christmas effect'' sometimes coincides with
%the season peak, but sometimes occurs before or after the season peak.  We have
%observed evidence that the seasonal structure of the SARIMA model picks up on this
%structure, and SARIMA tended to outperform KCDE on Christmas week and the weeks
%immediately thereafter on the influenza data set.  We believe that it would be
%possible to construct a variation on KCDE that captures this effect, for example
%by including indicator variables for the weeks around Christmas as conditioning
%variables.  However, we have not explored that avenue in this work.  We also
%note that even without these extensions, KCDE offered similar performance to
%SARIMA overall in the influenza application.

A major advantage of the approach we have outlined is its flexibility in terms
of cleanly handling both discrete and continuous data and a variety of
underlying disease mechanisms.
Our method consistently yielded reasonable predictions for all three prediction
targets in both applications. As we have seen, the HHH4 model is
formulated in terms of discrete case counts and so could not be directly
applied to the influenza data where the disease measure was continuous.  Even in
the data set where it could be used, the HHH4 model 
underperformed relative to KCDE in predictions for incidence in individual weeks
and incidence in the peak week.  Similarly, the standard
SARIMA model is formulated in terms of continuous distributions.  The
resulting continuous predictive distributions can be discretized as we have
done in this article, but without extra coding effort the method is not
appropriate for use with case count data when small integer numbers of cases 
are reported.  Furthermore, our approach consistently
equalled or exceeded the performance SARIMA across the applications to dengue
and influenza.

%One explanation for the difference in relative performance of the methods on
%these different prediction tasks may lie in the connection between the objective
%function used in parameter estimation and the prediction task.  
%We estimated the bandwidth parameters for KCDE by optimizing the log score of
%predictive distributions for incidence in individual weeks in the training data
%set.  This is the prediction target where KCDE outperformed SARIMA on the dengue
%data set.  It may be the case that performance on the other two prediction tasks
%could be improved by implementing a combined one-stage estimation
%strategy for both the KCDE and copula parameters that optimizes a measure of
%performance on the specific prediction task at hand.

There is room for extensions and improvements to the methods we
have outlined in this article.  One limitation of our work lies in the
selection of covariates for the predictive model.  We have simply
used incidence at the two most recent time points, and possibly the observation
time, as covariates.  We considered using a stepwise
variable selection approach to select the model specification,
but we found this to be too computationally expensive to be practical; the full
grid search suggested by De Gooijer and Gannoun
\cite{de2000nonparametricConditionalPredictiveRegions} in similar settings with only one bandwidth parameter would be far too slow for our methods.

Another method for improving our ability to use covariates would be to replace
variable selection with shrinkage.  \cite{hall2004crossvalidationKCDE}
show that when cross-validation is used to select the bandwidth parameters in
KCDE using product kernels, the estimated bandwidths corresponding to
irrelevant covariates tend to infinity asymptotically as the sample
size increases.  We conjecture
that by introducing an appropriate penalty on the elements bandwidth
matrix, bandwidths for irrelevant covariates could be driven to infinity
at lower sample sizes.  This technique should allow us to include more (possibly
irrelevant) covariates in the model.  
If successful, this would also enable further
exploration of using other predictive variables such as weather, 
incidence measures from neighboring locations, or data from internet searches and social media in the model.

In many disease incidence data sets, we observe multiple incidence
time series simultaneously.  For example, in addition to the national level
wILI index used in this article, the influenza-like illness data from the CDC
contain measures of incidence for 10 smaller regions within the United States,
and break down incidence within four age groups.  The methods described in
this article could be applied to make predictions with multiple time series.
One possible approach to this would be to fit a separate predictive model for
each time series, using the other time series as covariates that are conditioned on.

Another aspect of our method that should be explored further is the use of
log score in estimation.  We used log scores in this work to match the use
of log scores in evaluating and comparing the performance of different models. 
The log score has the advantage of defining a proper scoring
rule, but it has the disadvantage of being sensitive to extreme values.
Previous authors have suggested the use of other loss functions in estimation
for kernel-based density estimation methods that reduce these effects, such as
variations on integrated squared error
\cite{fan2004crossvalidationKCDE} or the continuous ranked probability
score \cite{jeon2012KCDEWindPower}.  Despite discussion in the literature of the
potential limitations of using log scores for estimation with kernel-based methods,
there is not conclusive evidence that use of log scores caused any difficulties
in our application. For example, while the predictive interval coverage rates
were too high in the application to influenza, coverage rates were too low in
the application to dengue fever.  Nevertheless, details of the loss function
used in estimation could impact the utility of the resulting predictions.

In the present article, we have simplified the disease prediction task by
assuming that the disease incidence measure is reported accurately and without delay.
This allowed us to focus on the narrower methodological question of
examining whether KCDE is able to capture infectious disease dynamics.
However, in order to apply the methods in a real time setting it will be
crucial to relax this assumption.  We envision two ways that this could be done.
First, we could model the relationship between initial reports of incidence
and the final revised incidence measure.  Using that model, we could use
initial reports of incidence at any given time to predict the revised incidence at
that time.  These predictions of final incidence could then be used as inputs
to the KCDE prediction model outlined in this article.  This approach is similar
in spirit to the methods used by Brooks \etal \cite{brooks2015empiricalBayes}.
An alternative approach could use KCDE to directly learn a relationship between
initial, unrevised, reports of disease incidence and the final incidence measure in future weeks.

The KCDE modeling framework could also be applied to directly model the joint distribution
of incidence in multiple future weeks without the use of a copula.  If we were to directly
model incidence in all remaining weeks of the season with KCDE the method would operate
more similarly to the approach of Brooks \etal \cite{brooks2015empiricalBayes}, who directly model
the trajectory of incidence over the course of the season.  However, we believe that this
line would have limited success since fully nonparametric estimation of the joint distribution of
incidence in 40 future weeks (for example) given only about 15 to 20 years of past data will
be challenging.  Another possible approach would be
to use KCDE to obtain a joint predictive density of incidence in smaller groups of weeks
(for example, 2 - 5 weeks at a time) and then combine
those predictive densities using a mechanism such as a copula.
Such an intermediate approach might be able to capture more information about
medium-term trends in incidence such as holiday effects than the method we have presented in this article
without suffering from the curse of dimensionality as much as direct application of KCDE to
an entire season at a time.

There is also a long history of using other modeling approaches such as
compartmental models for infectious disease prediction.  KCDE is distinguished
from these approaches in that it makes minimal assumptions about the data generating process.
This can be either an advantage or a disadvantage of KCDE.  On the positive
side, these minimal assumptions are what make KCDE appropriate for use with a
wide variety of disease processes with minimal changes to the model
specification.  On the other hand, we believe that a well-specified
mechanistic model might outperform KCDE in certain circumstances.
%Other approaches such as compartmental
%models require more tailoring to the specific disease being modeled.  However, in general we would expect a well-specified parametric model to outperform KCDE.  On the other hand, 
%because KCDE makes fewer assumptions about the data generating process, it might
%outperform incorrectly specified parametric models.
%%In general, flexible non-parametric methods such as KCDE exhibit low
%%bias but high variance.  If they are correctly specified, models with more
%%structure may achieve reduced variance without introducing bias.
%%On the other hand,   An
%An evaluation of the benefits of an approach such as KCDE is therefore dependent
%on the particular characteristics of the system being modeled, the data that are
%available, and the quality of the models that are considered as alternatives. 
However, rather than selecting one ``preferred'' modeling framework or
model formulation, we believe it may be fruitful to incorporate the methods
developed in this paper as components of an ensemble with several
different types of models.
%For example, in our application to influenza, we saw
%that the SARIMA model captured some features of the data generating process,
%such as the Christmas-week effect, that KCDE did not capture.  On the other
%hand, the KCDE approach was more flexible and yielded better predictions than
%SARIMA at other times -- most notably, in periods of high incidence in the
%application to dengue.  
An appropriately constructed ensemble incorporating predictions from KCDE as
well as other methods might perform better than any
of the component models on their own, and would be a valuable approach for
maximizing the utility of these predictions to public health decision makers.

\section{Software}

The estimation methods were implemented in {\tt R} and {\tt C}.  All source code
and data are available in {\tt R} packages hosted on GitHub
\cite{ReichLabGitHubDiseasePredWithKCDEPackage}.

\section{Supplementary Material}

The reader is referred to the on-line Supplementary Materials for
technical details and additional figures with further information
about the results.

\section*{Acknowledgments}

The authors thank the competition administrators for making disease incidence data
available.  This work was supported by the National Institute of Allergy and
Infectious Diseases at the National Institutes of Health (grants R21AI115173 and
R01AI102939).


%\bibliographystyle{plainnat}
\bibliographystyle{wileyj}
\bibliography{kde-bib}

\begin{figure}
<<PeriodicKernelPlot, echo = FALSE, fig.height = 7.75, dev = "postscript">>=
plot_df <- data.frame(t=seq_len(5 * 52))

kernel_center <- plot_df$t[nrow(plot_df)]
rho <- pi / 52

h <- 0.1
plot_df$kernel_h0.1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 1
plot_df$kernel_h1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 10
plot_df$kernel_h10 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

plot_df <- melt(plot_df, id.vars = "t")
plot_df$variable <- as.character(plot_df$variable)
plot_df$bandwidth <- "0.1"
plot_df$bandwidth[plot_df$variable == "kernel_h1"] <- "1"
plot_df$bandwidth[plot_df$variable == "kernel_h10"] <- "10"

p_per <- ggplot(plot_df) +
    geom_line(aes(x = t, y = value, linetype = bandwidth, colour = bandwidth)) +
#    geom_vline(xintercept = kernel_center) +
    scale_colour_manual("Bandwidth",
        breaks = c("0.1", "1", "10"),
        labels = c("0.1", "1", "10"),
        values = c("#E69F00", "#56B4E9", "#009E73")
    ) +
    scale_linetype("Bandwidth") +
    scale_x_continuous(breaks = c(0, seq(from = 52, length = 5, by = 52))) +
    ylab("Kernel Function Value") +
    xlab("Time in Weeks") +
    ggtitle("Periodic Kernel Component for Time") +
    theme_bw(base_size = 11)


cont_grid_bounds <- c(0.01, 10)
cont_grid_size <- 101
x_cont_grid <- 
    expand.grid(
        seq(from = cont_grid_bounds[1], to = cont_grid_bounds[2], length = cont_grid_size),
        seq(from = cont_grid_bounds[1], to = cont_grid_bounds[2], length = cont_grid_size)
    ) %>%
    `colnames<-`(c("X1", "X2"))
disc_grid_bounds <- c(0.5, 9.5)
x_disc_grid <-
    expand.grid(
        seq(from = disc_grid_bounds[1], to = disc_grid_bounds[2], by = 1),
        seq(from = disc_grid_bounds[1], to = disc_grid_bounds[2], by = 1)
    ) %>%
    `colnames<-`(c("X1", "X2"))


#' Compute log(round(exp(x))) in such a way that the rounding function
#' always rounds up or down to an integer + 0.5, and
#' an integer always gets rounded up.
#' 
#' @param x numeric
#' 
#' @return floor(x) - 1
log_round_to_integer_plus_0.5_exp <- function(x) {
    exp_x <- exp(x) + 0.5
    
    inds_ceil <- exp_x - floor(exp_x) >= 0.5
    
    exp_x[inds_ceil] <- ceiling(exp_x[inds_ceil])
    exp_x[!inds_ceil] <- floor(exp_x[!inds_ceil])
    
    return(log(exp_x - 0.5))
}

var_b <- 0.2
covar_b <- 0.15


continuous_density_df_b <- x_cont_grid %>%
    as.data.frame() %>%
    `$<-`("z",
        log_pdtmvn_mode_centered_kernel(x = x_cont_grid,
            center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
            bw = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
            bw_continuous = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
            continuous_vars = c("X1", "X2"),
            discrete_vars = character(0),
            continuous_var_col_inds = 1:2,
            discrete_var_col_inds = integer(0),
            discrete_var_range_fns = NULL,
            lower = c(X1 = -Inf, X2 = -Inf),
#            lower = c(X1 = log(0.5), X2 = log(0.5)),
            upper = c(X1 = Inf, X2 = Inf),
            x_names = c("X1", "X2"),
            log = FALSE)
    )

discrete_density_df_b <- x_disc_grid %>%
    as.data.frame() %>%
    `$<-`("z",
        log_pdtmvn_mode_centered_kernel(x = x_disc_grid,
            center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
            bw = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
            bw_continuous = matrix(0, nrow = 0, ncol = 0),
            continuous_vars = character(0),
            discrete_vars = c("X1", "X2"),
            continuous_var_col_inds = integer(0),
            discrete_var_col_inds = 1:2,
            discrete_var_range_fns = list(
                X1 = list(a = function(x) {
                        return(log(exp(x) - 0.5))
                    },
                    b = function(x) {
                        return(log(exp(x) + 0.5))
                    },
                    in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                        return(sapply(x, function(x_i) {
                                    return(
                                        isTRUE(all.equal(
                                                x_i,
                                                log_round_to_integer_plus_0.5_exp(x_i),
                                                tolerance = tolerance
                                            ))
                                    )
                                }))
                    },
                    discretizer = log_round_to_integer_plus_0.5_exp),
                X2 = list(a = function(x) {
                        return(log(exp(x) - 0.5))
                    },
                    b = function(x) {
                        return(log(exp(x) + 0.5))
                    },
                    in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                        return(sapply(x, function(x_i) {
                                    return(
                                        isTRUE(all.equal(
                                                x_i,
                                                log_round_to_integer_plus_0.5_exp(x_i),
                                                tolerance = tolerance
                                            ))
                                    )
                                }))
                    },
                    discretizer = log_round_to_integer_plus_0.5_exp)
            ),
            lower = c(X1 = -Inf, X2 = -Inf),
            upper = c(X1 = Inf, X2 = Inf),
            x_names = c("X1", "X2"),
            log = FALSE)
    )

p_inc <- ggplot() +
    geom_contour(aes(x = X1, y = X2, z = z, colour = ..level..), bins = 13, data = continuous_density_df_b) +
    geom_point(aes(x = X1, y = X2, colour = z), data = discrete_density_df_b) +
    scale_colour_gradientn("Kernel\nFunction   \nValue",
        colours = rev(c("#333333", "#777777", "#BBBBBB", "#FFFFFF")),
        trans = "log",
        limits = c(0.00001, 0.2),
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    xlab(expression(X[1])) +
    ylab(expression(X[2])) +
    ggtitle("Log-Normal Kernel Component for Incidence") +
    theme_bw(base_size = 11)

grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 1,
            heights = unit(c(1, 3.3), c("null", "null")),
            widths = unit(c(1), c("null")))))
print(p_per, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(p_inc, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
@
\caption{The components of the kernel function.  The top panel shows the
periodic kernel function illustrated as a function of time in weeks with
$\rho = \pi / 52$ and three possible values for the bandwidth parameter
$\eta$.  The lower panel shows the log-normal kernel function in the bivariate
case.  The curves indicate contours of the continuous kernel function and the
points indicate the discrete kernel function, which is obtained by integrating
the continuous kernel function.  The kernel is centered at $(2.5, 2.5)$ and has
bandwidth matrix $\begin{bmatrix}0.2 & 0.15 \\ 0.15 & 0.2\end{bmatrix}$.}
\label{fig:PeriodicKernelPlot}
\end{figure}

\begin{figure}
<<SimStudyResultsPlot, echo = FALSE, fig.height = 5, dev = "postscript">>=
## The following relative path assumes the working directory is inst/article/
sim_results <- readRDS("../results/sim-study/kcde-predictions.rds")
sim_results <- sim_results[1:2000, ]
sim_results$sim_run_inds <- rep(seq_len(500), each = 4)

sim_results_contrasts_within_n_dim <- sim_results %>%
    group_by(sim_n, sim_family, sim_run_inds) %>%
    summarize(
        KL_div_diff = KL_div[which(bw_parameterization == "diagonal")] - KL_div[which(bw_parameterization == "full")],
        Hellinger_dist_diff = Hellinger_dist[which(bw_parameterization == "diagonal")] - Hellinger_dist[which(bw_parameterization == "full")]
    )

ggplot() +
    geom_boxplot(aes(x = sim_n, y = Hellinger_dist_diff),
        data = sim_results_contrasts_within_n_dim) +
    geom_hline(yintercept = 0) +
    xlab("Training Set Sample Size") +
    ylab("Difference in Hellinger Distance for KCDE with\nDiagonal Bandwidth vs. Fully Parameterized Bandwidth") +
    ggtitle("Simulation Study Results\nImprovement in Integrated Hellinger Distance\nfrom Using Fully Parameterized Bandwidth") +
    theme_bw()
@
\caption{Box plots of results from the simulation study.  Positive values
indicate simulation trials where the full bandwidth specification outperformed the diagonal bandwidth
specification with the same training data set, as measured by Hellinger distance from the
target conditional density.}
\label{fig:SimStudyResultsPlot}
\end{figure}

\begin{figure}
<<InitialDataPlot, echo = FALSE, fig.height = 5, dev = "postscript">>=
dengue_plot <- ggplot() +
    geom_line(aes(x = as.Date(time), y = total_cases),
        data = dengue_sj) +
    geom_vline(aes(xintercept = as.numeric(as.Date(dengue_train_cutoff_time))),
        colour = "red", linetype = 2) +
    scale_x_date() +
    scale_y_continuous(limits = c(0, 500), expand = c(0, 0)) +
    xlab("Time") +
    ylab("Reported Cases") +
    ggtitle("Dengue Fever Data - San Juan, Puerto Rico") +
    theme_bw(base_size = 11)

ili_plot <- ggplot() +
    geom_line(aes(x = as.Date(time), y = weighted_ili),
        data = ili_national) +
    geom_vline(aes(xintercept = as.numeric(as.Date(time))),
        colour = "grey",
        data = ili_national[is.na(ili_national$weighted_ili), ]) +
    geom_vline(aes(xintercept = as.numeric(as.Date(ili_train_cutoff_time))),
        colour = "red", linetype = 2) +
    scale_x_date() +
    xlab("Time") +
    ylab("Weighted Influenza-like Illness\n") +
    ggtitle("Influenza Data - National United States") +
    theme_bw(base_size = 11)

grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 1)))
print(dengue_plot, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(ili_plot, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
@
\caption{Plots of the data sets we apply our methods to.  In each case, the last
four years of data are held out as a test data set; this cutoff is indicated
with a vertical dashed line.  For the flu data set, low-season incidence was not
recorded in early years of data collection.  These missing data are indicated
with vertical grey bars.}
\label{fig:IntialDataPlots}
\end{figure}



<<CalcLogScoreDiffs, echo = FALSE, fig.height = 8.4, dev = "postscript">>=
models_used <- c(
    unique(ili_prediction_results$full_model_descriptor[
    !ili_prediction_results$differencing & !(ili_prediction_results$max_seasonal_lag == 1)])
)

dengue_prediction_log_score_diffs_from_sarima_long$data_set <- "Dengue"
dengue_prediction_log_score_diffs_from_hhh4_long$data_set <- "Dengue"
ili_prediction_log_score_diffs_from_sarima_long$data_set <- "Influenza"
combined_prediction_log_score_diffs_from_sarima_long <-
    rbind(dengue_prediction_log_score_diffs_from_sarima_long,
        ili_prediction_log_score_diffs_from_sarima_long[
            !(ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor == "Periodic, Full Bandwidth KCDE" &
                    ili_prediction_log_score_diffs_from_sarima_long$prediction_horizon == 50L),])

color_palette <- c("#D55E00", "#56B4E9")

sarima_mean_differences_df <- combined_prediction_log_score_diffs_from_sarima_long[combined_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ] %>%
    group_by(reduced_model_descriptor, data_set) %>%
    summarize(mean_log_score_difference = mean(log_score_difference))
@


\begin{figure}
<<DengueLogScoreDiffVsReportedCasesCombined, echo = FALSE, fig.height = 8.4, fig.keep = "last", dev = "cairo_ps">>=
dengue_prediction_log_score_diffs_from_baseline <- rbind.fill(
    dengue_prediction_log_score_diffs_from_sarima_long_with_pred_time_covars,
    dengue_prediction_log_score_diffs_from_hhh4_long_with_pred_time_covars
)

color_palette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442")

scatterplot_dengue_diff_from_baseline_by_obs_cases <- ggplot() +
    geom_point(aes(x = total_cases, y = log_score_difference, colour = season, shape = season),
        alpha = 0.5,
        data = dengue_prediction_log_score_diffs_from_baseline[dengue_prediction_log_score_diffs_from_baseline$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full", ]) +
    geom_hline(yintercept = 0) +
    geom_smooth(aes(x = total_cases, y = log_score_difference, colour = season),
        method = lm,
        se = FALSE,
        size = 1.5,
        data = dengue_prediction_log_score_diffs_from_baseline[dengue_prediction_log_score_diffs_from_baseline$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full", ]) +
    scale_colour_manual("Season", values = color_palette) +
    scale_shape("Season") +
    facet_wrap( ~ baseline_model, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
            labels <- paste0("Baseline Model: ", labels)
            return(labels)
        })) +
    xlab("Reported Dengue Cases in Prediction Target Week") +
    ylab("Log Score Difference") +
    ggtitle("Comparison of Periodic, Full Bandwidth KCDE Model and Baseline Models\nvs. Reported Dengue Cases in Prediction Target Week") +
    theme_bw(base_size = 11) +
    theme(plot.margin = unit(c(6, 2, 6, 2), "mm"))

legend_grob <- get_legend_grob(scatterplot_dengue_diff_from_baseline_by_obs_cases)
scatterplot_dengue_diff_from_baseline_by_obs_cases <-
    scatterplot_dengue_diff_from_baseline_by_obs_cases +
    theme(legend.position = "none")

densityplot_dengue_cases <- ggplot() +
    geom_density(aes(x = total_cases, colour = season, shape = season),
        data = dengue_prediction_log_score_diffs_from_baseline[
          dengue_prediction_log_score_diffs_from_baseline$model == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" & 
          dengue_prediction_log_score_diffs_from_baseline$prediction_horizon == 1L, ]) +
    scale_colour_manual("Season", values = color_palette) +
    xlab("Reported Dengue Cases in Prediction Target Week") +
    ylab("Density") +
    theme_bw() +
    theme(legend.position = "none")


grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 2,
            heights = unit(c(3, 1), c("null", "null")),
            widths = unit(c(4, 1), c("null", "null")))))
print(scatterplot_dengue_diff_from_baseline_by_obs_cases,
  vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(densityplot_dengue_cases, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))

pushViewport(viewport(layout.pos.row = 1:2, layout.pos.col = 2))
grid.draw(legend_grob)
upViewport()
@
\caption{Differences in log scores for the weekly predictive distributions
obtained from the Periodic, Full Bandwidth KCDE model and the baseline
models, plotted against the observed incidence in the week being predicted.
For reference, a log score difference of 2.3 (4.6) indicates that the predictive density from KCDE was about 10 (100)
times as large as the predictive density from the baseline model at the realized outcome.  Each
point corresponds to a unique combination of prediction target week and
prediction horizon.
}
\label{fig:DengueLogScoreDiffVsReportedCasesCombined}
\end{figure}





\begin{table}[hp]
\begin{tabular}{lllllllll}
\toprule
         &          &              &    \multicolumn{6}{c}{Summary of Log Scores} \\
\cline{4-9}
 Disease & Subset & Model & Min & Q1 & Q2 & Mean & Q3 & Max \\ 
  \hline
<<PerformanceSummaryTableIndividualWeeksAbsolute, echo = FALSE, results = "asis">>=
ili_prediction_results <- ili_prediction_results %>%
    mutate(
        periodic = grepl("periodic_TRUE", full_model_descriptor),
        bw_full = grepl("bw_full", full_model_descriptor)
    )
ili_prediction_results$reduced_model_descriptor <- "Null KCDE"
ili_prediction_results$reduced_model_descriptor[
    ili_prediction_results$periodic & !ili_prediction_results$bw_full] <-
    "Periodic KCDE"
ili_prediction_results$reduced_model_descriptor[
    !ili_prediction_results$periodic & ili_prediction_results$bw_full] <-
    "Full Bandwidth KCDE"
ili_prediction_results$reduced_model_descriptor[
    ili_prediction_results$periodic & ili_prediction_results$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_prediction_results$reduced_model_descriptor[
    ili_prediction_results$full_model_descriptor == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_prediction_results$reduced_model_descriptor <-
    factor(ili_prediction_results$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "SARIMA"))

temp <- ili_national[, c("time", "weighted_ili")]
colnames(temp) <- c("prediction_time", "weighted_ili")

ili_prediction_results <-
  ili_prediction_results %>%
  left_join(temp, by = "prediction_time", copy = TRUE)

dengue_prediction_results <- dengue_prediction_results %>%
    mutate(
        periodic = grepl("periodic_TRUE", full_model_descriptor),
        bw_full = grepl("bw_full", full_model_descriptor)
    )
dengue_prediction_results$reduced_model_descriptor <- "Null KCDE"
dengue_prediction_results$reduced_model_descriptor[
    dengue_prediction_results$periodic & !dengue_prediction_results$bw_full] <-
    "Periodic KCDE"
dengue_prediction_results$reduced_model_descriptor[
    !dengue_prediction_results$periodic & dengue_prediction_results$bw_full] <-
    "Full Bandwidth KCDE"
dengue_prediction_results$reduced_model_descriptor[
    dengue_prediction_results$periodic & dengue_prediction_results$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_prediction_results$reduced_model_descriptor[
    dengue_prediction_results$full_model_descriptor == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_prediction_results$reduced_model_descriptor[
    dengue_prediction_results$full_model_descriptor == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_prediction_results$reduced_model_descriptor <-
    factor(dengue_prediction_results$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

temp <- dengue_sj[, c("time", "total_cases")]
colnames(temp) <- c("prediction_time", "total_cases")

dengue_prediction_results <-
    dengue_prediction_results %>%
    left_join(temp, by = "prediction_time", copy = TRUE)

temp <- dengue_prediction_results %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score),
    mean = mean(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(reduced_model_descriptor = 
  #   ifelse(reduced_model_descriptor %in% c("HHH4", "SARIMA"),
  #     reduced_model_descriptor,
  #     substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

lower_cutoff <- max(dengue_prediction_results$total_cases) / 3
upper_cutoff <- 2 * lower_cutoff

temp_nadir <- dengue_prediction_results %>%
  filter(total_cases <= lower_cutoff) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score),
    mean = mean(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(week_type = "Low Incidence",
  #   reduced_model_descriptor = substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5)) %>%
  as.data.frame()

temp_peak <- dengue_prediction_results %>%
  filter(total_cases >= upper_cutoff) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score),
    mean = mean(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(week_type = "Low Incidence",
  #   reduced_model_descriptor = substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5)) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat("Dengue & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}

cat("\\cline{2-9}\n")

# max_row <- which.max(temp_nadir$mean)
# min_row <- which.min(temp_nadir$min)
# 
# for(row_num in 1:6) {
#   if(identical(row_num, 1L)) {
#     cat(" & Low Incidence & ")
#   } else {
#     cat(" &  & ")
#   }
#   if(identical(row_num, max_row)) {
#     row_open_string <- max_open_string <- "\\textbf{"
#     row_close_string <- max_close_string <- "}"
#   } else {
#     row_open_string <- max_open_string <- ""
#     row_close_string <- max_close_string <- ""
#   }
#   if(identical(row_num, min_row)) {
#     min_open_string <- "\\underline{\\emph{"
#     min_close_string <- "}}"
#     row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
#     row_close_string <- paste0(row_close_string, "}}")
#   } else {
#     min_open_string <- ""
#     min_close_string <- ""
#     row_open_string <- row_open_string
#     row_close_string <- row_close_string
#   }
#   cat(paste0(
#     row_open_string,
#     as.character(temp_nadir[row_num, "reduced_model_descriptor"]),
#     row_close_string,
#     " & ",
#     min_open_string,
#     format(round(temp_nadir[row_num, "min"], 2), nsmall = 2),
#     min_close_string,
#     " & ",
#     format(round(temp_nadir[row_num, "Q1"], 2), nsmall = 2),
#     " & ",
#     format(round(temp_nadir[row_num, "median"], 2), nsmall = 2),
#     " & ",
#     max_open_string,
#     format(round(temp_nadir[row_num, "mean"], 2), nsmall = 2),
#     max_close_string,
#     " & ",
#     format(round(temp_nadir[row_num, "Q3"], 2), nsmall = 2),
#     " & ",
#     format(round(temp_nadir[row_num, "max"], 2), nsmall = 2),
#     "\\\\\n"
#   ))
# }

cat("\\cline{2-9}\n")

max_row <- which.max(temp_peak$mean)
min_row <- which.min(temp_peak$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat(" & High Incidence & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  
  cat(paste0(
    row_open_string,
    as.character(temp_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}

cat("\\midrule\n");


temp <- ili_prediction_results %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    mean = mean(log_score),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(reduced_model_descriptor = substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5)) %>%
  as.data.frame()

lower_cutoff <- max(ili_prediction_results$weighted_ili) / 3
upper_cutoff <- 2 * lower_cutoff

temp_nadir <- ili_prediction_results %>%
  filter(weighted_ili <= lower_cutoff) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score),
    mean = mean(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(reduced_model_descriptor = substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5)) %>%
  as.data.frame()

temp_peak <- ili_prediction_results %>%
  filter(weighted_ili >= upper_cutoff) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(log_score),
    Q1 = quantile(log_score, probs = 0.25),
    median = quantile(log_score, probs = 0.5),
    Q3 = quantile(log_score, probs = 0.75),
    max = max(log_score),
    mean = mean(log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor)) %>%
  # mutate(reduced_model_descriptor = substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5)) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:5) {
  if(identical(row_num, 1L)) {
    cat("Influenza & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}

cat("\\cline{2-9}\n")

# max_row <- which.max(temp_nadir$mean)
# min_row <- which.min(temp_nadir$min)
# 
# for(row_num in 1:5) {
#   if(identical(row_num, 1L)) {
#     cat(" & Low Incidence & ")
#   } else {
#     cat(" &  & ")
#   }
#   if(identical(row_num, max_row)) {
#     row_open_string <- max_open_string <- "\\textbf{"
#     row_close_string <- max_close_string <- "}"
#   } else {
#     row_open_string <- max_open_string <- ""
#     row_close_string <- max_close_string <- ""
#   }
#   if(identical(row_num, min_row)) {
#     min_open_string <- "\\underline{\\emph{"
#     min_close_string <- "}}"
#     row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
#     row_close_string <- paste0(row_close_string, "}}")
#   } else {
#     min_open_string <- ""
#     min_close_string <- ""
#     row_open_string <- row_open_string
#     row_close_string <- row_close_string
#   }
#   cat(paste0(
#     row_open_string,
#     as.character(temp_nadir[row_num, "reduced_model_descriptor"]),
#     row_close_string,
#     " & ",
#     min_open_string,
#     format(round(temp_nadir[row_num, "min"], 2), nsmall = 2),
#     min_close_string,
#     " & ",
#     format(round(temp_nadir[row_num, "Q1"], 2), nsmall = 2),
#     " & ",
#     format(round(temp_nadir[row_num, "median"], 2), nsmall = 2),
#     " & ",
#     max_open_string,
#     format(round(temp_nadir[row_num, "mean"], 2), nsmall = 2),
#     max_close_string,
#     " & ",
#     format(round(temp_nadir[row_num, "Q3"], 2), nsmall = 2),
#     " & ",
#     format(round(temp_nadir[row_num, "max"], 2), nsmall = 2),
#     "\\\\\n"
#   ))
# }

cat("\\cline{2-9}\n")

max_row <- which.max(temp_peak$mean)
min_row <- which.min(temp_peak$min)

for(row_num in 1:5) {
  if(identical(row_num, 1L)) {
    cat(" & High Incidence & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  
  cat(paste0(
    row_open_string,
    as.character(temp_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
@
\bottomrule
\end{tabular}
\caption{Summaries of model performance for predictions of incidence in individual weeks.
The ``All Weeks'' group summarizes log scores for all combinations of prediction horizon and target week in the test period;
the ``High Incidence" group summarizes log scores for predictions of indience in weeks where the
observed incidence was at least two thirds of the maximum weekly incidence in the test period.
The model in \textbf{bold} font
had the highest mean log score within each combination of disease and weeks subset.  The model in \underline{\emph{italicized and underlined}} font had the lowest minimum log score within each combination of disease and weeks subset.  In some cases, the same model had both the highest average log score and the lowest worst-case log score.
}
\label{tbl:IndWeeksIncidenceResults}
\end{table}

\begin{figure}
<<DengueDataRibbonsPredictionPlot95Intervals, echo = FALSE, fig.height = 8.6, dev = "cairo_ps">>=
color_palette_fill <- c("#D55E00", "#0072B2", "#FFFFFF")
color_palette_colour <- c("#D55E00", "#0072B2", "#000000")

dengue_ribbons_df <- dengue_prediction_results %>%
    select(prediction_time,
        prediction_horizon,
        full_model_descriptor,
        model,
        interval_pred_lb_95:interval_pred_ub_50) %>%
    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
    spread(bound_type, predictive_value)

dengue_ribbons_df <- rbind.fill(
  dengue_ribbons_df,
  dengue_ribbons_df %>%
    filter(model == "HHH4") %>%
    mutate(model = "Observed\nIncidence",
      full_model_descriptor == "Observed\nIncidence")
)

phs_used <- c(1, 6, 26)
models_used <- c("SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "Observed\nIncidence")
pi_type <- "95"
#pi_type <- "50"

reduced_dengue_sj <-   dengue_sj %>%
    filter(dengue_sj$season %in% paste0(2009:2012, "/", 2010:2013))
dengue_prediction_results_for_plot <- rbind.fill(
  dengue_prediction_results,
  data.frame(
      prediction_time = rep(reduced_dengue_sj$time, each = length(phs_used)),
      pt_pred = rep(reduced_dengue_sj$total_cases, each = length(phs_used)),
      prediction_horizon = rep(phs_used, times = nrow(reduced_dengue_sj)),
      full_model_descriptor = "Observed\nIncidence",
      model = "Observed\nIncidence"
  )
)

p_dengue <- ggplot() +
    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model, alpha = model),
        size = 0,
        data = dengue_ribbons_df[dengue_ribbons_df$prediction_horizon %in% phs_used &
                dengue_ribbons_df$full_model_descriptor %in% models_used &
                dengue_ribbons_df$interval_type == pi_type, ]) +
    # geom_line(aes(x = time, y = total_cases), data = dengue_sj[dengue_sj$season %in% paste0(2009:2012, "/", 2010:2013), ]) +
    geom_line(aes(x = prediction_time, y = pt_pred, colour = model, linetype = model, size = model),
#        size = 1,
        data = dengue_prediction_results_for_plot[dengue_prediction_results_for_plot$prediction_horizon %in% phs_used &
                dengue_prediction_results_for_plot$full_model_descriptor %in% models_used, ]) +
    scale_alpha_discrete("Model",
        limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
        range = c(0.45, 0.2, 0)) +
    scale_fill_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = color_palette_fill) +
    scale_colour_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = color_palette_colour) +
    scale_size_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = c(1, 1, 0.25)) +
    scale_linetype_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = c(1, 2, 1)) +
    facet_wrap( ~ prediction_horizon, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Prediction Horizon = ", labels))
            })) +
    xlab("Prediction Time") +
    ylab("Reported Cases") +
    ggtitle("(a) Point and 95% Interval Predictions, Dengue Fever") +
    theme_bw(base_size = 11)

ili_ribbons_df <- ili_prediction_results %>%
    select(prediction_time,
        prediction_horizon,
        full_model_descriptor,
        model,
        interval_pred_lb_95:interval_pred_ub_50) %>%
    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
    spread(bound_type, predictive_value)

ili_ribbons_df <- rbind.fill(
  ili_ribbons_df,
  ili_ribbons_df %>%
    filter(model == "SARIMA") %>%
    mutate(model = "Observed\nIncidence",
      full_model_descriptor == "Observed\nIncidence")
)

reduced_ili_national <-   ili_national %>%
    filter(ili_national$year %in% 2011:2014)
ili_prediction_results_for_plot <- rbind.fill(
  ili_prediction_results,
  data.frame(
      prediction_time = rep(reduced_ili_national$time, each = length(phs_used)),
      pt_pred = rep(reduced_ili_national$weighted_ili, each = length(phs_used)),
      prediction_horizon = rep(phs_used, times = nrow(reduced_ili_national)),
      full_model_descriptor = "Observed\nIncidence",
      model = "Observed\nIncidence"
  )
)


models_used <- c("SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "Observed\nIncidence")

p_ili <- ggplot() +
    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model, alpha = model),
        size = 0,
        data = ili_ribbons_df[ili_ribbons_df$prediction_horizon %in% phs_used &
                ili_ribbons_df$full_model_descriptor %in% models_used &
                ili_ribbons_df$interval_type == pi_type, ]) +
#    geom_line(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2011:2014, ]) +
    geom_line(aes(x = prediction_time, y = pt_pred, colour = model, linetype = model, size = model),
#        size = 1,
        data = ili_prediction_results_for_plot[ili_prediction_results_for_plot$prediction_horizon %in% phs_used &
                ili_prediction_results_for_plot$full_model_descriptor %in% models_used, ]) +
    scale_alpha_discrete("Model",
        limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
        range = c(0.45, 0.2, 0)) +
    scale_fill_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = color_palette_fill) +
    scale_colour_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = color_palette_colour) +
    scale_size_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = c(1, 1, 0.25)) +
    scale_linetype_manual("Model",
      limits = c("KCDE", "SARIMA", "Observed\nIncidence"),
      breaks = c("KCDE", "SARIMA", "Observed\nIncidence"),
      values = c(1, 2, 1)) +
    facet_wrap( ~ prediction_horizon, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Prediction Horizon = ", labels))
            })) +
    xlab("Prediction Time") +
    ylab("Weighted Influenza-like Illness") +
    ggtitle("(b) Point and 95% Interval Predictions, Influenza") +
    theme_bw(base_size = 11)

grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 1,
            heights = unit(c(1, 1), c("null", "null")),
            widths = unit(c(1), c("null")))))
print(p_dengue, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(p_ili, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
@
\caption{Plots of point and interval predictions from SARIMA and the
Periodic, Full Bandwidth KCDE model.  The point prediction is the median of the predictive
distribution for incidence in the given week.  The interval prediction is a percentile interval; for example, the endpoints of the 95\% prediction interval are the 2.5th percentile and the 97.5th percentile of the predictive distribution.}
\label{fig:DengueRibbonsPredictions}
\end{figure}


\begin{table}[ht]
\centering
\begin{tabular}{llll}
\toprule
         &       & \multicolumn{2}{c}{Nominal Coverage}   \\
\cline{3-4}
 Disease & Model & 50\% & 95\% \\ 
  \hline
<<CoverageTable2, echo = FALSE, results = "asis">>=
ili_ribbons_df <- ili_prediction_results %>%
    select(prediction_time,
        prediction_horizon,
        full_model_descriptor,
#        reduced_model_descriptor,
        model,
        interval_pred_lb_95:interval_pred_ub_50) %>%
    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
    spread(bound_type, predictive_value)

ili_coverage <- left_join(ili_ribbons_df %>% mutate(time = prediction_time),
    ili_national,
    by = "time") %>%
  mutate(in_interval = (weighted_ili <= upper & weighted_ili >= lower)) %>%
  group_by(full_model_descriptor, interval_type) %>%
  summarize(coverage = mean(in_interval) * 100) %>%
  mutate(disease = "Influenza")

 
dengue_coverage <- left_join(dengue_ribbons_df %>% mutate(time = prediction_time),
  dengue_sj,
  by = "time") %>%
  mutate(in_interval = (total_cases <= upper & total_cases >= lower)) %>%
  group_by(full_model_descriptor, interval_type) %>%
  summarize(coverage = mean(in_interval) * 100) %>%
  mutate(disease = "Dengue")

combined_coverage <- rbind.fill(ili_coverage, dengue_coverage) %>%
    mutate(
        periodic = grepl("periodic_TRUE", full_model_descriptor),
        bw_full = grepl("bw_full", full_model_descriptor)
    )

combined_coverage$reduced_model_descriptor <- "Null KCDE"
combined_coverage$reduced_model_descriptor[
    combined_coverage$periodic & !combined_coverage$bw_full] <-
    "Periodic KCDE"
combined_coverage$reduced_model_descriptor[
    !combined_coverage$periodic & combined_coverage$bw_full] <-
    "Full Bandwidth KCDE"
combined_coverage$reduced_model_descriptor[
    combined_coverage$periodic & combined_coverage$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
combined_coverage$reduced_model_descriptor[
    combined_coverage$full_model_descriptor == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
    ] <- "HHH4"
combined_coverage$reduced_model_descriptor[
    combined_coverage$full_model_descriptor == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
    ] <- "SARIMA"
combined_coverage$reduced_model_descriptor <-
    factor(combined_coverage$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))


# ggplot(res) +
#   geom_line(aes(x = interval_type, y = coverage, colour = full_model_descriptor, group = full_model_descriptor)) +
#   theme_bw()

target_coverage <- data.frame(
  interval_type = as.character(c(50, 50, 95, 95)),
  coverage = c(0.50, 0.50, 0.95, 0.95) * 100,
  disease = c("Dengue", "Influenza", "Dengue", "Influenza")
)

coverage_for_table <- combined_coverage[, c("disease", "interval_type", "reduced_model_descriptor", "coverage")] %>%
  transmute(Disease = disease,
    Model = reduced_model_descriptor,
    `Expected\nCoverage` = interval_type,
    `Actual\nCoverage` = coverage) %>%
  arrange(Disease, `Expected\nCoverage`, Model)

row_num_groups_list <- list(1:6, 7:12, 13:17, 18:22)
best_in_groups <- sapply(seq_along(row_num_groups_list),
  function(row_num_group_ind) {
    row_num_group <- row_num_groups_list[[row_num_group_ind]]
    return(row_num_group[
      which.min(
        abs(coverage_for_table[row_num_group, 4] -
            as.numeric(coverage_for_table[row_num_group, 3]))
      )
    ])
  })
for(row_num_group_ind in c(1L, 3L)) {
  row_num_group <- row_num_groups_list[[row_num_group_ind]]
  best_in_group <- best_in_groups[row_num_group_ind]
  second_row_num_group <- row_num_groups_list[[row_num_group_ind + 1]]
  best_in_second_group <- best_in_groups[row_num_group_ind + 1]
  for(row_num_ind in seq_along(row_num_group)) {
    row_num <- row_num_group[row_num_ind]
    second_row_num <- second_row_num_group[row_num_ind]
    cat(paste0(
      ifelse(identical(row_num_ind, 1L),
        coverage_for_table[row_num, 1],
        ""
      ),
      " & ",
      coverage_for_table[row_num, 2], " & ",
#      coverage_for_table[row_num, 3], " & ",
      ifelse(best_in_group == row_num,
        "\\textbf{",
        ""),
      format(round(coverage_for_table[row_num, 4], 2), nsmall = 2),
      ifelse(best_in_group == row_num,
        "}",
        ""),
      " & ",
      ifelse(best_in_second_group == second_row_num,
        "\\textbf{",
        ""),
      format(round(coverage_for_table[second_row_num, 4], 2), nsmall = 2),
      ifelse(best_in_second_group == second_row_num,
        "}",
        ""),
      "\\\\\n"
    ))
  }
  if(identical(row_num_group_ind, 1L)) {
    cat("\\cline{1-4}\n")
  }
}
@
\bottomrule
\end{tabular}
\caption{Coverage rates for predictions of disease incidence in individual weeks during the test time frame.  For each model specification, we have obtained the overall proportion of predictive intervals that contained the realized outcome, combining across all prediction horizons and all times in the test period at which the prediction was made.  For each combination of disease and target coverage rate, the result for the model with actual coverage rate closest to the target coverate rate is highlighted.}
\label{tbl:CoverageResults}
\end{table}



\begin{table}[hp]
\centering
\begin{tabular}{lllllllll}
\toprule
         &                    &          &              \multicolumn{6}{c}{Summary of Log Scores} \\
\cline{4-9}
 Disease & Subset & Model & Min & Q1 & Q2 & Mean & Q3 & Max \\ 
  \hline
<<PerformanceSummaryTablePeakIncidence, echo = FALSE, results = "asis">>=
#difference ~ N(mean_difference_model_pair, rho^(target time difference) * phi^(analysis time difference))???
dengue_peak_week_results$before_peak_week <- FALSE
for(season in unique(dengue_peak_week_results$analysis_time_season)) {
    dengue_peak_week_results$before_peak_week[
        dengue_peak_week_results$analysis_time_season == season &
            dengue_peak_week_results$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

temp <- dengue_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "HHH4", "SARIMA", "Equal Bin Probabilities"))) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_height_log_score),
    Q1 = quantile(peak_height_log_score, probs = 0.25),
    median = quantile(peak_height_log_score, probs = 0.5),
    Q3 = quantile(peak_height_log_score, probs = 0.75),
    max = max(peak_height_log_score),
    mean = mean(peak_height_log_score)
  ) %>%
  ungroup() %>%
  # mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
  #   week_type = "All Weeks") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor %in% c("SARIMA", "HHH4"),
  #       reduced_model_descriptor,
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

temp_before_peak <- dengue_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "HHH4", "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(before_peak_week) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_height_log_score),
    Q1 = quantile(peak_height_log_score, probs = 0.25),
    median = quantile(peak_height_log_score, probs = 0.5),
    Q3 = quantile(peak_height_log_score, probs = 0.75),
    max = max(peak_height_log_score),
    mean = mean(peak_height_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "Before Peak") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor %in% c("SARIMA", "HHH4"),
  #       reduced_model_descriptor,
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:7) {
  if(identical(row_num, 1L)) {
    cat("Dengue & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
cat("\\cline{2-9}\n")

max_row <- which.max(temp_before_peak$mean)
min_row <- which.min(temp_before_peak$min)

for(row_num in 1:7) {
  if(identical(row_num, 1L)) {
    cat(" & Before Peak & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp_before_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_before_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_before_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
cat("\\midrule\n");


ili_peak_week_results$before_peak_week <- FALSE
for(season in unique(ili_peak_week_results$analysis_time_season)) {
    ili_peak_week_results$before_peak_week[
        ili_peak_week_results$analysis_time_season == season &
            ili_peak_week_results$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

temp <- ili_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "SARIMA", "Equal Bin Probabilities"))) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_height_log_score),
    Q1 = quantile(peak_height_log_score, probs = 0.25),
    median = quantile(peak_height_log_score, probs = 0.5),
    Q3 = quantile(peak_height_log_score, probs = 0.75),
    max = max(peak_height_log_score),
    mean = mean(peak_height_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "All Weeks") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor == "SARIMA",
  #       "SARIMA",
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

temp_before_peak <- ili_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(before_peak_week) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_height_log_score),
    Q1 = quantile(peak_height_log_score, probs = 0.25),
    median = quantile(peak_height_log_score, probs = 0.5),
    Q3 = quantile(peak_height_log_score, probs = 0.75),
    max = max(peak_height_log_score),
    mean = mean(peak_height_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "Before Peak") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor == "SARIMA",
  #       "SARIMA",
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat("Influenza & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}

cat("\\cline{2-9}\n")

max_row <- which.max(temp_before_peak$mean)
min_row <- which.min(temp_before_peak$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat(" & Before Peak & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp_before_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_before_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_before_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
@
\bottomrule
\end{tabular}
\caption{Summaries of model performance for predictions of incidence in the peak week.
The ``All Weeks'' group summarizes results for all combinations of 
target week in the test period and prediction horizon; the ``Before Peak" group summarizes results
for predictions in weeks before the actual peak for the given season.  The model in \textbf{bold} font
had the highest mean log score within each combination of disease and weeks subset.  The model in \underline{\emph{italicized and underlined}} font had the lowest minimum log score within each combination of disease and weeks subset.  In some cases, the same model had both the highest average log score and the lowest worst-case log score.}
\label{tbl:PeakIncidenceResults}
\end{table}



\begin{table}[hp]
\centering
\begin{tabular}{lllllllll}
\toprule
         &                    &          &              \multicolumn{6}{c}{Summary of Log Scores} \\
\cline{4-9}
 Disease & Subset & Model & Min & Q1 & Q2 & Mean & Q3 & Max \\ 
  \hline
<<PerformanceSummaryTablePeakTiming, echo = FALSE, results = "asis">>=
#difference ~ N(mean_difference_model_pair, rho^(target time difference) * phi^(analysis time difference))???
dengue_peak_week_results$before_peak_week <- FALSE
for(season in unique(dengue_peak_week_results$analysis_time_season)) {
    dengue_peak_week_results$before_peak_week[
        dengue_peak_week_results$analysis_time_season == season &
            dengue_peak_week_results$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

temp <- dengue_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "HHH4", "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(!(analysis_time_season == "2009/2010" & analysis_time_season_week == 1L)) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_week_log_score),
    Q1 = quantile(peak_week_log_score, probs = 0.25),
    median = quantile(peak_week_log_score, probs = 0.5),
    Q3 = quantile(peak_week_log_score, probs = 0.75),
    max = max(peak_week_log_score),
    mean = mean(peak_week_log_score)
  ) %>%
  ungroup() %>%
  # mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
  #   week_type = "All Weeks") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor %in% c("SARIMA", "HHH4"),
  #       reduced_model_descriptor,
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

temp_before_peak <- dengue_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "HHH4", "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(before_peak_week) %>%
  filter(!(analysis_time_season == "2009/2010" & analysis_time_season_week == 1L)) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_week_log_score),
    Q1 = quantile(peak_week_log_score, probs = 0.25),
    median = quantile(peak_week_log_score, probs = 0.5),
    Q3 = quantile(peak_week_log_score, probs = 0.75),
    max = max(peak_week_log_score),
    mean = mean(peak_week_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "Before Peak") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor %in% c("SARIMA", "HHH4"),
  #       reduced_model_descriptor,
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:7) {
  if(identical(row_num, 1L)) {
    cat("Dengue & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
cat("\\cline{2-9}\n")

max_row <- which.max(temp_before_peak$mean)
min_row <- which.min(temp_before_peak$min)

for(row_num in 1:7) {
  if(identical(row_num, 1L)) {
    cat(" & Before Peak & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp_before_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_before_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_before_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
cat("\\midrule\n");


ili_peak_week_results$before_peak_week <- FALSE
for(season in unique(ili_peak_week_results$analysis_time_season)) {
    ili_peak_week_results$before_peak_week[
        ili_peak_week_results$analysis_time_season == season &
            ili_peak_week_results$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

temp <- ili_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(!(analysis_time_season == "2009/2010" & analysis_time_season_week == 1L)) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_week_log_score),
    Q1 = quantile(peak_week_log_score, probs = 0.25),
    median = quantile(peak_week_log_score, probs = 0.5),
    Q3 = quantile(peak_week_log_score, probs = 0.75),
    max = max(peak_week_log_score),
    mean = mean(peak_week_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "All Weeks") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor == "SARIMA",
  #       "SARIMA",
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

temp_before_peak <- ili_peak_week_results %>%
  mutate(reduced_model_descriptor =
      factor(reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE",
          "Periodic KCDE", "Periodic, Full Bandwidth KCDE",
          "SARIMA", "Equal Bin Probabilities"))) %>%
  filter(before_peak_week) %>%
  filter(!(analysis_time_season == "2009/2010" & analysis_time_season_week == 1L)) %>%
  group_by(reduced_model_descriptor) %>%
  summarize(
    min = min(peak_week_log_score),
    Q1 = quantile(peak_week_log_score, probs = 0.25),
    median = quantile(peak_week_log_score, probs = 0.5),
    Q3 = quantile(peak_week_log_score, probs = 0.75),
    max = max(peak_week_log_score),
    mean = mean(peak_week_log_score)
  ) %>%
  ungroup() %>%
  mutate(reduced_model_descriptor = as.character(reduced_model_descriptor),
    week_type = "Before Peak") %>%
  # mutate(reduced_model_descriptor = 
  #     ifelse(reduced_model_descriptor == "SARIMA",
  #       "SARIMA",
  #       substr(reduced_model_descriptor, 1, nchar(reduced_model_descriptor) - 5))) %>%
  as.data.frame()

max_row <- which.max(temp$mean)
min_row <- which.min(temp$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat("Influenza & All Weeks & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}

cat("\\cline{2-9}\n")

max_row <- which.max(temp_before_peak$mean)
min_row <- which.min(temp_before_peak$min)

for(row_num in 1:6) {
  if(identical(row_num, 1L)) {
    cat(" & Before Peak & ")
  } else {
    cat(" &  & ")
  }
  if(identical(row_num, max_row)) {
    row_open_string <- max_open_string <- "\\textbf{"
    row_close_string <- max_close_string <- "}"
  } else {
    row_open_string <- max_open_string <- ""
    row_close_string <- max_close_string <- ""
  }
  if(identical(row_num, min_row)) {
    min_open_string <- "\\underline{\\emph{"
    min_close_string <- "}}"
    row_open_string <- paste0(row_open_string, "\\underline{\\emph{")
    row_close_string <- paste0(row_close_string, "}}")
  } else {
    min_open_string <- ""
    min_close_string <- ""
    row_open_string <- row_open_string
    row_close_string <- row_close_string
  }
  cat(paste0(
    row_open_string,
    as.character(temp_before_peak[row_num, "reduced_model_descriptor"]),
    row_close_string,
    " & ",
    min_open_string,
    format(round(temp_before_peak[row_num, "min"], 2), nsmall = 2),
    min_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q1"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "median"], 2), nsmall = 2),
    " & ",
    max_open_string,
    format(round(temp_before_peak[row_num, "mean"], 2), nsmall = 2),
    max_close_string,
    " & ",
    format(round(temp_before_peak[row_num, "Q3"], 2), nsmall = 2),
    " & ",
    format(round(temp_before_peak[row_num, "max"], 2), nsmall = 2),
    "\\\\\n"
  ))
}
@
\bottomrule
\end{tabular}
\caption{Summaries of model performance for predictions of peak week timing.
The ``All Weeks'' group summarizes results for all combinations of 
target week in the test period and prediction horizon; the ``Before Peak" group summarizes results
for predictions in weeks before the actual peak for the given season.  The model in \textbf{bold} font
had the highest mean log score within each combination of disease and weeks subset.  The model in \underline{\emph{italicized and underlined}} font had the lowest minimum log score within each combination of disease and weeks subset.  In some cases, the same model had both the highest average log score and the lowest worst-case log score.}
\label{tbl:PeakTimingResults}
\end{table}



\begin{figure}
<<PeakIncidenceLogScoreDiffFromEqualBoxplots, echo = FALSE, fig.keep = "last", fig.height = 8.3, dev = "postscript">>=
dengue_peak_incidence_log_score_diffs_from_equal_wide <- dengue_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
    spread(full_model_descriptor, peak_height_log_score)

dengue_peak_incidence_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
    dengue_peak_incidence_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
    dengue_peak_incidence_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

dengue_peak_incidence_log_score_diffs_from_equal_long <- dengue_peak_incidence_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & !dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$periodic & dengue_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_incidence_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(dengue_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)) {
    dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week[
        dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season == season &
            dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

#ggplot() +
#    geom_boxplot(aes(x = reduced_model_descriptor, y = log_score_difference),
#        data = dengue_peak_incidence_log_score_diffs_from_equal_long[dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ]) +
#    facet_wrap(~ analysis_time_season) +
#    theme_bw()



dengue_prediction_log_score_diffs_from_hhh4_wide <- dengue_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

dengue_prediction_log_score_diffs_from_hhh4_wide[, unique(dengue_prediction_results$full_model_descriptor)] <-
    dengue_prediction_log_score_diffs_from_hhh4_wide[, unique(dengue_prediction_results$full_model_descriptor)] -
    dengue_prediction_log_score_diffs_from_hhh4_wide[, "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

dengue_prediction_log_score_diffs_from_hhh4_long <- dengue_prediction_log_score_diffs_from_hhh4_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor <- "Null KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$periodic & !dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Periodic KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    !dengue_prediction_log_score_diffs_from_hhh4_long$periodic & dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$periodic & dengue_prediction_log_score_diffs_from_hhh4_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_hhh4_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor <-
    factor(dengue_prediction_log_score_diffs_from_hhh4_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "SARIMA"))


ili_peak_incidence_log_score_diffs_from_equal_wide <- ili_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
    spread(full_model_descriptor, peak_height_log_score)

ili_peak_incidence_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
    ili_peak_incidence_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
    ili_peak_incidence_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

ili_peak_incidence_log_score_diffs_from_equal_long <- ili_peak_incidence_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$periodic & !ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !ili_peak_incidence_log_score_diffs_from_equal_long$periodic & ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$periodic & ili_peak_incidence_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_incidence_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(ili_peak_incidence_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)) {
    ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week[
        ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season == season &
            ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}

#ggplot() +
#    geom_boxplot(aes(x = reduced_model_descriptor, y = log_score_difference),
#        data = ili_peak_incidence_log_score_diffs_from_equal_long[ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ]) +
#    facet_wrap(~ analysis_time_season) +
#    theme_bw()


ili_peak_incidence_log_score_diffs_from_equal_long$data_set_and_season <-
    paste0("Influenza\n", ili_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)
dengue_peak_incidence_log_score_diffs_from_equal_long$data_set_and_season <-
    paste0("Dengue\n", dengue_peak_incidence_log_score_diffs_from_equal_long$analysis_time_season)


combined_test_data <- rbind(
    dengue_sj[dengue_sj$season %in% paste0(2009:2012, "/", 2010:2013), c("total_cases", "season", "season_week")] %>%
        `colnames<-`(c("incidence", "season", "season_week")) %>%
        mutate(data_set_and_season = paste0("Dengue: ", season)),
    ili_national[ili_national$season %in% paste0(2011:2013, "/", 2012:2014), c("weighted_ili", "season", "season_week")] %>%
        `colnames<-`(c("incidence", "season", "season_week")) %>%
        mutate(data_set_and_season = paste0("Influenza: ", season))
)



dengue_peak_timing_log_score_diffs_from_equal_wide <- dengue_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
    spread(full_model_descriptor, peak_week_log_score)

dengue_peak_timing_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
    dengue_peak_timing_log_score_diffs_from_equal_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
    dengue_peak_timing_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

dengue_peak_timing_log_score_diffs_from_equal_long <- dengue_peak_timing_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$periodic & !dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !dengue_peak_timing_log_score_diffs_from_equal_long$periodic & dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$periodic & dengue_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    dengue_peak_timing_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(dengue_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)) {
    dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week[
        dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season == season &
            dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season_week <
            dengue_peak_week_times$peak_week[dengue_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}



ili_peak_timing_log_score_diffs_from_equal_wide <- ili_peak_week_results %>%
    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
    spread(full_model_descriptor, peak_week_log_score)

ili_peak_timing_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
    ili_peak_timing_log_score_diffs_from_equal_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
    ili_peak_timing_log_score_diffs_from_equal_wide[, "Equal Bin Probabilities"]

ili_peak_timing_log_score_diffs_from_equal_long <- ili_peak_timing_log_score_diffs_from_equal_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <- "Null KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$periodic & !ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    !ili_peak_timing_log_score_diffs_from_equal_long$periodic & ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Full Bandwidth KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$periodic & ili_peak_timing_log_score_diffs_from_equal_long$bw_full] <-
    "Periodic, Full Bandwidth KCDE"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$model == "HHH4-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "HHH4"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor[
    ili_peak_timing_log_score_diffs_from_equal_long$model == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"
] <- "SARIMA"
ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor <-
    factor(ili_peak_timing_log_score_diffs_from_equal_long$reduced_model_descriptor,
        levels = c("Null KCDE", "Full Bandwidth KCDE", "Periodic KCDE", "Periodic, Full Bandwidth KCDE", "HHH4", "SARIMA"))

ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week <- FALSE
for(season in unique(ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)) {
    ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week[
        ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season == season &
            ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season_week <
            ili_peak_week_times$peak_week[ili_peak_week_times$analysis_time_season == season]
    ] <- TRUE
}


ili_peak_timing_log_score_diffs_from_equal_long$data_set_and_season <-
    paste0("Influenza\n", ili_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)
dengue_peak_timing_log_score_diffs_from_equal_long$data_set_and_season <-
    paste0("Dengue\n", dengue_peak_timing_log_score_diffs_from_equal_long$analysis_time_season)

p_boxplots <- ggplot() +
    geom_hline(yintercept = 0) +
    geom_boxplot(aes(x = reduced_model_descriptor, y = log_score_difference),
        data = rbind.fill(
            rbind.fill(
                ili_peak_timing_log_score_diffs_from_equal_long[ili_peak_timing_log_score_diffs_from_equal_long$before_peak_week, ],
                dengue_peak_timing_log_score_diffs_from_equal_long[dengue_peak_timing_log_score_diffs_from_equal_long$before_peak_week, ]
            ) %>%
                mutate(prediction_target = "Peak Timing"),
            rbind.fill(
                ili_peak_incidence_log_score_diffs_from_equal_long[ili_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ],
                dengue_peak_incidence_log_score_diffs_from_equal_long[dengue_peak_incidence_log_score_diffs_from_equal_long$before_peak_week, ]
            ) %>%
                mutate(prediction_target = "Peak Incidence")
        )
    ) +
    facet_grid(data_set_and_season ~ prediction_target) +
    scale_x_discrete(labels = c("Null KCDE" = "Null KCDE",
            "Full Bandwidth KCDE" = "Full Bandwidth KCDE",
            "Periodic KCDE" = "Periodic KCDE",
            "Periodic, Full Bandwidth KCDE" = "Periodic,\nFull Bandwidth KCDE",
            "HHH4" = "HHH4",
            "SARIMA" = "SARIMA")) +
    xlab("Model") +
    ylab("Log Score Difference from Equal Bin Probabilities") +
#    ggtitle("Summary of Results for Peak Week Timing") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))


grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 10, ncol = 3,
            heights = unit(c(1, 0.37, rep(1, 7), 1.57), c("lines", rep("null", 9))),
            widths = unit(c(1, 1, 0.25), c("null", "lines", "null")))))
grid.text("Summary of Results for Peak Week Incidence and Timing",
    gp = gpar(fontsize = 14),
    vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
#grid.text("Observed\nIncidence",
#    gp = gpar(fontsize = 14),
#    vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
grid.text("Reported Weekly Incidence",
    rot = 90,
    gp = gpar(fontsize = 12),
    vp = viewport(layout.pos.row = 2:9, layout.pos.col = 2))
grid.text("Season Week",
#    just = "left",
    vjust = 0.5,
    hjust = 0.35,
    gp = gpar(fontsize = 12),
    vp = viewport(layout.pos.row = 9:10, layout.pos.col = 3))

suppressWarnings(print(p_boxplots, vp = viewport(layout.pos.row = 2:10, layout.pos.col = 1)))

data_set_season_combos <- unique(combined_test_data$data_set_and_season)
limits <- cbind(rep(0, 7), c(rep(300, 4), rep(6.2, 3)))
left_margins <- c(rep(-5, 4), rep(-1, 3))
for(i in seq_along(data_set_season_combos)) {
    p_1_season <-         ggplot(combined_test_data[combined_test_data$data_set_and_season == data_set_season_combos[i], ]) +
            geom_line(aes(x = season_week, y = incidence)) +
#            facet_wrap(~ data_set_and_season, ncol = 1) +
            ylim(limits[i, ]) +
            xlab("") +
            ylab("") +
            theme_bw() +
            theme(panel.margin = unit(0, "mm"),
                plot.margin = unit(c(0, 0, -9, left_margins[i]), "mm"))
    if(i < 7) {
        p_1_season <- p_1_season +
            theme(axis.text.x = element_blank(),
                axis.ticks.x = element_blank(),
                plot.margin = unit(c(0, 0, -5, left_margins[i]), "mm"))
    }
    print(
        p_1_season,
        vp = viewport(layout.pos.row = 2 + i, layout.pos.col = 3)
    )
}
@
\caption{A summary of performance of each method for predicting incidence in the peak week and
peak week timing.  Each boxplot summarizes all predictions made by a method in a given
season in weeks before the actual peak week for that season.  The vertical
axis is the difference in log scores between the given method and a naive approach assigning
equal probability to each week of the year.  Positive values indicate cases when
the method did better than using equal bin probabilities.  The plots on the right display the trajectory of
incidence over each season.}
\label{fig:CombinedPeakWeekTimingPredictionLogScores}
\end{figure}


\end{document}
