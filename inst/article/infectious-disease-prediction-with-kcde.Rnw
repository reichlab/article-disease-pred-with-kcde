\documentclass[Crown, sagev]{sagej}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage[list=off]{caption} % list=off option prevents errors when using math environments within captions


\include{GrandMacros}
\newcommand{\cdf}{{c.d.f.} }
\newcommand{\pdf}{{p.d.f.} }
\newcommand{\ind}{\mathbb{I}}

\begin{document}

\title{Infectious disease prediction with kernel conditional density
estimation}

\author{Evan L. Ray\affilnum{1},
Krzysztof Sakrejda\affilnum{1},
Stephen A. Lauer\affilnum{1},
Michael Johansen\affilnum{2} and
Nicholas G. Reich\affilnum{1}}

\affiliation{\affilnum{1}Department of Biostatistics and Epidemiology,
University of Massachusetts, Amherst\\
\affilnum{2}CDC, Puerto Rico}

\corrauth{Evan Ray, UMass Address Here}

\email{elray@umass.edu}

\begin{abstract}
Abstract
\end{abstract}
\keywords{copula, dengue fever, infectious disease, influenza, kernel
conditional density estimation, prediction}

\maketitle

<<knitrGlobalSetup, echo = FALSE>>=
library(cdcfluview)

library(stringr)

library(reshape2)
library(plyr)
suppressMessages(library(dplyr))
library(tidyr)
suppressMessages(library(lubridate))

library(ggplot2)
library(grid)

library(forecast)
library(kcde)
suppressMessages(suppressWarnings(library(pdtmvn)))

opts_chunk$set(cache = TRUE)
#opts_chunk$set(cache = TRUE, autodep = TRUE)
#opts_chunk$set(cache = FALSE)
@

\section{Introduction}
\label{sec:Intro}

Accurate prediction of infectious disease incidence is important for public
health officials planning disease prevention and control measures such as vector
control and increased use of personal protective equipment by medical
personnel during periods of high disease incidence
\cite{wallinga2010optimizingIDInterventions,
hatchett2007interventionsIntensity1918flu}.
Several quantities have emerged as being of particular utility in making these
planning decisions; in this article we focus
on measures of weekly incidence, the timing of the season peak, and incidence
in the peak week.  Predictive distributions for these quantities are preferred
to point predictions because they communicate uncertainty in the predictions and
give decision makers more information in cases where the predictive distribution
is skewed or has multiple modes.
In this work, we employ a non-parametric approach referred to as kernel
conditional density estimation (KCDE) to obtain separate predictive
distributions for disease incidence in each week of the season, and then combine
those marginal distributions using copulas to obtain joint predictive distributions for the
trajectory of incidence over the course of multiple weeks.  Predictive
distributions relating to the timing of and incidence at the peak week can be
obtained from this joint predictive distribution for the trajectory of
disease incidence.
In addition to the novel application of these methods to predicting disease incidence,
our contributions include the use of a periodic kernel specification to capture
seasonality in disease incidence and a method for obtaining multivariate
kernel functions that handle discrete data while allowing for a fully
parameterized bandwidth matrix.

KCDE has not previously been applied to obtain predictive distributions for
infectious disease incidence, but it has been successfully used for prediction
in other settings such as survival time of lung cancer patients\cite{hall2004crossvalidationKCDE}, female labor force
participation\cite{hall2004crossvalidationKCDE}, bond yields and value at risk
in financial markets\cite{fan2004crossvalidationKCDE}, and wind
power\cite{jeon2012KCDEWindPower} among others.  Although KCDE has not
previously been applied to predicting infectious disease, closely related methods for obtaining point predictions have been employed for
diseases such as measles\cite{sugihara1990nonlinearForecasting} and
influenza\cite{viboud2003predictionInfluenzaMoA}.  In the infectious disease
literature these methods have been referred to as state space reconstruction and
the method of analogues, but they amount to applications of nearest neighbors
regression.  The point prediction obtained from nearest neighbors
regression is equal to the expected value of the predictive distribution
obtained from KCDE if a particular kernel function is used in the formulation of
KCDE\cite{HastieTibshiraniESL}.  However, KCDE offers the advantage of providing
a complete predictive distribution rather than only a point prediction.  Methods
similar to those we explore in this article can also be formulated in the
Bayesian framework.  One example along these lines is Zhou et
{al.}\cite{zhou2015DirichletProcessCopulaAmphibianDiseaseArrival}, who model the
time to arrival of a disease in amphibian populations using Dirichlet
processes and copulas.

There is also a long history of using other modeling approaches such as
compartmental models for infectious disease prediction.  A full discussion of
those methods is beyond the scope of this article; see Brown et {al.}\cite{brown2016IDPredictionReview}
for a recent review.  KCDE is distinguished from these approaches in that it
makes minimal assumptions about the data generating process.  This can be
either an advantage or a disadvantage of KCDE.  In general, we would expect a
well-specified parametric model to outperform KCDE.  On the other hand, because
non-parametric approaches such as KCDE make fewer assumptions about the data
generating process, they may outperform incorrectly specified parametric models.
%In general, flexible non-parametric methods such as KCDE exhibit low
%bias but high variance.  If they are correctly specified, models with more
%structure may achieve reduced variance without introducing bias.
%On the other hand,   An
An evaluation of the benefits of an approach such as KCDE is therefore dependent
on the particular characteristics of the system being modeled, the data that are
available, and the quality of the models that are considered as alternatives. 
We will return to this point in our conclusions.

\lboxit{FIX CITATION INFO FOR LEXI'S REVIEW PAPER OR FIND AN ALTERNATIVE}

%There is an extensive literature on KCDE, focusing mainly on estimation of
%continuous conditional densities.  Here we offer a brief overview emphasizing
%the case with mixed continuous and discrete variables; Li and
%Racine\cite{li2007nonparametricEconometrics} offer a detailed discussion of this
%case.

%KCDE is a method for estimating the conditional distribution of
%a random vector $\bY$ given observations of another vector $\bX$.  

As we will describe in more detail below, KCDE estimates the conditional
density of a random vector $\bY$ given another vector
$\bX$ as a weighted sum of contributions from previously observed pairs $(\bx_t,
\by_t)$.  In our work,
$\bY$ is a measure of disease incidence at some future date (the
prediction target) and $\bX$ is a vector of predictive variables that we condition
on in order to make our prediction.  In our example applications,
$\bX$ includes observations of incidence over several recent time
points and variables indicating the time of year at which we are
making a prediction.  In general, it would be possible to include other
predictive variables such as weather covariates.

The observation weights and the scale of the
contribution from each observation to the final density are determined by a
kernel function.
To our knowledge, all previous authors using kernel methods to estimate multivariate
densities involving discrete variables have employed a kernel function that is a
product of univariate kernel functions
\cite{aitchison1976multivariateBinaryKernel, wang1981SmoothEstDiscreteDistn,
li2003nonparametricEstDistnsCategoricalContinuous,
ouyang2006crossvalidationEstDistnCategorical}.
Using a product kernel simplifies the mathemetical formulation of the kernel
function when discrete variables are present, but has the effect of forcing the
kernel function to be orientied in line with the coordinate axes.  In settings
with only continuous variables, asymptotic
analysis and experience with applications have shown that using a multivariate
kernel function with a bandwidth parameterization that allows for
other orientations can result in improved density estimates
in many cases (cite ***).  We introduce an approach to allowing for discrete
kernels with orientation by discretizing an underlying continuous kernel
function.

%A variety of functional forms have been proposed for this purpose, including
%geometric, triangular and Poisson among others
%\cite{aitchison1976multivariateBinaryKernel, wang1981SmoothEstDiscreteDistn, li2008nonparametricConditionalCDFQuantile, li2003nonparametricEstDistnsCategoricalContinuous,
%ouyang2006crossvalidationEstDistnCategorical}.

%\cite{wang1981SmoothEstDiscreteDistn},
%\cite{li2008nonparametricConditionalCDFQuantile}\cite{li2003nonparametricEstDistnsCategoricalContinuous},
% \cite{ouyang2006crossvalidationEstDistnCategorical}].

%One possibility for introducing orientation
%to the kernel function is to use a fully parameterized
%bandwidth matrix, allowing the kernel to be oriented in any direction.  Another
%common alternative is to fix the orientation to be in the directions of the
%eigenvectors of the sample covariance matrix.

%Estimation.  Two main strategies:  cross validation and rule-based.  Targets
% for optimization in cross-validation.  For
%estimating joint densities without
%conditioning, \cite{hart1990bandwidthEstDependentData} have shown that with
%dependent data, but small gains in the mean integrated square error of the
%density estimate relative to the true conditional density can be achieved by
%leaving out observations adjacent to the time point whose density is being
%estimated.

A limitation of kernel-based density estimation methods is that their
performance may not scale well with the dimension of the vector whose
distribution is being estimated.  This is particularly relevant in our
application, where it is desired to obtain joint predictive distributions for
disease incidence over the course of many weeks.  Copulas present one strategy
for estimating the joint distribution of moderate to high dimensional random
vectors, and work by specifying a relatively simple parametric model for the
dependence relations among those variables.  This simple dependence model
ties separate marginal distribution estimates together into a joint
distribution.  In our case, we obtain those marginal distribution estimates
through an application of KCDE to each prediction horizon.

%Specifically, we model the joint distribution of $Y_1,
%\ldots, Y_D$ by $F_{Y_1, \ldots, Y_D}(y_1, \ldots, y_D) = C(F_{Y_1}(y_1),
%\ldots, F_{Y_D}(y_D) ; \bxi)$.  Here $C: [0,1]^D \rightarrow [0,1]$ is
%the copula function depending on parameters $\bxi$ and mapping the vector
%of marginal {c.d.f.} values to the joint {c.d.f.} value.

%It would be possible to handle
%this task using just the formulation of KCDE we discussed above, but a
%direct application of this approach has some limitations.  First, the
%performance of kernel-based density estimation methods scales poorly with the
%dimension of the random vector whose density is being estimated (cite ***). 
%Second, we have found that different information is available in the data at
%different prediction horizons.  For example, we will demonstrate in our
%applications below that recently observed incidence is important for
%making short-term predictions, but terms capturing seasonality are more
%important for making long-term predictions.

%We make several contributions in this article.  First, we apply KCDE to
%prediction of infectious disease (specifically, Dengue fever and Influenza), a
%novel application of the method which gives rise to several challenges and
%opportunities.  Among these challenges is the fact that infectious disease
%incidence can be quite noisy, with a lot of variation around a longer term
%trend; we will illustrate this in two real data sets in Section ***.  As we will
%see, this noise can cause difficulty for the method when applied to prediction
%of future incidence directly from recent observations of incidence.  Our
%solution is to introduce an initial low-pass filtering step on the observed
%incidence counts that are used as inputs to the predictions.

%Another challenge is in capturing seasonality in disease incidence.  In order
%to address this, we consider the use of periodic functions of the observation
%time as conditioning variables.  Effectively, this means that we can base our
%predictive density on previous observations that have been recorded at the time
%of year we are interested in.

%A third challenge is that for some applications, observations of disease
%incidence may take the form of discrete counts (i.e., the number of new cases
%in the last week).  If these incidence counts span a large range, it may be
%reasonable to approximate their predictive distribution with a continuous
%density function.  However, in our data for Dengue fever, the number of cases
%often falls within a limited range so that this continuous approximation is not
%reasonable.  We address this by discretizing an underlying continuous density
%function.  To our knowledge, this approach is novel in the KCDE literature.

The remainder of this article is organized as follows.  First, we describe our
approach to prediction using KCDE and copulas, including development of the
discretized kernel function and periodic kernel function.  Next, we present the
results of a simulation study comparing the performance of KCDE for
estimating discrete distributions using a fully parameterized bandwidth
matrix and a diagonal bandwidth matrix.  We then illustrate our methods by applying them to predicting
disease incidence in two data sets: one with a measure of weekly incidence of
influenza in the United States and a second with a measure of weekly incidence
of Dengue fever in San Juan, Puerto Rico.  We conclude with a discussion of
these results.

\section{Method Description}
\label{sec:Methods}

In this Section, we give a detailed discussion of our methods. Suppose we
observe a measure $z_t$ of disease incidence at evenly spaced times indexed by $t = 1,
\ldots, T$.  We allow the incidence measure to be either continuous or discrete and  
use the term density to refer to the Radon-Nikodym derivative of the
(conditional) cumulative distribution function
with respect to an appropriately defined measure.  We will use a colon notation
to specify vectors: for example, $\bz_{s:t} = (z_s, \ldots, z_t)$.
Let $W$ denote the number of time points in a disease season (typically $W = 52$ if we have weekly data).  For each time $t^*$, let $S_{t^*}$
denote the time index of the last time point in the \textit{previous} season,
and let $H_{t^*} = W - (t^* - S_{t^*})$ denote the number of time points
remaining in the current season.  At time $T$, we obtain predictive distributions for each of
three prediction targets.  We frame these quantities as suitable integrals of a
predictive distribution $f(\bz_{(T + 1):(T + H_T)} | T, \bz_{1:T})$ for
the trajectory of incidence over all remaining weeks in the season:
\begin{enumerate}
  \item Incidence in a single future week:
    \begin{align}
    &f(z_{T + h} | T, \bz_{1:T}) \nonumber \\
    &\qquad = \int \cdots \int f(\bz_{(T + 1):(T + H_T)} | T, \bz_{1:T}) \, d z_{T + 1} \cdots d z_{T + h - 1} \, d z_{T + h + 1} \cdots d z_{T + H_T}
    \end{align}
  \item Timing of the peak week of the current season:
    \begin{align}
    &P(\text{Peak Week} = w) = P(Z_{S_T + w} \geq Z_{S_T + w^*} \forall w^* = 1, \ldots, W | T, \bz_{1:T}) \nonumber \\
    &\qquad = \int_{\{\bz_{(T + 1):(T + H_T)}: z_{S_T + w} \geq z_{S_T + w^*} \forall w^* = 1, \ldots, W\}} f(z_{(T + 1):(T + H_T)} | T, \bz_{1:T}) \, d \bz_{(T + 1):(T + H_T)}. \label{eqn:PeakPredTimingIntegral}
    \end{align}
  \item Binned incidence in the peak week of the current season:
    \begin{align}
    &P(\text{Incidence in Peak Week} \in [a, b]) = P(a \leq \max{w} Z_{S_T + w} \leq b | T, \bz_{1:T}) \nonumber \\
    &\qquad = \int_{\{(\bz_{(T + 1):(T + H_T)}): a \leq \max{w} Z_{S_T + w} \leq b\}} f(\bz_{(T + 1):(T + H_T)} | T, \bz_{1:T}) \, d \bz_{(T + 1):(T + H_T)}. \label{eqn:PeakPredIncidenceIntegral}
    \end{align}
\end{enumerate}

Our approach is to specify a model for $f(\bz_{(T + 1):(T + H_T)} | T, \bz_{1:T})$,
and then obtain predictive distributions for the desired quantities by computing
the integrals above.  In practice, we use Monte Carlo integration to evaluate
the integrals in Equations \eqref{eqn:PeakPredTimingIntegral} and
\eqref{eqn:PeakPredIncidenceIntegral} by sampling incidence trajectories from
the joint predictive distribution.

At time $t^*$, our model approximates $f(\bz_{(t^* + 1):(t^* + H_{t^*})} | t^*, \bz_{1:t^*})$
by conditioning only on the time at which we are making the predictions and observed incidence at
a few recent time points with lags given by the non-negative integers $l_1, \ldots, l_M$:
$f(\bz_{(t^* + 1):(t^* + H_{t^*})} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M})$.
The time $t^*$ is equal to $T$ when we are applying the method to perform prediction, but takes
other values in the estimation procedure we describe below.  The model
represents this density as follows:
\begin{align}
&f(z_{(t^* + 1):(t^* + H_{t^*})} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}) = \nonumber \\
&\qquad c^{H_{t^*}}\{f^{1}(z_{t^* + 1} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^1), \ldots, f^{H_{t^*}}(z_{t^* + H_{t^*}} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^H) ; \bxi^{H_{t^*}}\}. \label{eqn:ModelKCDECopula}
\end{align}
%In Equation~\eqref{eqn:ModelKCDECopula}, each
Here, each $f^{h}(z_{t^* + h} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M};
\btheta^h)$ is a predictive density for one prediction horizon obtained through KCDE.  The
distribution for each prediction horizon depends on a separate parameter vector $\btheta^h$.
The function $c^{H_{t^*}}(\cdot)$ is a copula
used to tie these marginal predictive densities together into a joint predictive
density, and depends on parameters $\bxi^{H_{t^*}}$.  In our
applications, we will obtain a separate copula fit for each trajectory length
$H_{t^*}$ of interest for the prediction task.

%Second, we discuss the use of a copula to tie
%these predictive distributions for individual prediction horizons into a joint
%distribution over the full range of horizons from 1 to $H$.  In order to handle
%both continuous and discrete random variables cleanly, we frame this discussion
%in terms of cumulative distribution functions.

%It would also be possible to
%condition on other covariates such as weather, but we have not pursued that line in this work.


%Throughout, we
%use the term density to refer to the Radon-Nikodym derivative of the
%cumulative distribution function with respect to an appropriately defined measure.
%In the case of random vectors where some
%components are continuous random variables and other are discrete, we take this
%measure to be a product of Lebesgue and counting measures for the corresponding
%random variables.  We use bold letters to indicate column vectors or matrices;
%capital letters are random variables and lower case letters are observations of those
%random variables.


%We estimate this joint predictive density in two stages.  First, we
%use KCDE to obtain separate predictive distributions for each prediction
%horizon $h = 1, \ldots, H$: $f(z_{T + h} | z_{1}, \ldots, z_{T}, T)$.  Next, we
%use a copula to combine these 



Broadly, estimation for the model parameters proceeds in two stages:
first we estimate the parameters for KCDE separately for each prediction
horizon $h = 1, \ldots, H_{t^*}$, and second we estimate the copula parameters while holding the KCDE
parameters fixed.  The efficiency of two-stage estimation procedures for copula
models has been studied in the literature both theoretically and through
simulation studies.  In general the two-stage approach may result in some loss
of efficiency relative to one-stage methods, but this efficiency loss is small
for some model specifications\cite{joe2005asymptoticEfficiencyTwoStageCopula}.
We pursue the two-stage strategy in this work because it results in a large
reduction in the computational cost of parameter estimation.

In the following subsections we describe the formulations of KCDE and
the copula in more detail and give our estimation strategy for each set of
model parameters.

\subsection{KCDE for Predictive Densities at Individual Prediction Horizons}
\label{subsec:Methods:KCDE}

We now discuss the methods we use to obtain the predictive density
$f^{h}(z_{t^* + h} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^h)$
for disease incidence at a particular horizon $h$ after time $t^*$.  In order to
simplify the notation we define two new variables: $Y_t^{h} = Z_{t + h}$ represents
the prediction target relative to time $t$, and $\bX_t = (t, Z_{t -
l_1}, \ldots, Z_{t - l_M})$ represents the vector of predictive
variables relative to time $t$.  With this notation, the distribution we wish to estimate is
$f^{h}(y_{t^*}^{h} | \bx_{t^*}; \btheta^h)$.

In order to estimate this distribution, we use the observed data to form the
pairs $(\bx_t, y_t^{h})$ for all $t = 1 + \max_m l_m, \ldots, T - h$;
for smaller values of $t$ there are not enough observations before $t$ to form
$\bx_t$ and for larger values of t there are not enough observations after $t$ to form
$y_t^{h}$.  We then regard these pairs as a (dependent) sample from the joint
distribution of $(\bX, Y^h)$ and estimate the conditional distribution of $Y^h | \bX$ via KCDE:
\begin{align}
&\widehat{f}^h(y^h_{t^*} | \bx_{t^*}) = \frac{\sum_{t \in \btau} K^{\bX, Y}\left\{(\bx_{t^*}, y^h_{t^*}), (\bx_t, y^h_t); \btheta^h \right\}}{\sum_{t \in \btau}K^{\bX}(\bx_{t^*}, \bx_t ; \btheta^h)} \label{eqn:KCDEDefinition} \\
&\qquad = \frac{\sum_{t \in \btau} K^{Y | \bX}(y^h_{t^*}, y^h_t | \bx_{t^*}, \bx_t; \btheta^h) K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h)}{\sum_{t \in \btau} K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h) } \label{eqn:KDESubKDEJtMarginal} \\
&\qquad = \sum_{t \in \btau} w^h_t K^{Y | \bX}(y^h_{t^*}, y^h_t | \bx_{t^*}, \bx_t; \btheta^h) \text{, where} \label{eqn:KDEwt} \\
&w^h_t = \frac{ K^{\bX}(\bx_{t^*}, \bx_t; \btheta^h) }{\sum_{s \in \btau} K^{\bX}(\bx_{t^*}, \bx_{s}; \btheta^h) } \label{eqn:KCDEWeightsDef}
\end{align}

Here we are working with a slightly restricted specification in which
the kernel function $K^{\bX, Y}$ can be written as the product of $K^{\bX}$ and a ``conditional kernel'' $K^{\bY|\bX}$.
With this restriction, we can
interpret $K^{\bX}$ as a weighting function determining how much each observation
$(\bx_t, y^h_t)$ contributes to our final density estimate according to how
similar $\bx_t$ is to the value $\bx_{t^*}$ that we are conditioning on.
For each $y^h_t$, $K^{\bY | \bX}$ is a density function that contributes
mass to the final density estimate near $y^h_t$.  The
parameters $\btheta^h$ control the locality and orientation of the weighting
function and the contributions to the density estimate from each observation.
In Equations \eqref{eqn:KCDEDefinition} through \eqref{eqn:KCDEWeightsDef},
$\btau \subseteq \{1 + \max_m l_m, \ldots, T - h\}$ indexes the subset of
observations used in obtaining the conditional density estimate; we return to
how this subset of observations is defined in the discussion of estimation
below.
%In practice, we have parameterized these matrices in terms of the Cholesky
% decomposition.
%; we further take the bandwidth matrix $\bB^X$ to be a sub-matrix of $\bB^{Y,X}$
%  Hall, Racine,
%and Li \cite{hall2004crossvalidationKCDE} say that this "does not adversely affect
%the rate of convergence of estimators..."

We take the kernel function $K^{Y, \bX}$ to be a product kernel with one
component being a periodic kernel in time and the other component capturing the
remaining covariates:
\begin{align}
&K^{\bX, Y}\left\{(\bx_{t^*}, y^h_{t^*}), (\bx_t, y^h_t); \btheta^h \right\} \nonumber \\
&\qquad = K^{\bX, Y}\left\{(t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}, z_{t^* + h}), (t, z_{t - l_1}, \ldots, z_{t - l_M}, z_{t + h}); \btheta^h \right\} \nonumber \\
&\qquad = K^{Periodic}(t^*, t; \btheta^h) K^{Incidence}\{(z_{t^* - l_1}, \ldots, z_{t^* - l_M}, z_{t^* + h}), (z_{t - l_1}, \ldots, z_{t - l_M}, z_{t + h}); \btheta^h\} \nonumber
\end{align}

The periodic kernel function was originally developed in the
literature on Gaussian Processes\cite{mackay1998introductionGP}, and is defined by
\begin{equation}
K^{Periodic}(t^*, t; \rho, \theta) = \exp\left[- \frac{\sin^2\{\rho (t^* - t)\}}{2\theta^2} \right]. \label{eqn:PeriodicKernel}
\end{equation}
We illustrate this kernel function in Figure \ref{fig:PeriodicKernelPlot}. 
It has two parameters: $\rho$, which determines the length of the
periodicity, and $\theta$, which determines the strength and locality
of this periodic component in computing the observation weights $w_t^h$.
In our applications, we have fixed $\rho = \pi / 52$, so that the kernel has
period of length 1 year with weekly data.  Using this periodic kernel provides a
mechanism to capture seasonality in disease incidence by allowing the
observation weights to depend on the similarity of the time of year that an
observation was collected and the time of year at which we are making a prediction.

%  In the final density estimate, $\btau$ typically includes all
%available time points, but proper subsets are used in the cross-validation
%procedures we discuss later for parameter estimation.

%In order to do this, we employ kernel density estimation.  Let $K^{\bY}(\by, \by^*,
%H^{\bY})$ and $K^{\bX}(\bx, \bx^*, H^{\bX})$ be kernel functions centered at
%$\by^*$ and $\bx^*$ respectively and with bandwidth matrices $H^{\bY}$ and
%$H^{\bX}$.  We estimate the conditional distribution of $\bY | \bX$ as follows:
%\begin{align}
%&\widehat{f}_{\bY|\bX}(\by | \bX = \bx) = \frac{\widehat{f}_{\bY, \bX}(\by, \bx)}{\widehat{f}_{\bX}(\bx)} \label{eqn:KDECondDef} \\
%&\qquad = \frac{\sum_{t \in \tau} K^{\bY, \bX}\{(\by, \bx), (\by_t, \bx_t), H^{\bY, \bX}\}}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
%&\qquad = \frac{\sum_{t \in \tau} K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) K^{\bX}(\bx, \bx_t, H^{\bX})}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
%&\qquad = \sum_{t \in \tau} w_t K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) \text{, where} \label{eqn:KDEwt} \\
%&w_t = \frac{ K^{\bX}(\bx, \bx_t, H^{\bX}) }{\sum_{t^* \in \tau} K^{\bX}(\bx, \bx_{t^*}, H^{\bX}) } \label{eqn:KDEWeightsDef}
%\end{align}

%In Equation~\eqref{eqn:KDECondDef}, we are making use of the fact that the
% conditional density for $\bY | \bX$ can be written as the quotient of the joint density for $(\bY, \bX)$ and the marginal density for $\bX$.  In Equation~\eqref{eqn:KDESubKDEJtMarginal}, we obtain separate kernel density estimates for the joint and marginal densities in this quotient.  In Equation~\eqref{eqn:KDEwt}, we rewrite this quotient by passing the denominator of Equation~\eqref{eqn:KDESubKDEJtMarginal} into the summation in the numerator.  We can interpret the result as a weighted kernel density estimate, where each observation $t \in \tau$ contributes a different amount to the final conditional density estimate.  The amount of the contribution from observation $t$ is given by the weight $w_t$, which effectively measures how similar $\bx_t$ is to the point $\bx$ at which we are estimating the conditional density.  If $\bx_t^{(\bl^{max})}$ is similar to $\bx_{t^*}^{(\bl^{max})}$, a large weight is assigned to $t$; if $\bx_t^{(\bl^{max})}$ is different from $\bx_{t^*}^{(\bl^{max})}$, a small weight is assigned to $t$.

%In kernel density estimation, it is generally required that the kernel
% functions integrate to $1$ in order to obtain valid density estimates.  However, after conditioning on $\bX$, it is no longer necessary that $K^{\bX}(\bx, \bx_t, H^{\bX})$ integrate to $1$.  In fact, as can be seen from Equation~\eqref{eqn:KDEWeightsDef}, any multiplicative constants of proportionality will cancel out when we form the observation weights.  We can therefore regard $K^{\bX}(\bx, \bx_t, H^{\bX})$ as a more general weighting function that measures the similarity between $\bx$ and $\bx_t$.  As we will see, eliminating the constraint that $K^{\bX}$ integrates to $1$ is a useful expansion the space of functions that can be used in calculating the observation weights.  However, we still require that $K^{\bY}$ integrates to $1$.

%In Equations \eqref{eqn:KDECondDef} through \eqref{eqn:KDEWeightsDef}, $\tau$
% is an index set of time points used in obtaining the density estimate.  In most settings, we can take $\tau = \{1 + P + L, \ldots, T\}$.  These are the time points for which we can form the lagged observation vector $\bx_t$ and the prediction target vector $\by_t$.  However, we will place additional restrictions on the time points included in $\tau$ in the cross-validation procedure discussed in Section \ref{sec:Estimation}.

%In Equation~\eqref{eqn:KCDEDefinition}, if both $K^{\bX, \bY}$ and $K^{\bX}$
%integrate to 1 with respect to $\bx$ and $y$ then the numerator is a kernel
%density estimate of the joint density of $\bX$ and $\bY$ and the denominator
%is a kernel density estimate of the marginal density of $\bX$; forming the quotient yields an
%estimate of the conditional density of $\bY | \bX$.  However, it is not strictly
%required that 

%\begin{equation}
%K^{\bX,\bY}\left\{(\bx', \by')', (\bx'_t, \by'_t)'; \bH^{\bX,\bY}\right\} = K^{\bX}\left\{\bx, \bx_t; \bH^{\bX}\right\} K^{\bY | \bX}\left\{\by, \by_t | \bx, \bx_t; \bH^{\bX,\bY}\right\}.
%\end{equation}



%With this restriction, we can rearrange Equation~\eqref{eqn:KCDEDefinition} to
%obtain
%\begin{align}
%\widehat{f}_{\bY|\bX}(\by | \bx) &= \sum_{t \in \btau} w_t K^{\bY | \bX}\left\{\by, \by_t | \bx, \bx_t; \bH^{\bX,\bY}\right\}, \text{ where} \label{eqn:KCDEDefinitionWeighted} \\
%w_t &= \frac{K^{\bX}\left\{\bx, \bx_t; \bH^{\bX}\right\}}{\sum_{t^* \in \btau} K^{\bX}\left\{\bx, \bx_{t^*}; \bH^{\bX}\right\}}.
%\end{align}

%In order to complete the formulation of the KCDE estimator, we must specify the
%kernel function.  We take this kernel to be a product of two components.  The
%first is a periodic kernel in the time at which we are making the prediction,
%and allows us to capture seasonality in disease incidence within the KCDE
%framework.  

\begin{figure}
\caption{The periodic kernel function illustrated as a function of time in
weeks with $\rho = \pi / 52$ and three possible values for the bandwidth
parameter $\theta$.}
\label{fig:PeriodicKernelPlot}
<<PeriodicKernelPlot, echo = FALSE, fig.height = 2>>=
plot_df <- data.frame(t=seq_len(5 * 52))

kernel_center <- plot_df$t[nrow(plot_df)]
rho <- pi / 52

h <- 0.1
plot_df$kernel_h0.1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 1
plot_df$kernel_h1 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

h <- 10
plot_df$kernel_h10 <- exp( -0.5 * (sin(rho * (kernel_center - plot_df$t)) / h)^2)

plot_df <- melt(plot_df, id.vars = "t")
plot_df$variable <- as.character(plot_df$variable)
plot_df$bandwidth <- "0.1"
plot_df$bandwidth[plot_df$variable == "kernel_h1"] <- "1"
plot_df$bandwidth[plot_df$variable == "kernel_h10"] <- "10"

ggplot(plot_df) +
    geom_line(aes(x = t, y = value, linetype = bandwidth, colour = bandwidth)) +
    geom_vline(xintercept = kernel_center) +
    scale_colour_manual("Bandwidth",
        breaks = c("0.1", "1", "10"),
        labels = c("0.1", "1", "10"),
        values = c("#E69F00", "#56B4E9", "#009E73")
    ) +
    scale_linetype("Bandwidth") +
    ylab("Kernel Function Value") +
    xlab("Time in Weeks") +
#    ggtitle("The Periodic Kernel") +
    theme_bw(base_size = 11)
@
\end{figure}

The second component of our kernel is a multivariate kernel incorporating
all of the other variables in $\bx_t$ and $y_t^h$.  In our
applications, these variables are measures of incidence; for brevity of
notation, we collect them in the vector $\tilde{\bz}_t = (z_{t - l_1}, \ldots,
z_{t - l_M}, z_{t + h})$.
These incidence measures are continuous in the application to Influenza and
discrete case counts in the application to Dengue fever.  In the continuous
case, we have used a multivariate log-normal kernel function.  This kernel
specification automatically handles the restriction that counts are
non-negative, and approximately captures the long tail in disease incidence that
we will illustrate in the applications Section below.  This kernel function has
the following functional form:
\begin{align}
&K^{Incidence}_{cont}(\tilde{\bz}_{t^*}, \tilde{\bz}_{t}; \bB^h) &= \frac{\exp\left[ -\frac{1}{2} \{\log(\tilde{\bz}_{t^*}) - \log(\tilde{\bz}_t)\}' \bB^{-1} \{\log(\tilde{\bz}_{t^*}) - \log(\tilde{\bz}_t)\} \right]}{(2 \pi)^{\frac{M+1}{2}} \vert \bB \vert^{\frac{1}{2}} z_{t^* + h} \prod_{m = 1}^M z_{t^* - l_m} }
\end{align}

In this expression, the $\log$ operator applied to a vector takes the log of
each component of that vector.  The matrix $\bB$ is the bandwidth matrix,
controling the orientation and scale of the kernel function as illustrated in
Figure~\ref{fig:IncidenceKernelPlots}.  This bandwidth matrix is parameterized by $\btheta^h$.  In this
work we have considered two parameterizations: a diagonal bandwidth matrix, and
a fully parameterized bandwidth based on the Cholesky decomposition.

In the discrete case, we obtain the kernel function by discretizing an
underlying continuous kernel function:
\begin{align*}
&K^{Incidence}_{disc}(\tilde{\bz}_{t^*}, \tilde{\bz}_{t}; \bB^h) = \int_{a_{z_{t^* - l_1}}}^{b_{z_{t^* - l_1}}} \cdots \int_{a_{z_{t^* + h}}}^{b_{z_{t^* + h}}} K^{Incidence}_{cont}(\tilde{\bz}_{t^*}, \tilde{\bz}_{t}; \bB^h) \, d z_{t^* - l_1} \cdots d z_{t^* + h}
\end{align*}
For each component variable in $(z_{t^* - l_1}, \ldots, z_{t^* - l_M}, z_{t^* +
h})$, we associate lower and upper bounds of integration $a_{z_j}$ and $b_{z_j}$
with each value in the domain of that random variable.  The value of the
kernel function is obtained by integrating over the hyper-rectangle
specified by these bounds.
In our application, the possible values of the random variables are non-negative
integer case counts.  In order to facilitate use of the log-normal
kernel, we add $0.5$ to the observed case counts; the
corresponding integration bounds are the non-negative integers as illustrated in
Figure~\ref{fig:IncidenceKernelPlots}.

\begin{figure}
\caption{Illustrations of $K^{Incidence}_{cont}$ and
$K^{Incidence}_{disc}$ in the bivariate case.  Solid lines show contours of the
continuous kernel function.  Grey dots indicate the value of the discrete kernel
function.  The value of the discrete kernel is
obtained by integrating the continuous kernel over regions as illustrated by the
dashed lines in panels (a) and (b).  In all panels the kernel function is
centered at $(2.5, 2.5)$.  In panels (a) and (b) the bandwidth matrix is 
$\begin{bmatrix}0.2 & 0 \\ 0 & 0.2\end{bmatrix}$, and in panels (c) and (d)
the bandwidth matrix is $\begin{bmatrix}0.2 & 0.15 \\ 0.15 & 0.2\end{bmatrix}$.
 We illustrate each case with both linear and logarithmic scale axes.}
\label{fig:IncidenceKernelPlots}
<<IncidenceKernelPlots, echo = FALSE, fig.keep = "last">>=
cont_grid_bounds <- c(0.01, 10)
cont_grid_size <- 101
x_cont_grid <- 
    expand.grid(
        seq(from = cont_grid_bounds[1], to = cont_grid_bounds[2], length = cont_grid_size),
        seq(from = cont_grid_bounds[1], to = cont_grid_bounds[2], length = cont_grid_size)
    ) %>%
    `colnames<-`(c("X1", "X2"))
disc_grid_bounds <- c(0.5, 9.5)
x_disc_grid <-
    expand.grid(
        seq(from = disc_grid_bounds[1], to = disc_grid_bounds[2], by = 1),
        seq(from = disc_grid_bounds[1], to = disc_grid_bounds[2], by = 1)
    ) %>%
    `colnames<-`(c("X1", "X2"))


#' Compute log(round(exp(x))) in such a way that the rounding function
#' always rounds up or down to an integer + 0.5, and
#' an integer always gets rounded up.
#' 
#' @param x numeric
#' 
#' @return floor(x) - 1
log_round_to_integer_plus_0.5_exp <- function(x) {
    exp_x <- exp(x) + 0.5
    
    inds_ceil <- exp_x - floor(exp_x) >= 0.5
    
    exp_x[inds_ceil] <- ceiling(exp_x[inds_ceil])
    exp_x[!inds_ceil] <- floor(exp_x[!inds_ceil])
    
    return(log(exp_x - 0.5))
}




continuous_density_df_a <- x_cont_grid %>%
    as.data.frame() %>%
    `$<-`("z",
#        sapply(seq_len(nrow(x_cont_grid)), function(x_grid_row_ind) {
#                log_pdtmvn_kernel(x = as.matrix(x_cont_grid)[x_grid_row_ind, , drop = FALSE],
                log_pdtmvn_mode_centered_kernel(x = x_cont_grid,
                    center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
                    bw = matrix(c(.2, 0, 0, .2), nrow = 2, ncol = 2),
                    bw_continuous = matrix(c(.2, 0, 0, .2), nrow = 2, ncol = 2),
                    continuous_vars = c("X1", "X2"),
                    discrete_vars = character(0),
                    continuous_var_col_inds = 1:2,
                    discrete_var_col_inds = integer(0),
                    discrete_var_range_fns = NULL,
#                    lower = c(X1 = log(0.5), X2 = log(0.5)),
                    lower = c(X1 = -Inf, X2 = -Inf),
                    upper = c(X1 = Inf, X2 = Inf),
                    x_names = c("X1", "X2"),
                    log = FALSE)
#            })
    )
discrete_density_df_a <- x_disc_grid %>%
    as.data.frame() %>%
    `colnames<-`(c("X1", "X2")) %>%
    `$<-`("z",
#        sapply(seq_len(nrow(x_disc_grid)), function(x_grid_row_ind) {
#                log_pdtmvn_kernel(x = as.matrix(x_disc_grid)[x_grid_row_ind, , drop = FALSE],
                log_pdtmvn_mode_centered_kernel(x = x_disc_grid,
                    center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
                    bw = matrix(c(.2, 0, 0, .2), nrow = 2, ncol = 2),
                    bw_continuous = matrix(0, nrow = 0, ncol = 0),
                    continuous_vars = character(0),
                    discrete_vars = c("X1", "X2"),
                    continuous_var_col_inds = integer(0),
                    discrete_var_col_inds = 1:2,
                    discrete_var_range_fns = list(
                        X1 = list(a = function(x) {
                                return(log(exp(x) - 0.5))
                            },
                            b = function(x) {
                                return(log(exp(x) + 0.5))
                            },
                            in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                                return(sapply(x, function(x_i) {
                                            return(
                                                isTRUE(all.equal(
                                                        x_i,
                                                        log_round_to_integer_plus_0.5_exp(x_i),
                                                        tolerance = tolerance
                                                    ))
                                            )
                                        }))
                            },
                            discretizer = log_round_to_integer_plus_0.5_exp),
                        X2 = list(a = function(x) {
                                return(log(exp(x) - 0.5))
                            },
                            b = function(x) {
                                return(log(exp(x) + 0.5))
                            },
                            in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                                return(sapply(x, function(x_i) {
                                            return(
                                                isTRUE(all.equal(
                                                        x_i,
                                                        log_round_to_integer_plus_0.5_exp(x_i),
                                                        tolerance = tolerance
                                                    ))
                                            )
                                        }))
                            },
                            discretizer = log_round_to_integer_plus_0.5_exp)
                        ),
#                    lower = c(X1 = log(0.5), X2 = log(0.5)),
                    lower = c(X1 = -Inf, X2 = -Inf),
                    upper = c(X1 = Inf, X2 = Inf),
                    x_names = c("X1", "X2"),
                    log = FALSE)
#            })
    )

int_area_xlim <- 1:2
int_area_ylim <- 5:6
integration_area_polygon_df <-
    data.frame(id = 1,
        value = 1,
        x = rep(int_area_xlim, each = 2),
        y = c(int_area_ylim, rev(int_area_ylim)))

p_a_regular_scale <- ggplot() +
    geom_polygon(aes(x = x, y = y, group = id),
        fill = "grey",
        data = integration_area_polygon_df) +
    geom_hline(yintercept = int_area_ylim, linetype = 2) +
    geom_vline(xintercept = int_area_xlim, linetype = 2) +
    geom_contour(aes(x = X1, y = X2, z = z, colour = z), bins = 7, data = continuous_density_df_a) +
    geom_point(aes(x = X1, y = X2, colour = z), data = discrete_density_df_a) +
    scale_colour_gradientn("Discrete\nKernel\nValue",
        colours = rev(c("#333333", "#777777", "#BBBBBB", "#FFFFFF")),
#        colours = rev(c("#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
#        values = c(0.0001, 0.001, 0.01, 0.1, 1),
#        values = c(-4, -3, -2, -1),
        limits = c(0.00001, 0.2),
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
#        breaks = c(0.001, 0.01, 0.1, 1),
#        labels = c(expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    xlab(expression(X[1])) +
    ylab(expression(X[2])) +
    ggtitle("(a) Diagonal bandwidth\nLinear axes") +
    theme_bw()

print(p_a_regular_scale)
legend_grob <- grid.get("guide-box.3-5-3-5")

## update p_a_regular_scale to not print legend
p_a_regular_scale <- p_a_regular_scale +
    theme(legend.position = "none")
    

p_a_log_scale <- p_a_regular_scale +
    scale_x_log10() +
    scale_y_log10() +
    ggtitle("(b) Diagonal bandwidth\nLogarithmic axes")

var_b <- 0.2
covar_b <- 0.15


continuous_density_df_b <- x_cont_grid %>%
    as.data.frame() %>%
    `$<-`("z",
        log_pdtmvn_mode_centered_kernel(x = x_cont_grid,
            center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
            bw = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
            bw_continuous = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
            continuous_vars = c("X1", "X2"),
            discrete_vars = character(0),
            continuous_var_col_inds = 1:2,
            discrete_var_col_inds = integer(0),
            discrete_var_range_fns = NULL,
            lower = c(X1 = -Inf, X2 = -Inf),
#            lower = c(X1 = log(0.5), X2 = log(0.5)),
            upper = c(X1 = Inf, X2 = Inf),
            x_names = c("X1", "X2"),
            log = FALSE)
    )

#x_disc_grid <- data.frame(X1 = c(1, 3), X2 = c(1, 3))
#debug(pdtmvn::dpdtmvn)
discrete_density_df_b <- x_disc_grid %>%
    as.data.frame() %>%
    `$<-`("z",
#        sapply(seq_len(nrow(x_disc_grid)), function(x_grid_row_ind) {
                log_pdtmvn_mode_centered_kernel(x = x_disc_grid,
                    center = as.matrix(data.frame(X1 = 2.5, X2 = 2.5)),
                    bw = matrix(c(var_b, covar_b, covar_b, var_b), nrow = 2, ncol = 2),
                    bw_continuous = matrix(0, nrow = 0, ncol = 0),
                    continuous_vars = character(0),
                    discrete_vars = c("X1", "X2"),
                    continuous_var_col_inds = integer(0),
                    discrete_var_col_inds = 1:2,
                    discrete_var_range_fns = list(
                        X1 = list(a = function(x) {
                                return(log(exp(x) - 0.5))
                            },
                            b = function(x) {
                                return(log(exp(x) + 0.5))
                            },
                            in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                                return(sapply(x, function(x_i) {
                                            return(
                                                isTRUE(all.equal(
                                                        x_i,
                                                        log_round_to_integer_plus_0.5_exp(x_i),
                                                        tolerance = tolerance
                                                    ))
                                            )
                                        }))
                            },
                            discretizer = log_round_to_integer_plus_0.5_exp),
                        X2 = list(a = function(x) {
                                return(log(exp(x) - 0.5))
                            },
                            b = function(x) {
                                return(log(exp(x) + 0.5))
                            },
                            in_range = function(x, tolerance = .Machine$double.eps^0.5) {
                                return(sapply(x, function(x_i) {
                                            return(
                                                isTRUE(all.equal(
                                                        x_i,
                                                        log_round_to_integer_plus_0.5_exp(x_i),
                                                        tolerance = tolerance
                                                    ))
                                            )
                                        }))
                            },
                            discretizer = log_round_to_integer_plus_0.5_exp)
                        ),
#                    lower = c(X1 = log(0.5), X2 = log(0.5)),
                    lower = c(X1 = -Inf, X2 = -Inf),
                    upper = c(X1 = Inf, X2 = Inf),
                    x_names = c("X1", "X2"),
                    log = FALSE)
#            })
    )

p_b_regular_scale <- ggplot() +
    geom_contour(aes(x = X1, y = X2, z = z), bins = 7, data = continuous_density_df_b) +
    geom_point(aes(x = X1, y = X2, colour = z), data = discrete_density_df_b) +
#    scale_colour_gradientn("Predictive\nDistribution\nProbability",
#        colours = rev(c("#333333", "#777777", "#BBBBBB", "#FFFFFF")),
##        colours = rev(c("#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
##        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
##        limits = c(10^{-10}, 1),
#        trans = "log",
##        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
##        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
##        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
#        values = c(0.0001, 0.001, 0.01, 0.1, 1),
##        values = c(-4, -3, -2, -1),
#        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
#        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
##        breaks = c(0.001, 0.01, 0.1, 1),
##        labels = c(expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
#        na.value = "white"
#    ) +
    scale_colour_gradientn("Predictive\nDistribution\nProbability",
        colours = rev(c("#333333", "#777777", "#BBBBBB", "#FFFFFF")),
#        colours = rev(c("#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
#        values = c(0.0001, 0.001, 0.01, 0.1, 1),
#        values = c(-4, -3, -2, -1),
        limits = c(0.00001, 0.2),
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
#        breaks = c(0.001, 0.01, 0.1, 1),
#        labels = c(expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    #    geom_point(aes(x = exp(log(3) - var_b - covar_b), y = exp(log(3) - var_b - covar_b)), colour = "red") +
    xlab(expression(X[1])) +
    ylab(expression(X[2])) +
    ggtitle("(c) Non-diagonal bandwidth\nLinear axes") +
    theme_bw() +
    theme(legend.position = "none")

p_b_log_scale <- p_b_regular_scale +
    scale_x_log10() +
    scale_y_log10() +
    ggtitle("(c) Non-diagonal bandwidth\nLogarithmic axes")

grid.newpage()
#grid.layout(nrow = 2, ncol = 2, heights = unit(rep(1, 2), c("null", "lines")))
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 3,
            heights = unit(rep(1, 2), c("null", "null")),
            widths = unit(c(1, 1, 0.3), c("null", "null", "null")))))
pushViewport(viewport(layout.pos.row = 1:2, layout.pos.col = 3))
grid.draw(legend_grob)
upViewport()
print(p_a_regular_scale, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(p_a_log_scale, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
print(p_b_regular_scale, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
print(p_b_log_scale, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))
@
\end{figure}

%Here, $L(\bw, \bw^{*} ; \bH)$ is a continuous multivariate kernel function
%defined on $\prod_{j = 1}^{J} \mathcal{E}^j$.  For each discrete variable indexed by $j
%= 1, \ldots, J^d$, we associate lower and upper bounds of integration $a_{z_j}$
%and $b_{z_j}$ with each value $z_j \in \mathcal{D}^j$.  In order to ensure that
%our final density estimate integrates to $1$, we require that these integration
%bounds form a disjoint cover of $\mathcal{E}^j$ in the sense that $\cup_{z_j \in
%\mathcal{D}^j} [a_{z_j}, b_{z_j}) = \mathcal{E}^j$ and $\cap_{z_j \in
%\mathcal{D}^j} [a_{z_j}, b_{z_j}) = \emptyset$, the empty set.  At a vector of
%values $\bz \in \prod_{j = 1}^J \mathcal{D}^j$, we define the partially
%discretized kernel as follows:
%\begin{equation*}
%K(\bz, \bz^{*} ; \bH) = \int_{a_{z_1}}^{b_{z_1}} \cdots
%\int_{a_{z_{J^d}}}^{b_{z_{J^d}}} L(\bz, \bz^{*} ; \bH) \, d z_{1} \cdots d
%z_{J^d}
%\end{equation*}

%To make this concrete, consider a $J$-dimensional random
%vector $\bW = (\bW^{d'}, \bW^{c'})'$ that is partitioned into a
%$J^d$-dimensional subvector $\bZ^d$ of discrete random variables and a
%$J^c$-dimensional subvector $\bZ^c$ of continuous random variables.  Without
%loss of generality, we assume that the discrete variables are the first $J^d$
%variables.  For each component random variable $Z_j$, $j = 1, \ldots, J$, we
%denote the set of values that $Z_j$ may take by $\mathcal{D}^j \subseteq
%\mathbb{R}$.  In the continuous cases, we take $\mathcal{D}^j = \mathbb{R}$.  In
%the discrete cases, $\mathcal{D}^j$ is a discrete set such as the positive
%integers.  Our definitions could be modified to handle a component random
%variable whose distribution comprised a combination of discrete and continuous
%parts; however, this is not required for our applications so we have not pursued
%that line here.




%\begin{figure}[height=2in]

We estimate the bandwidth parameters by numerically
maximizing the cross-validated log score of the predictive distributions for the
observations in the training data:
$\widehat{\btheta}^h \approx \argmax{\btheta^h} \sum_{t^* = 1}^{T_{\text{train}}} \widehat{f}^h_{-t^*}(y^h_{t^*} | \bx_{t^*} ; \btheta^h)$.
Here $\widehat{f}^h_{-t^*}(y^h_{t^*} | \bx_{t^*})$ is as in
Equation~\eqref{eqn:KCDEDefinition}.  In order to obtain the term corresponding
to time $t^*$, we leave the year of training data before and after the time
$t^*$ out of the set $\btau$.  Hart and
Vieu\cite{hart1990bandwidthEstDependentData} show that when kernel density
estimation is used to estimate a marginal density with dependent observations,
leaving out a window of times around the target time point in cross
validation can yield small improvements in the integrated squared error of the
density estimate under certain assumptions about the form of the dependence.  We
expect that a similar result holds for the case of conditional density
estimation.  We perform the optimization using the limited memory
box constrained optimization method of Byrd
\etal\cite{byrd1995limitedmemoryoptim}, implemented by the {\tt optim} function
in {\tt R}\cite{RCoreLanguage}.%  Other optimization strategies may yield
% improved computation times, but we have not explored these ideas; for one example along these lines, see .

Our primary motivation for using the log score as the optimization target during
estimation is that this is the criteria that has been used to evaluate and
compare prediction methods in two recent government-sponsored infectious
disease prediction contests
\cite{PandemicPredictionandForecastingScienceandTechnologyInteragencyWorkingGroup2015Announcement,
EpidemicPredictionInitiative2015Index}.
We will apply our method to the data sets
from those competitions in the applications Section below, and will report log
scores in order to facilitate comparisons with other results from those
competitions that may be published in the future.  Our intuition is that
it is beneficial to align the criteria used in estimation with the criteria used for comparing methods.
In general, the log score is a strictly proper scoring rule; i.e., its
expectation is uniquely maximized by the true predictive
distribution\cite{gneiting2007strictlyProperScoringRules}.
However, its use as an optimization criterion can be criticised as it may be
sensitive to outliers\cite{gneiting2007strictlyProperScoringRules}.

\subsection{Combining Marginal Predictive Distributions with Copulas}
\label{subsec:Methods:Copulas}

We use copulas to tie the marginal predictive distributions for individual
prediction horizons obtained from KCDE together into a joint predictive
distribution for the trajectory of incidence over multiple time points. 
In this Section, we will provide a brief overview of copulas and our approach
to using them in this application.
A complete review of copulas is beyond the scope of this article; see (cite
cite) for more thorough introductions.
In order to describe our methods for both continuous and discrete distributions, it is most convenient to
frame the discussion in this Section in terms of {c.d.f.}s instead of density
functions.
We will use a capital $C$ to denote the copula function for distributions and a
lower case $c$ to denote the copula function for densities.  Similarly, the predictive
densities $f^{h}(z_{t^* + h} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M};
\btheta^h)$ we obtained in the previous section naturally yield
corresponding predictive {c.d.f.}s
$F^{h}(z_{t^* + h} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^h)$.

Our model specifies the joint {c.d.f.} for $\bZ_{(t^* + 1):(t^* + H_{t^*})}$ as
follows:
\begin{align}
&F^{H_{t^*}}(\bz_{(t^* + 1):(t^* + H_{t^*})} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^1, \ldots, \btheta^{H_{t^*}}, \bxi^{H_{t^*}}) = \nonumber \\
&\qquad C\{F^{1}(z_{t^* + 1} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^1), \ldots, F^{h}(z_{t^* + H_{t^*}} | t^*, z_{t^* - l_1}, \ldots, z_{t^* - l_M}; \btheta^{H_{t^*}}); \bxi^{H_{t^*}}\}
\end{align}

The copula function $C$ maps the marginal {c.d.f.} values to the joint
{c.d.f.} value.  We use the isotropic Gaussian
copula implemented in the {\tt R} package {\tt copula}
\cite{HofertRCopulaPackage}.  The copula function is given by
\begin{equation}
C(u_1, \ldots, u_J ; \bxi^H) = \Phi_{\Sigma^H}(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_J)),
\end{equation}
where $\Phi^{-1}$ is the inverse {c.d.f.} of a standard univariate Gaussian
distribution and $\Phi_{\Sigma^H}$ is the {c.d.f.} of a multivariate Gaussian
distribution with mean $\b0$ and covariance matrix $\Sigma^H$.  The isotropic
specification sets $\Sigma^H = [{\sigma^H_{i,j}}]$, where 
\begin{equation}
{\sigma^H_{i,j}} = \begin{cases} 1 \text{ if $i = j$,} \\ \xi^H_d \text{ if $\vert i - j \vert = d$} \end{cases}
\end{equation}
Intuitively, $\xi^H_d$ captures the amount of dependence between incidence
levels at future times that are $d$ weeks apart.

We obtain a separate copula fit for each value of $H$ from 2 to $W$ (note that a
copula is not required for ``trajectories'' of length $H = 1$).  In order to do this, we follow the
two-stage estimation strategy outlined by
Joe\cite{joe2005asymptoticEfficiencyTwoStageCopula}.  Briefly, this procedure
follows three main steps:
\begin{enumerate}
  \item Estimate the parameters for marginal predictive distributions
  using the procedures described in the previous Subsection.
  \item Form vectors of ``pseudo-observations'' by passing observed incidence
  trajectories from previous seasons through the marginal predictive {c.d.f.}s
  obtained in step 1:
  \begin{align}
  &(u_{k,1}, \ldots, u_{k, H}) = \nonumber \\
  &\qquad \{F^{1}(z_{t_k^* + 1} | t_k^*, z_{t_k^* - l_1}, \ldots, z_{t_k^* - l_M}; \btheta^1), \ldots, F^{H}(z_{t_k^* + 1} | t_k^*, z_{t_k^* - l_1}, \ldots, z_{t_k^* - l_M}; \btheta^H)\} \nonumber
  \end{align}
  We form one such vector of pseudo-observations for each season in the training
  data; in the notation here, these seasons are indexed by $k$.  The relevant
  time points $t_k^*$ are the times in those previous seasons falling $H$ time
  points before the end of the season.
  \item Estimate the copula parameters $\bxi^H$ by maximizing the likelihood of the
  pseudo-observations.
\end{enumerate}

\section{Simulation Study}
\label{sec:SimStudy}

In this Section, we conduct a simulation study designed to examine the
utility of using a non-diagonal bandwidth matrix specification when
estimating conditional distributions with KCDE.  There are many factors that
determine the relative performance of KCDE estimators with different bandwidth
parameterizations.  In this simulation study, we vary just two of these factors:
the number of conditioning variables (either $1$ or $3$) and the sample size
($N = 100$ or $N = 1000$).  We hold other factors that may be related to the
relative performance of different bandwidth specifications fixed.

The distributions that we simulate from are discretized multivariate normal
distributions of dimension either $D = 2$ or $D = 4$.  To define this distribution,
let $\bU \sim MVN(\b0, \Sigma)$ where $\Sigma$ is a $D \times D$ matrix $1$ on
the diagonal and $0.9$ off-diagonal.  This is the multivariate normal
distribution that was used in one of the simulation studies conducted by Duong and
Hazelton\cite{duong2005crossvalidationBandwidthMultivariateKDE} in studying the
impact of the bandwidth specification for multivariate (unconditional) density
estimation with continuous distributions.  We treat $\bU$ as a latent variable
and discretize it to obtain the random variable $\bX$ using the same approach
described in the methods section above for discretizing the kernel function.

We conduct 500 simulation trials for each combination of the sample size $N$ and
dimension $D$.  In each simulation trial, we simulate $N$ observations of the
discretized multivariate normal random variable $\bX$.  Using these
observations as a training data set, we estimate the bandwidth
parameters for a KCDE model for the conditional distribution of $X_1 | X_2,
\ldots, X_D$.

We then evaluate the KCDE density estimate by an importance
sampling approximation of the Kullback-Leibler divergence of the conditional
density estimate from the true conditional density, integrated over the range of
the conditioning variables.  Specifically if we denote the Kullback-Leibler
divergence of the estimated density $\widehat{f}$ from the true density $f$ by
$\text{Div}_{\text{K-L}} (f, \widehat{f})$, the score is given by:

\begin{align}
&\text{Score}\{\widehat{f}(x_1 | x_2, \ldots, x_D)\} \nonumber \\
&\qquad = \int \cdots \int \left[\text{Div}_{\text{K-L}} \{ f(x_1 | x_2, \ldots, x_D), \widehat{f}(x_1 | x_2, \ldots, x_D) \}\right] f(x_2, \ldots, x_D) d x_2 \cdots d x_D \nonumber \\
&\qquad = \int \cdots \int \log \left\{ \frac{f(x_1 | x_2, \ldots, x_D)}{\widehat{f}(x_1 | x_2, \ldots, x_D)} \right\} f(x_1, \ldots, x_D) d x_1 \cdots d x_D \nonumber \\
&\qquad \approx \sum_{i = 1}^{N_{\text{eval}}} \log \left\{ \frac{f(x_{i,1} | x_{i,2}, \ldots, x_{i,D})}{\widehat{f}(x_{i,1} | x_{i,2}, \ldots, x_{i,D})} \right\} \label{eqn:SimStudyEvalKLApprox}
\end{align}

In Equation~\eqref{eqn:SimStudyEvalKLApprox}, the $N_{\text{eval}}$ observations
$(x_{i,1}, \ldots, x_{i,D})$ are sampled from the joint distribution of $\bX$.

\lboxit{To do: report simulation study results (not done running yet)}

<<LoadFluData, echo = FALSE>>=
junk <- capture.output({
    usflu <- suppressMessages(get_flu_data("national", "ilinet", years=1997:2015))
})
ili_national <- suppressWarnings(transmute(usflu,
    region.type = REGION.TYPE,
    region = REGION,
    year = YEAR,
    week = WEEK,
    weighted_ili = as.numeric(X..WEIGHTED.ILI)))
ili_national$time <- ymd(paste(ili_national$year, "01", "01", sep = "-"))
week(ili_national$time) <- ili_national$week
ili_national$time_index <- seq_len(nrow(ili_national))

## Season column: for example, weeks of 2010 up through and including week 30 get season 2009/2010;
## weeks after week 30 get season 2010/2011
ili_national$season <- ifelse(
    ili_national$week <= 30,
    paste0(ili_national$year - 1, "/", ili_national$year),
    paste0(ili_national$year, "/", ili_national$year + 1)
)

## Season week column: week number within season
ili_national$season_week <- sapply(seq_len(nrow(ili_national)), function(row_ind) {
        sum(ili_national$season == ili_national$season[row_ind] &
                ili_national$time_index <= ili_national$time_index[row_ind])
    })


## Subset to data actually used in this analysis -- up through end of 2014.
ili_national <- ili_national[ili_national$year <= 2014, , drop = FALSE]

## cutoff time for training data
ili_train_cutoff_time <- ili_national$time[max(which(ili_national$year == 2010))]
@

<<LoadDengueData, echo = FALSE>>=
dengue_sj <- read.csv("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/data-raw/San_Juan_Testing_Data.csv")

## convert dates
dengue_sj$time <- ymd(dengue_sj$week_start_date)

## cutoff time for training data
dengue_train_cutoff_time <- dengue_sj$time[max(which(dengue_sj$season == "2008/2009"))]
@

<<SimStudyMotivationPlot, echo = FALSE>>=
n <- nrow(dengue_sj)
#plot(log(dengue_sj$total_cases[seq_len(n - 1)] + 100) ~ log(dengue_sj$total_cases[seq(from = 2, to = n)] + 100))
#plot(log(dengue_sj$total_cases[seq_len(n - 1)] + 0.5) ~ log(dengue_sj$total_cases[seq(from = 2, to = n)] + 0.5))
plot(dengue_sj$total_cases[seq_len(n - 1)] ~ dengue_sj$total_cases[seq(from = 2, to = n)])
@



%<<SimStudyDistributionsDiscretizedDuongHazelton>>=
%library(ggplot2)
%library(grid)
%library(plyr)
%library(dplyr)
%library(tidyr)
%library(pdtmvn)
%library(kcde)
%source("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/code/sim-densities-sim-study-discretized-Duong-Hazelton.R")
%
%## Density family bivariate-A
%n_sim <- 10000
%discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-A-discretized") %>%
%    as.data.frame()
%continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-A") %>%
%    as.data.frame()
%discrete_sample_counts <- discrete_sample %>%
%    count(X1, X2)
%
%pa <- ggplot() +
%    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
%    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
%pa
%
%## Density family bivariate-B
%n_sim <- 10000
%discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-B-discretized") %>%
%    as.data.frame()
%continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-B") %>%
%    as.data.frame()
%discrete_sample_counts <- discrete_sample %>%
%    count(X1, X2)
%
%pb <- ggplot() +
%    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
%    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
%pb
%
%## Density family bivariate-C
%n_sim <- 10000
%discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-C-discretized") %>%
%    as.data.frame()
%continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-C") %>%
%    as.data.frame()
%discrete_sample_counts <- discrete_sample %>%
%    count(X1, X2)
%
%pc <- ggplot() +
%    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
%    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
%pc
%
%## Density family bivariate-D
%n_sim <- 10000
%discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-D-discretized") %>%
%    as.data.frame()
%continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "bivariate-D") %>%
%    as.data.frame()
%discrete_sample_counts <- discrete_sample %>%
%    count(X1, X2)
%
%pd <- ggplot() +
%    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
%    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
%pd
%
%## Density family multivariate-2d
%n_sim <- 10000
%discrete_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "multivariate-2d-discretized") %>%
%    as.data.frame()
%continuous_sample <- sim_from_pdtmvn_mixt(n = n_sim, sim_family = "multivariate-2d") %>%
%    as.data.frame()
%discrete_sample_counts <- discrete_sample %>%
%    count(X1, X2)
%
%pd <- ggplot() +
%    geom_density_2d(aes(x = X1, y = X2), data = continuous_sample) +
%    geom_point(aes(x = X1, y = X2, colour = n), data = discrete_sample_counts)
%pd
%
%@

\section{Applications}
\label{sec:Applications}

In this Section, we illustrate our methods through applications to prediction
of infectious disease incidence in two examples with real disease incidence data
sets: one with a weekly measure of incidence of Dengue fever in San
Juan, Puerto Rico, and a second with a weekly measure of incidence of influenza
like illness in the United States.  These data sets were used in two recent
prediction competitions sponsored by the United States federal
government\cite{PandemicPredictionandForecastingScienceandTechnologyInteragencyWorkingGroup2015Announcement,
EpidemicPredictionInitiative2015Index}.  In the Dengue data set, the incidence
measure is an integer number of reported cases in the given week.  In the
Influenza data set the incidence measure is continuous, a weighted
proportion of doctor visits with influenza-like illness.

Figure~\ref{fig:IntialDataPlots} displays each time series.  As indicated in the
figure, we have divided each data set into two subsets.  The first period is used as a training set in
estimating the model parameters.  The last four years of each data set are
reserved as a test set for evaluating model performance.  All predictions are
made as though in real time, using data only up through a given week in order to
make predictions for incidence after that week.

\begin{figure}
\caption{Plots of the data sets we apply our methods to.  In each case, the last
four years of data are held out as a test data set; this cutoff is indicated
with a vertical dashed line.  For the flu data set, low-season incidence was not
recorded in early years of data collection; these missing data are indicated
with vertical grey bars.}
\label{fig:IntialDataPlots}
<<InitialDataPlot, echo = FALSE, fig.height = 5>>=
#time_limits <- c(min(c(ili_national$time, dengue_sj$time)),
#    max(c(ili_national$time, dengue_sj$time)))

dengue_plot <- ggplot() +
    geom_line(aes(x = as.Date(time), y = total_cases),
        data = dengue_sj) +
    geom_vline(aes(xintercept = as.numeric(as.Date(dengue_train_cutoff_time))),
        colour = "red", linetype = 2) +
    scale_x_date() +
#    scale_x_date(limits = time_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 500), expand = c(0, 0)) +
    xlab("Time") +
    ylab("Total Cases") +
    ggtitle("(a) Dengue Fever Data - San Juan, Puerto Rico") +
    theme_bw(base_size = 11)

ili_plot <- ggplot() +
    geom_line(aes(x = as.Date(time), y = weighted_ili),
        data = ili_national) +
    geom_vline(aes(xintercept = as.numeric(as.Date(time))),
        colour = "grey",
        data = ili_national[is.na(ili_national$weighted_ili), ]) +
    geom_vline(aes(xintercept = as.numeric(as.Date(ili_train_cutoff_time))),
        colour = "red", linetype = 2) +
    scale_x_date() +
#    scale_x_date(limits = time_limits, expand = c(0, 0)) +
    xlab("Time") +
    ylab("Weighted Influenza-like Illness\n") +
    ggtitle("(b) Influenza Data - National United States") +
    theme_bw(base_size = 11)

grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 2, ncol = 1)))
print(dengue_plot, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(ili_plot, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
@
\end{figure}

As we discussed in the methods section, there are three prediction targets for
each data set, based closely on the prediction targets that were used in the
original competitions.  First, for each week in the test data, we obtain a
predictive distribution for the incidence measure in that week at each prediction horizon
from 1 to 52 weeks ahead.  Second, in each week of the test data set, we make
predictions for the timing of the peak week of the corresponding season. 
Third, in each week of the test data set we predict incidence in the peak
week for the corresponding season.  Following the precedent set in the
competitions, we make predictions for \textit{binned} incidence in the peak
week.  For the Dengue data set, the bins are $[0, 50), [50, 100), \ldots, [500, \infty)$.
For the Influenza data set, the bins are $[0, 0.5), [0.5, 1), \ldots, [13, \infty)$.
Our predictions for incidence in individual weeks are for the raw, unbinned,
incidence measure.

\lboxit{To do: Figure illustrating prediction targets?}

We use a seasonal ARIMA model as a baseline to compare our approach to.  In
fitting this model, we first transformed the observed incidence measure to the
log scale (after adding $1$ in the Dengue data set, which included some
observations of $0$ cases); this transformation makes the normality assumptions
of the ARIMA model more plausible.  We then performed first-order seasonal
differencing, and obtained the final model fits
using the {\tt auto.arima} function in {\tt R}'s {\tt forecast}
package\cite{hyndmanRForecastPackage}; this function uses a stepwise
procedure to determine the terms to include in the model.
This procedure resulted in a 
<<ILISarimaModelFitSummary, echo = FALSE, results = "asis">>=
ili_sarima_fit <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/estimation-results/sarima-fit.rds")
temp <- capture.output(summary(ili_sarima_fit))
cat(paste0("SARIMA(", substr(temp[2], 7, 15), as.integer(substr(temp[2], 16,16)) + 1, substr(temp[2], 17, 18), ")$_{52}$"))
@
model for the influenza data and a 
<<DengueSarimaModelFitSummary, echo = FALSE, results = "asis">>=
dengue_sarima_fit <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/dengue_sj/estimation-results/sarima-fit.rds")
temp <- capture.output(summary(dengue_sarima_fit))
cat(paste0("SARIMA(", substr(temp[2], 7, 15), as.integer(substr(temp[2], 16, 16)) + 1, substr(temp[2], 17, 18), ")$_{52}$"))
@
model for the Dengue data.  We note that a different SARIMA model was
used as a baseline in the Dengue competition, but the SARIMA model we obtained
using this procedure performed slightly better on the test set than that
previous baseline model.  In the application to Dengue, we discretized the
predictive distribution from SARIMA using a similar procedure to that we
described for discretizing the kernel function in KCDE in the Methods section.

Our applications include four variations on KCDE model specifications.  The
``Null KCDE Model'' omits the periodic component of the kernel function and uses
a diagonal bandwidth matrix specification.  The other three variations are
obtained by adding the periodic kernel component to this null KCDE model, using
a fully parameterized bandwidth matrix, or both.

We compare the models using the
log score of the predictive distributions: for a random variable $X$ with observed value $x$ the
log score of the predictive distribution $f_X$ is $log\{f_X(x)\}$.  A larger log
score indicates better model performance.  Our discussion of the
results is divided into two Subsections: one focusing on predictions for
incidence in individual weeks and the second focusing on predictions for the timing of and incidence in the peak week.

\subsection{Predictive Distributions for Individual Weeks}

Figure~\ref{fig:DengueRibbonsPredictions} displays the median and 50\% interval
limits for the predictive distributions obtained at prediction horizons from $1$
to $26$ weeks ahead from SARIMA and from the KCDE specification with a fully
parameterized bandwidth matrix and a periodic kernel component.  For predictions
of Dengue fever incidence, the most salient difference in the predictions from
KCDE and SARIMA is the large difference in the widths of the predictive
intervals.  It appears that the predictive distributions from SARIMA are
over-confident: the observed case counts are often well outside of the interval
bounds from SARIMA.  This is particularly the case for the 2012/2013 season, where the
SARIMA predictions are well below the realized values throughout most of the
season, with tight interval bounds.  On the other hand, the intervals from KCDE
may be too wide.
%We believe that this may reflect decisions we made for the
%parameter starting values and estimation procedure for KCDE.
However, we prefer too-wide intervals reflecting an ``honest'' statement of uncertainty to
too-narrow intervals indicating false confidence in the predictions.
Both methods appear to miss the timing of the 2010/2011 season at moderately large prediction horizons.

\lboxit{Update qualitative story based on interval predictions plot if it
changes when I update the fits used in making the plot based on different
starting values: intervals may be narrower from KCDE?}

For the predictions of Influenza incidence, there is much less of a noticeable
distinction between the predictions given by the two methods.  Throughout, the
point predictions and intervals are similar.

\lboxit{I have plotted 50\% intervals here because for Dengue the 95\%
intervals are too wide for the plot to be readable, and I wanted to same
interval width in both plots.  I expect that the Dengue intervals may be
smaller with revised starting parameter values.  If that is the case, I will
change plots to 95\% intervals, and there is a visible interesting story for
interval predictions for Influenza: the predictions are basically the same at
all times for prediction horizon 26, seasonality is the only thing that
mattered.  This is still the case with 50\% intervals but hard to see.}


%In our first example, we apply the method for prediction of
%influenza with prediction horizons of 1 through 4 weeks.  Data on influenza
%incidence are available through {\tt R}'s {\tt cdcfluview} package.  Here we
%create a data set with a nationally aggregated measure of flu incidence
%
%
%There are several methods that we could employ to handle these missing data:
%\begin{enumerate}
%\item Impute the missing values.  They are all in the low season, so this should be relatively easy to do.
%\item Drop all data up through the last NA.
%\item Use the data that are available.
%\end{enumerate}
%Of these approaches, the first is probably preferred.  The concern with the second
%is that we are not making use of all of the available data.  The potential concern with the
%third is that in the data used in estimation, there will be more examples of prediction of values in the high season
%using values in the high season and middle of the season than of prediction of values in the high season using values in the low season.
%This could potentially affect our inference.  However, we do not expect this effect to be large,
%so we proceed with this option for the purposes of this example.

%We also plot histograms of the observed total cases on the original scale and on the log scale.
%
%<<FluDataHistogramPlotTotalCases, echo = FALSE>>=
%hist_df <- rbind(
%	data.frame(value = ili_national$weighted_ili,
%    	variable = "Weighted ILI"),
%    data.frame(value = log(ili_national$weighted_ili),
%    	variable = "log(Weighted ILI)")
%)
%
%ggplot(aes(x = value), data = hist_df) +
%    geom_histogram() +
%    facet_wrap( ~ variable, ncol = 2) +
%    xlab("Weighted ILI") +
%    theme_bw()
%@
%
%These plots demonstrate that total cases follows an approximately log-normal
%distribution.  In the application below, we will consider modeling these data on
%both the original scale and the log scale.  Intuitively, since we are using a
%kernel that is obtained from a Gaussian, modeling the data on the log scale
%should yield better performance.  On the other hand, the performance gain may be
%negligible if we have enough data.

%Finally, we plot the autocorrelation function:
%
%<<FluDataACFPlotTotalCases, echo = FALSE>>=
%last_na_ind <- max(which(is.na(ili_national$weighted_ili)))
%non_na_inds <- seq(from = last_na_ind + 1, to=nrow(ili_national))
%acf(ili_national$weighted_ili[non_na_inds],
%  lag.max = 52 * 4)
%@
%
%This plot illustrates the annual periodicity that was also visible in the
%initial data plot above.  There is no apparent evidence of longer term annual
%cycles.  We therefore include a periodic kernel acting on the time index with a
%period of 52.2 weeks (the length of the period is motivated by the fact that
%in our data, there is a year with 53 weeks once every 5 or 6 years).


%<<FluDataKernelComponentsSetup, echo = TRUE>>=
%## Definitions of kernel components.  A couple of notes:
%##   1) In the current implementation, it is required that separate kernel
%##      components be used for lagged (predictive) variables and for leading
%##      (prediction target) variables.
%##   2) The current syntax is verbose; in a future version of the package,
%##      convenience functions may be provided.
%
%## Define kernel components -- 3 pieces:
%##   1) Periodic kernel acting on time index
%##   2) pdtmvn kernel acting on lagged total cases (predictive) -- all continuous
%##   3) pdtmvn kernel acting on lead total cases (prediction target) -- all continuous
%kernel_components <- list(
%    list(
%        vars_and_offsets = data.frame(var_name = "time_index",
%            offset_value = 0L,
%            offset_type = "lag",
%            combined_name = "time_index_lag0",
%            stringsAsFactors = FALSE),
%        kernel_fn = periodic_kernel,
%        theta_fixed = list(period=pi / 52.2),
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_periodic_kernel,
%        initialize_kernel_params_args = NULL,
%        vectorize_kernel_params_fn = vectorize_params_periodic_kernel,
%        vectorize_kernel_params_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_periodic_kernel,
%        update_theta_from_vectorized_theta_est_args = NULL
%    ),
%    list(
%        vars_and_offsets = data.frame(var_name = "weighted_ili",
%            offset_value = 1L,
%            offset_type = "horizon",
%            combined_name = "time_index_horizon1",
%            stringsAsFactors = FALSE),
%        kernel_fn = pdtmvn_kernel,
%        rkernel_fn = rpdtmvn_kernel,
%        theta_fixed = list(
%            parameterization = "bw-diagonalized-est-eigenvalues",
%            continuous_vars = "weighted_ili_horizon1",
%            discrete_vars = NULL,
%            discrete_var_range_fns = NULL,
%            lower = -Inf,
%            upper = Inf
%        ),
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
%        initialize_kernel_params_args = NULL,
%        vectorize_kernel_params_fn = vectorize_params_pdtmvn_kernel,
%        vectorize_kernel_params_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
%        update_theta_from_vectorized_theta_est_args = NULL
%    ))#,
%    list(
%        vars_and_lags = vars_and_lags[3:5, ],
%        kernel_fn = pdtmvn_kernel,
%        rkernel_fn = rpdtmvn_kernel,
%        theta_fixed = NULL,
%        theta_est = list("bw"),
%        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
%        initialize_kernel_params_args = list(
%            continuous_vars = vars_and_lags$combined_name[3:4],
%            discrete_vars = vars_and_lags$combined_name[5],
%            discrete_var_range_fns = list(
%                c_lag2 = list(a = pdtmvn::floor_x_minus_1, b = floor, in_range = pdtmvn::equals_integer, discretizer = round_up_.5))
%        ),
%        vectorize_theta_est_fn = vectorize_params_pdtmvn_kernel,
%        vectorize_theta_est_args = NULL,
%        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
%        update_theta_from_vectorized_theta_est_args = list(
%            parameterization = "bw-diagonalized-est-eigenvalues"
%        )
%    ))
%@

<<DengueDataMergePredictionResults, echo = FALSE>>=
dengue_prediction_results_sarima <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/dengue_sj/prediction-results/sarima-predictions.rds")
dengue_prediction_results_kcde <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/dengue_sj/prediction-results/kcde-predictions.rds")
dengue_prediction_results_kcde$model <- "KCDE"
dengue_prediction_results <- rbind.fill(dengue_prediction_results_sarima[!is.na(dengue_prediction_results_sarima$log_score), ],
    dengue_prediction_results_kcde)
dengue_prediction_results$AE <- unlist(dengue_prediction_results$AE)
     
dengue_prediction_results$full_model_descriptor <- paste0(dengue_prediction_results$model,
    "-seasonal_lag_", dengue_prediction_results$max_seasonal_lag,
#    "-filtering_", dengue_prediction_results$filtering,
    "-differencing_", dengue_prediction_results$differencing,
    "-periodic_", dengue_prediction_results$seasonality,
    "-bw_", dengue_prediction_results$bw_parameterization)

dengue_prediction_log_score_diffs_from_sarima_wide <- dengue_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

dengue_prediction_log_score_diffs_from_sarima_wide[, unique(dengue_prediction_results$full_model_descriptor)] <-
    dengue_prediction_log_score_diffs_from_sarima_wide[, unique(dengue_prediction_results$full_model_descriptor)] -
    dengue_prediction_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

dengue_prediction_log_score_diffs_from_sarima_long <- dengue_prediction_log_score_diffs_from_sarima_wide %>%
    gather_("model", "log_score_difference", unique(dengue_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$periodic & !dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic Kernel"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    !dengue_prediction_log_score_diffs_from_sarima_long$periodic & dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Full Bandwidth"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    dengue_prediction_log_score_diffs_from_sarima_long$periodic & dengue_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic Kernel,\nFull Bandwidth"
dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
    factor(dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor,
        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
@

<<FluDataMergePredictionResults, echo = FALSE>>=
ili_prediction_results_sarima <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/prediction-results/sarima-predictions.rds")
ili_prediction_results_kcde <- readRDS("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results/ili_national/prediction-results/kcde-predictions.rds")
ili_prediction_results_kcde$model <- "KCDE"
ili_prediction_results <- rbind.fill(ili_prediction_results_sarima[!is.na(ili_prediction_results_sarima$log_score), ],
    ili_prediction_results_kcde)
ili_prediction_results$AE <- unlist(ili_prediction_results$AE)

ili_prediction_results$full_model_descriptor <- paste0(ili_prediction_results$model,
    "-seasonal_lag_", ili_prediction_results$max_seasonal_lag,
#    "-filtering_", ili_prediction_results$filtering,
    "-differencing_", ili_prediction_results$differencing,
    "-periodic_", ili_prediction_results$seasonality,
    "-bw_", ili_prediction_results$bw_parameterization)

ili_prediction_log_score_diffs_from_sarima_wide <- ili_prediction_results %>%
    select(full_model_descriptor, prediction_time, prediction_horizon, log_score) %>%
    spread(full_model_descriptor, log_score)

ili_prediction_log_score_diffs_from_sarima_wide[, unique(ili_prediction_results$full_model_descriptor)] <-
    ili_prediction_log_score_diffs_from_sarima_wide[, unique(ili_prediction_results$full_model_descriptor)] -
    ili_prediction_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]

ili_prediction_log_score_diffs_from_sarima_long <- ili_prediction_log_score_diffs_from_sarima_wide %>%
    gather_("model", "log_score_difference", unique(ili_prediction_results$full_model_descriptor)) %>%
    mutate(
        seasonal_lag = grepl("seasonal_lag_1", model),
        differencing = grepl("differencing_TRUE", model),
        periodic = grepl("periodic_TRUE", model),
        bw_full = grepl("bw_full", model)
    )
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    ili_prediction_log_score_diffs_from_sarima_long$periodic & !ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic Kernel"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    !ili_prediction_log_score_diffs_from_sarima_long$periodic & ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Full Bandwidth"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[
    ili_prediction_log_score_diffs_from_sarima_long$periodic & ili_prediction_log_score_diffs_from_sarima_long$bw_full] <-
    "Periodic Kernel,\nFull Bandwidth"
ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
    factor(ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor,
        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
@



\begin{figure}
\caption{Plots of point and interval predictions from SARIMA and the KCDE
specification with a fully parameterized bandwidth and periodic kernel
component.}
\label{fig:DengueRibbonsPredictions}
<<DengueDataRibbonsPredictionPlot95Intervals, echo = FALSE, fig.height = 4.3>>=
ribbons_df <- dengue_prediction_results %>%
    select(prediction_time,
        prediction_horizon,
        full_model_descriptor,
        model,
        interval_pred_lb_95:interval_pred_ub_50) %>%
    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
    spread(bound_type, predictive_value)

phs_used <- c(1, 5, 13, 26)
models_used <- c("SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
#models_used <- c("SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
#    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal")

ggplot() +
    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model),
        alpha = 0.4,
        size = 0,
        data = ribbons_df[ribbons_df$prediction_horizon %in% phs_used &
                ribbons_df$full_model_descriptor %in% models_used &
                ribbons_df$interval_type == "50", ]) +
    geom_line(aes(x = time, y = total_cases), data = dengue_sj[dengue_sj$season %in% paste0(2009:2012, "/", 2010:2013), ]) +
#    geom_point(aes(x = time, y = total_cases), data = dengue_sj[dengue_sj$year %in% 2010:2014, ]) +
    geom_line(aes(x = prediction_time, y = pt_pred, colour = model, linetype = model),
        size = 1,
        data = dengue_prediction_results[dengue_prediction_results$prediction_horizon %in% phs_used &
                dengue_prediction_results$full_model_descriptor %in% models_used, ]) +
#    scale_alpha_discrete("Prediction\nInterval\nCoverage",
#        labels = c("50 Percent", "95 Percent"),
#        limits = c("50", "95"),
#        range = c(0.4, 0.2)) +
#    scale_fill_manual("Model", values = c("#0072B2", "#E69F00")) +
#    scale_colour_manual("Model", values = c("#0072B2", "#E69F00")) +
    scale_fill_manual("Model", values = c("#E69F00", "#0072B2")) +
    scale_colour_manual("Model", values = c("#E69F00", "#0072B2")) +
    scale_linetype("Model") +
    facet_wrap( ~ prediction_horizon, ncol = 1) +
    xlab("Prediction Time") +
    ylab("Total Cases") +
    ggtitle("(a) Point and 50% Interval Predictions, Dengue Fever") +
    theme_bw()

ribbons_df <- ili_prediction_results %>%
    select(prediction_time,
        prediction_horizon,
        full_model_descriptor,
        model,
        interval_pred_lb_95:interval_pred_ub_50) %>%
    gather("bound_type", "predictive_value", interval_pred_lb_95:interval_pred_ub_50) %>%
    mutate(interval_type = ifelse(grepl("50", bound_type), "50", "95"),
        bound_type = ifelse(grepl("lb", bound_type), "lower", "upper")) %>%
    spread(bound_type, predictive_value)

#phs_used <- c(1, 6, 13, 26)
models_used <- c("SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")

ggplot() +
    geom_ribbon(aes(x = prediction_time, ymin = lower, ymax = upper, colour = model, fill = model),
        alpha = 0.4,
        size = 0,
        data = ribbons_df[ribbons_df$prediction_horizon %in% phs_used &
                ribbons_df$full_model_descriptor %in% models_used &
                ribbons_df$interval_type == "50", ]) +
    geom_line(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2011:2014, ]) +
#    geom_point(aes(x = time, y = weighted_ili), data = ili_national[ili_national$year %in% 2010:2014, ]) +
    geom_line(aes(x = prediction_time, y = pt_pred, colour = model, linetype = model),
        size = 1,
        data = ili_prediction_results[ili_prediction_results$prediction_horizon %in% phs_used &
                ili_prediction_results$full_model_descriptor %in% models_used, ]) +
#    scale_alpha_discrete("Prediction\nInterval\nCoverage",
#        labels = c("50 Percent", "95 Percent"),
#        limits = c("50", "95"),
#        range = c(0.4, 0.2)) +
#    scale_fill_manual("Model", values = c("#0072B2", "#E69F00")) +
#    scale_colour_manual("Model", values = c("#0072B2", "#E69F00")) +
    scale_fill_manual("Model", values = c("#E69F00", "#0072B2")) +
    scale_colour_manual("Model", values = c("#E69F00", "#0072B2")) +
    scale_linetype("Model") +
    facet_wrap( ~ prediction_horizon, ncol = 1) +
    xlab("Prediction Time") +
    ylab("Weighted Influenza-like Illness") +
    ggtitle("(b) Point and 50% Interval Predictions, Influenza") +
#    ggtitle("Point and 95% Interval Predictions") +
    theme_bw()

@
\end{figure}

Figure~\ref{fig:AggregatedFluResultsBoxPlots} offers a more quantitative summary
of these results in terms of log scores.  Panel (a) of the figure compares the
predictions from KCDE with the predictions from SARIMA.  For the Dengue data
set, every KCDE specification consistently outperformed the SARIMA model by a
wide margin.
For the Influenza data set, the KCDE method performed slightly worse than SARIMA
when the periodic kernel component was not included, but it performed about as
well as SARIMA on average when the periodic kernel component was used.
%  For that data set, the average difference in log scores
% is slightly positive (indicating slightly improved performance relative to SARIMA)
%for the KCDE specification with both a periodic kernel component and a fully
%parameterized bandwidth matrix, but is slightly negative for the other KCDE
%specifications.

%% Relevant code to verify medians
%models_used <- unique(ili_prediction_results$full_model_descriptor[
%    !ili_prediction_results$differencing & !(ili_prediction_results$max_seasonal_lag == 1)])
%inds <- which(ili_prediction_log_score_diffs_from_sarima_long$model %in% models_used)
%tapply(ili_prediction_log_score_diffs_from_sarima_long$log_score_difference[inds],
%    ili_prediction_log_score_diffs_from_sarima_long$model[inds],
%    median)
%tapply(ili_prediction_log_score_diffs_from_sarima_long$log_score_difference[inds],
%    ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[inds],
%    mean)
%tapply(ili_prediction_log_score_diffs_from_sarima_long$log_score_difference[inds],
%    ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor[inds],
%    min)
%ili_prediction_results

Panel (b) of Figure~\ref{fig:AggregatedFluResultsBoxPlots} shows that for the
Influenza data set, including the periodic kernel component yields improvements
in the log scores that are obtained with the KCDE method.  The periodic
kernel component did not help as much in the application to predicting incidence of Dengue fever.  We believe this to
be the result of convergence problems in the estimation routine that could be
resolved with improved starting values or estimation methods.

\lboxit{Update discussion of results for periodic kernel in Dengue data set. 
Current results displayed in plot are not meaningful, I expect them to change.}

Panel (c) of Figure~\ref{fig:AggregatedFluResultsBoxPlots} shows that for
the Influenza data set, using a fully parameterized bandwidth matrix did not
lead to any improvements in log scores for the KCDE method relative
to the corresponding KCDE specifications with a diagonal bandwidth matrix.
%The
%mean and median difference in log scores for the KCDE specifications including the fully parameterized bandwidth relative to the
%specifications with diagonal bandwidths are positive for both base model
%specifications.
The plot appears to show that the bandwidth specification was more helpful in
the Dengue data set, but we do not believe that this is meaningful.
There is evidence of convergence problems in the estimation routine with a
diagonal bandwidth specification that led to poor performance with that model. 
We believe that these issues could be resolved with different choices of
starting values or modifications to the estimation routine; making these changes
would likely improve the performance of the KCDE specification with a diagonal
bandwidth specification.

%% Relevant code to verify medians
%inds <- which(combined_contrast_bw_full$data_set == "Influenza")
%tapply(combined_contrast_bw_full$contrast_value[inds],
%    factor(combined_contrast_bw_full$fixed_values[inds]),
%    median)
%tapply(combined_contrast_bw_full$contrast_value[inds],
%    factor(combined_contrast_bw_full$fixed_values[inds]),
%    mean)

\lboxit{Discuss results for bandwidth specification in Dengue data set.  Current
results displayed in plot are not meaningful, I expect them to change.}


\begin{figure}
\caption{Differences in log scores for the weekly predictive distributions among
pairs of models across all combinations of prediction horizon and prediction time in the test period.
In panel (a) positive values indicate cases when KCDE outperformed SARIMA.  In panel
(b) positive values indicate cases when the specification of KCDE with the
periodic kernel outperformed the corresponding specification without the periodic kernel.
In panel (c) positive values indicate cases when the specification of KCDE with
a fully parameterized bandwidth outperformed the KCDE specification with a diagonal
bandwidth matrix.}
\label{fig:AggregatedFluResultsBoxPlots}
<<FluDataResultsAggregatedBoxplots, echo = FALSE>>=
models_used <- unique(ili_prediction_results$full_model_descriptor[
    !ili_prediction_results$differencing & !(ili_prediction_results$max_seasonal_lag == 1)])

dengue_prediction_log_score_diffs_from_sarima_long$data_set <- "Dengue"
ili_prediction_log_score_diffs_from_sarima_long$data_set <- "Influenza"
combined_prediction_log_score_diffs_from_sarima_long <-
    rbind(dengue_prediction_log_score_diffs_from_sarima_long,
        ili_prediction_log_score_diffs_from_sarima_long[
            !(ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor == "Periodic Kernel,\nFull Bandwidth" &
                    ili_prediction_log_score_diffs_from_sarima_long$prediction_horizon == 50L),])

boxplot_sarima_contrasts <- ggplot() +
    geom_hline(yintercept = 0) +
    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor, colour = data_set),
        data = combined_prediction_log_score_diffs_from_sarima_long[combined_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ]) +
#        data = ili_prediction_log_score_diffs_from_sarima_long[ili_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ]) +
#    ylim(c(-4, 4)) +
    scale_colour_discrete("Data Set") +
    ggtitle("(a) Comparison of KCDE with SARIMA") +
#    xlab("Model") +
    xlab("") +
    ylab("Log Score Difference") +
#    ylab("Log Score KCDE -\nLog Score SARIMA") +
    theme_bw(base_size = 11)# +
#    theme(axis.text.x=element_text(angle = -90, hjust = 0))



#ili_contrast_periodic_kernel <- ili_prediction_log_score_diffs_from_sarima_long %>%
#    filter(model %in% models_used) %>%
#    select_("prediction_time", "prediction_horizon", "log_score_difference",
#        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
#    spread_("periodic",
#        "log_score_difference") %>%
#    mutate(
#        contrast_value = `TRUE` - `FALSE`)
#ili_contrast_periodic_kernel$fixed_values <- "Null Model"
#ili_contrast_periodic_kernel$fixed_values[ili_contrast_periodic_kernel$bw_full] <-
#    "Full Bandwidth"
#ili_contrast_periodic_kernel$fixed_values <-
#    factor(ili_contrast_periodic_kernel$fixed_values,
#        levels = c("Null Model", "Full Bandwidth"))

combined_contrast_periodic_kernel <- combined_prediction_log_score_diffs_from_sarima_long %>%
    filter(model %in% models_used) %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full", "data_set") %>%
    spread_("periodic",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`)
combined_contrast_periodic_kernel$fixed_values <- "Null Model"
combined_contrast_periodic_kernel$fixed_values[combined_contrast_periodic_kernel$bw_full] <-
    "Full Bandwidth"
combined_contrast_periodic_kernel$fixed_values <-
    factor(combined_contrast_periodic_kernel$fixed_values,
        levels = c("Null Model", "Full Bandwidth"))


#boxplot_periodic_kernel_contrasts <- ggplot(ili_contrast_periodic_kernel) +
boxplot_periodic_kernel_contrasts <- ggplot(combined_contrast_periodic_kernel) +
    geom_hline(yintercept = 0) +
    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = data_set)) +
#    facet_wrap( ~ fixed_values, ncol = 1) +
    scale_colour_discrete("Data Set") +
    ggtitle("(b) Effect of Adding Periodic Kernel to Model") +
#    xlab("Base Model") +
    xlab("") +
    ylab("Log Score Difference") +
#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
    theme_bw(base_size = 11)



#ili_contrast_bw_full <- ili_prediction_log_score_diffs_from_sarima_long %>%
#    filter(model %in% models_used) %>%
#    select_("prediction_time", "prediction_horizon", "log_score_difference",
#        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
#    spread_("bw_full",
#        "log_score_difference") %>%
#    mutate(
#        contrast_value = `TRUE` - `FALSE`)
#ili_contrast_bw_full$fixed_values <- "Null Model"
#ili_contrast_bw_full$fixed_values[ili_contrast_bw_full$periodic] <-
#    "Periodic Kernel"
#ili_contrast_bw_full$fixed_values <-
#    factor(ili_contrast_bw_full$fixed_values,
#        levels = c("Null Model", "Periodic Kernel"))

combined_contrast_bw_full <- combined_prediction_log_score_diffs_from_sarima_long %>%
    filter(model %in% models_used) %>%
    select_("prediction_time", "prediction_horizon", "log_score_difference",
        "seasonal_lag", "differencing", "periodic", "bw_full", "data_set") %>%
    spread_("bw_full",
        "log_score_difference") %>%
    mutate(
        contrast_value = `TRUE` - `FALSE`)
combined_contrast_bw_full$fixed_values <- "Null Model"
combined_contrast_bw_full$fixed_values[combined_contrast_bw_full$periodic] <-
    "Periodic Kernel"
combined_contrast_bw_full$fixed_values <-
    factor(combined_contrast_bw_full$fixed_values,
        levels = c("Null Model", "Periodic Kernel"))

#boxplot_bw_full_contrasts <- ggplot(ili_contrast_bw_full) +
boxplot_bw_full_contrasts <- ggplot(combined_contrast_bw_full) +
    geom_hline(yintercept = 0) +
    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = data_set)) +
#    facet_wrap( ~ fixed_values, ncol = 1) +
    scale_colour_discrete("Data Set") +
    ggtitle("(c) Effect of Adding Fully Parameterized BW to Model") +
#    xlab("Base Model") +
    xlab("") +
    ylab("Log Score Difference") +
#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
    theme_bw(base_size = 11)


grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow = 3, ncol = 1)))
print(boxplot_sarima_contrasts, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(boxplot_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
print(boxplot_bw_full_contrasts, vp = viewport(layout.pos.row = 3, layout.pos.col = 1))


#ili_prediction_log_score_diffs_from_sarima_long$fixed_values <-
#    as.character(ili_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor)
#ili_prediction_log_score_diffs_from_sarima_long$contrast_value <-
#    ili_prediction_log_score_diffs_from_sarima_long$log_score_difference
#ili_prediction_log_score_diffs_from_sarima_long$contrast_type <- "Difference from SARIMA"
#ili_contrast_periodic_kernel$contrast_type <- "Effect of Adding Periodic Kernel"
#ili_contrast_bw_full$contrast_type <- "Effect of Adding Full Bandwidth"
#
#ili_contrasts_merged <- rbind.fill(
#    ili_prediction_log_score_diffs_from_sarima_long[ili_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ],
#    ili_contrast_periodic_kernel,
#    ili_contrast_bw_full)
#
#ggplot(ili_contrasts_merged) +
#    geom_hline(yintercept = 0) +
#    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value)) +
#    facet_wrap( ~ contrast_type, ncol = 1) +
##    ggtitle("Effect of Adding Fully Parameterized BW to Model") +
#    xlab("") +
#    ylab("Log Score Difference") +
#    theme_bw()
@
\end{figure}

%\begin{figure}
%\caption{Plots of point and interval predictions from SARIMA and KCDE.}
%\label{fig:FluRibbonsPredictions}
%<<FluDataRibbonsPredictionPlot95Intervals, echo = FALSE>>=
%@
%\end{figure}





%\begin{figure}
%\caption{Differences in log scores for the weekly predictive distributions for
%Dengue among pairs of models across all combinations of prediction horizon and
%prediction time in the test period.
%In panel (a) positive values indicate cases when KCDE outperformed SARIMA.  In panel
%(b) positive values indicate cases when the specification of KCDE with the
%periodic kernel outperformed the corresponding specification without the periodic kernel.
%In panel (c) positive values indicate cases when the specification of KCDE with
%a fully parameterized bandwidth outperformed the KCDE specification with a diagonal
%bandwidth matrix.}
%\label{fig:AggregatedDengueResultsBoxPlots}
%<<DengueDataResultsAggregatedBoxplots, echo = FALSE>>=
%models_used <- unique(dengue_prediction_results$full_model_descriptor[
%    !dengue_prediction_results$differencing & !(dengue_prediction_results$max_seasonal_lag == 1)])
%
%boxplot_sarima_contrasts <- ggplot() +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor),
%        data = dengue_prediction_log_score_diffs_from_sarima_long[dengue_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ]) +
%#    ylim(c(-4, 4)) +
%    ggtitle("(a) Comparison of KCDE with SARIMA") +
%#    xlab("Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score KCDE -\nLog Score SARIMA") +
%    theme_bw(base_size = 11)# +
%#    theme(axis.text.x=element_text(angle = -90, hjust = 0))
%
%
%
%dengue_contrast_periodic_kernel <- dengue_prediction_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("prediction_time", "prediction_horizon", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("periodic",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_contrast_periodic_kernel$fixed_values <- "Null Model"
%dengue_contrast_periodic_kernel$fixed_values[dengue_contrast_periodic_kernel$bw_full] <-
%    "Full Bandwidth"
%dengue_contrast_periodic_kernel$fixed_values <-
%    factor(dengue_contrast_periodic_kernel$fixed_values,
%        levels = c("Null Model", "Full Bandwidth"))
%
%boxplot_periodic_kernel_contrasts <- ggplot(dengue_contrast_periodic_kernel) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("(b) Effect of Adding Periodic Kernel to Model") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
%    theme_bw(base_size = 11)
%
%
%
%dengue_contrast_bw_full <- dengue_prediction_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("prediction_time", "prediction_horizon", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("bw_full",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_contrast_bw_full$fixed_values <- "Null Model"
%dengue_contrast_bw_full$fixed_values[dengue_contrast_bw_full$periodic] <-
%    "Periodic Kernel"
%dengue_contrast_bw_full$fixed_values <-
%    factor(dengue_contrast_bw_full$fixed_values,
%        levels = c("Null Model", "Periodic Kernel"))
%
%boxplot_bw_full_contrasts <- ggplot(dengue_contrast_bw_full) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("(c) Effect of Adding Fully Parameterized BW to Model") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
%    theme_bw(base_size = 11)
%
%
%grid.newpage()
%pushViewport(viewport(layout = grid.layout(nrow = 3, ncol = 1)))
%print(boxplot_sarima_contrasts, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
%print(boxplot_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
%print(boxplot_bw_full_contrasts, vp = viewport(layout.pos.row = 3, layout.pos.col = 1))
%
%
%#dengue_prediction_log_score_diffs_from_sarima_long$fixed_values <-
%#    as.character(dengue_prediction_log_score_diffs_from_sarima_long$reduced_model_descriptor)
%#dengue_prediction_log_score_diffs_from_sarima_long$contrast_value <-
%#    dengue_prediction_log_score_diffs_from_sarima_long$log_score_difference
%#dengue_prediction_log_score_diffs_from_sarima_long$contrast_type <- "Difference from SARIMA"
%#dengue_contrast_periodic_kernel$contrast_type <- "Effect of Adding Periodic Kernel"
%#dengue_contrast_bw_full$contrast_type <- "Effect of Adding Full Bandwidth"
%#
%#dengue_contrasts_merged <- rbind.fill(
%#    dengue_prediction_log_score_diffs_from_sarima_long[dengue_prediction_log_score_diffs_from_sarima_long$model %in% models_used, ],
%#    dengue_contrast_periodic_kernel,
%#    dengue_contrast_bw_full)
%#
%#ggplot(dengue_contrasts_merged) +
%#    geom_hline(yintercept = 0) +
%#    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value)) +
%#    facet_wrap( ~ contrast_type, ncol = 1) +
%##    ggtitle("Effect of Adding Fully Parameterized BW to Model") +
%#    xlab("") +
%#    ylab("Log Score Difference") +
%#    theme_bw()
%@
%\end{figure}




\subsection{Predictive Distributions for Peak Week Timing and Incidence}

Figure~\ref{fig:CombinedPeakWeekTimingPredictionLogScores} displays the log
score of the predictive distributions for peak week timing obtained from SARIMA
and KCDE models over the course of each season in the test data sets.
There is no consistent pattern of KCDE either outperforming or underperforming
relative to SARIMA for predictions of peak week timing.
Rather, in some seasons SARIMA is better than KCDE and in other seasons KCDE
tends to do better than SARIMA.

If we consider the three seasons in the Influenza data set, KCDE tended to
outperform SARIMA for predicting peak week timing in the 2011/2012 season, but
underperformed relative to SARIMA in the 2012/2013 and 2013/2014 seasons.  It is
difficult to extrapolate from three test seasons, but a look at the data reveals two interesting factors related
to historical timing of season peaks that may explain differences in the
relative performance of the models in these seasons.
First, the 2011/2012 season had the latest peak week of any season in the data
set.  Second, there is a consistent ``mini-peak'' in reported incidence for
influenza at Christmas week, which we illustrate in Figure *** in the supplemental materials.  In the
2012/2013 and 2013/2014 seasons, this ``Christmas effect'' coincided with the season peak.
The structure of the SARIMA model using seasonal differencing and seasonally
lagged incidence allows that model to pick up on this Christmas effect, and the
method scored well for those two seasons.  In the season when the peak
occurred later than it had in the training data, KCDE outperformed SARIMA. 
Overall, this points to SARIMA having picked up on a relevant feature of the
data generating process that KCDE did not, but the predictive distributions from
KCDE are generally concentrated in the right area and are broad enough to capture unusual
events.



%We have plotted this distribution for 2012/2013 season
%in Figure ***; the distributions for all seasons are in the supplemental materials.  In that plot,
%we can see that from the start, the SARIMA model assigns high probability that
%the season peak will be during the week of Christmas as there was a local peak
%in Christmas week the prior season.  The KCDE model does not have this
%structure, and so does not pick up on the Christmas week effect.  Its predictive
%distribution for peak week timing is much smoother than the distribution from
%SARIMA.  In seasons where the peak occurred at Christmas week, this is a
%disadvantage for KCDE.  On the other hand, in the 2011/2012 season, the peak
%occurred much later.  In that season, KCDE put much larger probability on the
%peak week that was eventually realized than SARIMA did.

\lboxit{to do: christmas effect plot in supplemental materials}

In the predictions of peak week timing for
Dengue, the log scores from the KCDE method are much choppier than the log
scores from SARIMA.  This indicates a different defficiency of our approach
relative to SARIMA in that data set: whereas the predictive distributions from SARIMA
converge relatively smoothly to the correct peak week, the distributions from
KCDE do not converge as smoothly.  We believe this to be related to problems
with convergence of the parameter estimates in the model fitting
process for the Dengue data.

<<FluDataMergePeakWeekPredictionResults, echo = FALSE>>=
data_set <- "ili_national"

prediction_save_path <- file.path("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results",
    data_set,
    "prediction-results")

all_max_lags <- as.character(c(1L))
#all_max_seasonal_lags <- as.character(c(0L, 1L))
all_max_seasonal_lags <- as.character(c(0L))
all_filtering_values <- c("FALSE")
#all_differencing_values <- c("FALSE", "TRUE")
all_differencing_values <- c("FALSE")
all_seasonality_values <- c("FALSE", "TRUE")
all_bw_parameterizations <- c("diagonal", "full")

case_definitions <- expand.grid(
        data_set,
        all_max_lags,
        all_max_seasonal_lags,
        all_filtering_values,
        all_differencing_values,
        all_seasonality_values,
        all_bw_parameterizations,
        stringsAsFactors = FALSE) %>%
    `colnames<-`(c("data_set",
            "max_lag",
            "max_seasonal_lag",
            "filtering",
            "differencing",
            "seasonality",
            "bw_parameterization"))
 
ili_peak_week_results <- rbind.fill(
    c(
        list(
            readRDS(file.path(prediction_save_path,
                        paste0("peak-week-sarima-", data_set, ".rds"))) %>%
                mutate(model = "SARIMA")
        ),
        lapply(seq_len(nrow(case_definitions)), function(case_row_ind) {
                max_lag <- case_definitions$max_lag[case_row_ind]
                max_seasonal_lag <- case_definitions$max_seasonal_lag[case_row_ind]
                filtering <- case_definitions$filtering[case_row_ind]
                differencing <- case_definitions$differencing[case_row_ind]
                seasonality <- case_definitions$seasonality[case_row_ind]
                bw_parameterization <- case_definitions$bw_parameterization[case_row_ind]
                
                case_descriptor <- paste0(
                    data_set,
                    "-max_lag_", max_lag,
                    "-max_seasonal_lag_", max_seasonal_lag,
                    "-filtering_", filtering,
                    "-differencing_", differencing,
                    "-seasonality_", seasonality,
                    "-bw_parameterization_", bw_parameterization
                )
                
                readRDS(file.path(prediction_save_path,
                            paste0("peak-week-", case_descriptor, ".rds"))) %>%
                    mutate(model = "KCDE",
                        max_lag = max_lag,
                        max_seasonal_lag = max_seasonal_lag,
                        filtering = filtering,
                        differencing = differencing,
                        seasonality = seasonality,
                        bw_parameterization = bw_parameterization)
            })
    )
)

ili_peak_week_results$full_model_descriptor <- paste0(ili_peak_week_results$model,
    "-seasonal_lag_", ili_peak_week_results$max_seasonal_lag,
#    "-filtering_", ili_prediction_results$filtering,
    "-differencing_", ili_peak_week_results$differencing,
    "-periodic_", ili_peak_week_results$seasonality,
    "-bw_", ili_peak_week_results$bw_parameterization)

ili_peak_week_results$peak_week_log_score[ili_peak_week_results$peak_week_log_score < -50] <- -50
ili_peak_week_results$peak_height_log_score[ili_peak_week_results$peak_height_log_score < -50] <- -50

ili_peak_week_results$reduced_model_descriptor <- "Null KCDE Model"
ili_peak_week_results$reduced_model_descriptor[
    as.logical(ili_peak_week_results$seasonality) & !(ili_peak_week_results$bw_parameterization == "full")] <-
    "Periodic Kernel"
ili_peak_week_results$reduced_model_descriptor[
    !as.logical(ili_peak_week_results$seasonality) & (ili_peak_week_results$bw_parameterization == "full")] <-
    "Full Bandwidth"
ili_peak_week_results$reduced_model_descriptor[
    as.logical(ili_peak_week_results$seasonality) & (ili_peak_week_results$bw_parameterization == "full")] <-
    "Periodic Kernel,\nFull Bandwidth"
ili_peak_week_results$reduced_model_descriptor[
    ili_peak_week_results$model == "SARIMA"] <-
    "SARIMA"

num_analysis_time_season_values <- length(unique(ili_peak_week_results$analysis_time_season))
num_analysis_time_season_week_values <- length(unique(ili_peak_week_results$analysis_time_season_week))
ili_peak_week_results <- rbind.fill(ili_peak_week_results,
    data.frame(
        full_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        reduced_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        analysis_time_season = rep(unique(ili_peak_week_results$analysis_time_season), each = num_analysis_time_season_week_values),
        analysis_time_season_week = rep(unique(ili_peak_week_results$analysis_time_season_week), times = num_analysis_time_season_values),
        peak_week_log_score = rep(log(1/52), num_analysis_time_season_week_values * num_analysis_time_season_values),
        peak_height_log_score = rep(log(1/27), num_analysis_time_season_week_values * num_analysis_time_season_values)
    ))


ili_peak_week_results$reduced_model_descriptor <-
    factor(ili_peak_week_results$reduced_model_descriptor,
        levels = c("SARIMA", "Null KCDE Model", "Full Bandwidth", "Periodic Kernel",
            "Periodic Kernel,\nFull Bandwidth",
            "Equal Bin Probabilities"
        ))

#geom_hline(yintercept = log(1/31), colour = "grey", linetype = 2)

ili_peak_week_times <- data.frame(
    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
    peak_week = sapply(unique(ili_peak_week_results$analysis_time_season),
        function(season_val) {
            max_incidence_in_season <-
                max(ili_national$weighted_ili[ili_national$season == season_val])
            return(ili_national$season_week[ili_national$season == season_val &
                        ili_national$weighted_ili == max_incidence_in_season])
        })
)

ili_peak_week_heights <- data.frame(
    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
    peak_height = sapply(unique(ili_peak_week_results$analysis_time_season),
        function(season_val) {
            return(max(ili_national$weighted_ili[ili_national$season == season_val]))
        })
)
        
ili_peak_week_results$peak_week_log_score[ili_peak_week_results$peak_week_log_score == -50] <- NA
ili_peak_week_results$peak_height_log_score[ili_peak_week_results$peak_height_log_score == -50] <- NA
@


%\begin{figure}
%\caption{Differences in log scores for the predictive distributions for the peak
%week and incidence at the peak week among pairs of models across all analysis
%times in the test period.
%In panel (a) positive values indicate cases when KCDE outperformed SARIMA.  In panel
%(b) positive values indicate cases when the specification of KCDE with the
%periodic kernel outperformed the corresponding specification without the periodic kernel.
%In panel (c) positive values indicate cases when the specification of KCDE with
%a fully parameterized bandwidth outperformed the KCDE specification with a diagonal
%bandwidth matrix.  In the plot for peak week timing in panel (a), the log score
%differences are not displayed for one analysis time when none of the simulated
%trajectories from SARIMA peaked at the true peak week.  In that case, our
%monte carlo estimate of the difference in log scores is infinity.}
%\label{fig:FluPeakWeekPredictions}
%<<FluDataPeakWeekPredictionBoxPlots, echo = FALSE>>=
%ili_peak_week_times <- data.frame(
%    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
%    peak_week = sapply(unique(ili_peak_week_results$analysis_time_season),
%        function(season_val) {
%            max_incidence_in_season <-
%                max(ili_national$weighted_ili[ili_national$season == season_val])
%            return(ili_national$season_week[ili_national$season == season_val &
%                        ili_national$weighted_ili == max_incidence_in_season])
%        })
%)
%
%ili_peak_week_heights <- data.frame(
%    analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
%    peak_height = sapply(unique(ili_peak_week_results$analysis_time_season),
%        function(season_val) {
%            return(max(ili_national$weighted_ili[ili_national$season == season_val]))
%        })
%)
%
%## Contrasts with SARIMA for peak week timing
%ili_peak_timing_log_score_diffs_from_sarima_wide <- ili_peak_week_results %>%
%    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
%    spread(full_model_descriptor, peak_week_log_score)
%
%ili_peak_timing_log_score_diffs_from_sarima_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
%    ili_peak_timing_log_score_diffs_from_sarima_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
%    ili_peak_timing_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
%
%ili_peak_timing_log_score_diffs_from_sarima_long <- ili_peak_timing_log_score_diffs_from_sarima_wide %>%
%    gather_("model", "log_score_difference", unique(ili_peak_week_results$full_model_descriptor)) %>%
%    mutate(
%        seasonal_lag = as.logical(grepl("seasonal_lag_1", model)),
%        differencing = as.logical(grepl("differencing_TRUE", model)),
%        periodic = as.logical(grepl("periodic_TRUE", model)),
%        bw_full = as.logical(grepl("bw_full", model))
%    )
%ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
%ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    ili_peak_timing_log_score_diffs_from_sarima_long$periodic & !ili_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel"
%ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    !ili_peak_timing_log_score_diffs_from_sarima_long$periodic & ili_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Full Bandwidth"
%ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    ili_peak_timing_log_score_diffs_from_sarima_long$periodic & ili_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel,\nFull Bandwidth"
%ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
%    factor(ili_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor,
%        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
%
%ili_peak_timing_log_score_diffs_from_sarima_long$leq_peak_week <-
%    ili_peak_timing_log_score_diffs_from_sarima_long$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_peak_timing_log_score_diffs_from_sarima_long$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%models_used <- unique(ili_peak_week_results$full_model_descriptor[
%        !as.logical(ili_peak_week_results$differencing) & !(ili_peak_week_results$max_seasonal_lag == 1)])
%
%boxplot_timing_sarima_contrasts <- ggplot() +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor, colour = leq_peak_week),
%        data = ili_peak_timing_log_score_diffs_from_sarima_long[
%            ili_peak_timing_log_score_diffs_from_sarima_long$model %in% models_used & 
%                ili_peak_timing_log_score_diffs_from_sarima_long$log_score_difference < 40, ]) +
%#    ylim(c(-4, 4)) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score KCDE -\nLog Score SARIMA") +
%    theme_bw(base_size = 11)# +
%
%
%
%## Contrasts with SARIMA for peak week height
%ili_peak_height_log_score_diffs_from_sarima_wide <- ili_peak_week_results %>%
%    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
%    spread(full_model_descriptor, peak_height_log_score)
%
%ili_peak_height_log_score_diffs_from_sarima_wide[, unique(ili_peak_week_results$full_model_descriptor)] <-
%    ili_peak_height_log_score_diffs_from_sarima_wide[, unique(ili_peak_week_results$full_model_descriptor)] -
%    ili_peak_height_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
%
%ili_peak_height_log_score_diffs_from_sarima_long <- ili_peak_height_log_score_diffs_from_sarima_wide %>%
%    gather_("model", "log_score_difference", unique(ili_peak_week_results$full_model_descriptor)) %>%
%    mutate(
%        seasonal_lag = as.logical(grepl("seasonal_lag_1", model)),
%        differencing = as.logical(grepl("differencing_TRUE", model)),
%        periodic = as.logical(grepl("periodic_TRUE", model)),
%        bw_full = as.logical(grepl("bw_full", model))
%    )
%ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
%ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    ili_peak_height_log_score_diffs_from_sarima_long$periodic & !ili_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel"
%ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    !ili_peak_height_log_score_diffs_from_sarima_long$periodic & ili_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Full Bandwidth"
%ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    ili_peak_height_log_score_diffs_from_sarima_long$periodic & ili_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel,\nFull Bandwidth"
%ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
%    factor(ili_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor,
%        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
%
%ili_peak_height_log_score_diffs_from_sarima_long$leq_peak_week <-
%    ili_peak_height_log_score_diffs_from_sarima_long$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_peak_height_log_score_diffs_from_sarima_long$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%models_used <- unique(ili_peak_week_results$full_model_descriptor[
%        !as.logical(ili_peak_week_results$differencing) & !(ili_peak_week_results$max_seasonal_lag == 1)])
%
%boxplot_height_sarima_contrasts <- ggplot() +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor, colour = leq_peak_week),
%        data = ili_peak_height_log_score_diffs_from_sarima_long[ili_peak_height_log_score_diffs_from_sarima_long$model %in% models_used, ]) +
%#    ylim(c(-4, 4)) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score KCDE -\nLog Score SARIMA") +
%    theme_bw(base_size = 11)# +
%
%
%
%ili_timing_contrast_periodic_kernel <- ili_peak_timing_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("periodic",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%ili_timing_contrast_periodic_kernel$fixed_values <- "Null Model"
%ili_timing_contrast_periodic_kernel$fixed_values[ili_timing_contrast_periodic_kernel$bw_full] <-
%    "Full Bandwidth"
%ili_timing_contrast_periodic_kernel$fixed_values <-
%    factor(ili_timing_contrast_periodic_kernel$fixed_values,
%        levels = c("Null Model", "Full Bandwidth"))
%
%ili_timing_contrast_periodic_kernel$leq_peak_week <-
%    ili_timing_contrast_periodic_kernel$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_timing_contrast_periodic_kernel$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%
%ili_height_contrast_periodic_kernel <- ili_peak_height_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("periodic",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%ili_height_contrast_periodic_kernel$fixed_values <- "Null Model"
%ili_height_contrast_periodic_kernel$fixed_values[ili_height_contrast_periodic_kernel$bw_full] <-
%    "Full Bandwidth"
%ili_height_contrast_periodic_kernel$fixed_values <-
%    factor(ili_height_contrast_periodic_kernel$fixed_values,
%        levels = c("Null Model", "Full Bandwidth"))
%
%ili_height_contrast_periodic_kernel$leq_peak_week <-
%    ili_height_contrast_periodic_kernel$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_height_contrast_periodic_kernel$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%
%boxplot_timing_periodic_kernel_contrasts <- ggplot(ili_timing_contrast_periodic_kernel) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
%    theme_bw(base_size = 11)
%
%
%boxplot_height_periodic_kernel_contrasts <- ggplot(ili_height_contrast_periodic_kernel) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
%    theme_bw(base_size = 11)
%
%
%ili_timing_contrast_bw_full <- ili_peak_timing_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("bw_full",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%ili_timing_contrast_bw_full$fixed_values <- "Null Model"
%ili_timing_contrast_bw_full$fixed_values[ili_timing_contrast_bw_full$periodic] <-
%    "Periodic Kernel"
%ili_timing_contrast_bw_full$fixed_values <-
%    factor(ili_timing_contrast_bw_full$fixed_values,
%        levels = c("Null Model", "Periodic Kernel"))
%
%ili_timing_contrast_bw_full$leq_peak_week <-
%    ili_timing_contrast_bw_full$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_timing_contrast_bw_full$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%
%ili_height_contrast_bw_full <- ili_peak_height_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("bw_full",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%ili_height_contrast_bw_full$fixed_values <- "Null Model"
%ili_height_contrast_bw_full$fixed_values[ili_height_contrast_bw_full$periodic] <-
%    "Periodic Kernel"
%ili_height_contrast_bw_full$fixed_values <-
%    factor(ili_height_contrast_bw_full$fixed_values,
%        levels = c("Null Model", "Periodic Kernel"))
%
%ili_height_contrast_bw_full$leq_peak_week <-
%    ili_height_contrast_bw_full$analysis_time_season_week <=
%    ili_peak_week_times$peak_week[
%        sapply(ili_height_contrast_bw_full$analysis_time_season,
%            function(season_val) which(ili_peak_week_times$analysis_time_season == season_val))]
%
%
%boxplot_timing_bw_full_contrasts <- ggplot(ili_timing_contrast_bw_full) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
%    theme_bw(base_size = 11)
%
%boxplot_height_bw_full_contrasts <- ggplot(ili_height_contrast_bw_full) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
%    theme_bw(base_size = 11)
%
%
%grid.newpage()
%pushViewport(viewport(layout =
%    grid.layout(nrow = 6, ncol = 2, heights = unit(rep(1, 6), rep(c("lines", "null"), times = 3)))))
%grid.text("(a) Comparison of KCDE with SARIMA",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2))
%print(boxplot_timing_sarima_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
%print(boxplot_height_sarima_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))
%grid.text("(b) Effect of Adding Periodic Kernel to Model",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 3, layout.pos.col = 1:2))
%print(boxplot_timing_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 4, layout.pos.col = 1))
%print(boxplot_height_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 4, layout.pos.col = 2))
%grid.text("(c) Effect of Adding Fully Parameterized BW to Model",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 5, layout.pos.col = 1:2))
%print(boxplot_timing_bw_full_contrasts, vp = viewport(layout.pos.row = 6, layout.pos.col = 1))
%print(boxplot_height_bw_full_contrasts, vp = viewport(layout.pos.row = 6, layout.pos.col = 2))
%@
%\end{figure}
%
%\begin{figure}
%\caption{Differences in log scores for the predictive distributions for the peak
%week and incidence at the peak week for Dengue among pairs of models across all
%analysis times in the test period.
%In panel (a) positive values indicate cases when KCDE outperformed SARIMA.  In panel
%(b) positive values indicate cases when the specification of KCDE with the
%periodic kernel outperformed the corresponding specification without the periodic kernel.
%In panel (c) positive values indicate cases when the specification of KCDE with
%a fully parameterized bandwidth outperformed the KCDE specification with a diagonal
%bandwidth matrix.  In the plot for peak week timing in panel (a), the log score
%differences are not displayed for one analysis time when none of the simulated
%trajectories from SARIMA peaked at the true peak week.  In that case, our
%monte carlo estimate of the difference in log scores is infinity.}
%\label{fig:DenguePeakWeekPredictions}
%<<DengueDataPeakWeekPredictionBoxPlots, echo = FALSE, dependson = c("DengueDataMergePeakWeekPredictionResults")>>=
%dengue_peak_week_times <- data.frame(
%    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
%    peak_week = sapply(unique(dengue_peak_week_results$analysis_time_season),
%        function(season_val) {
%            max_incidence_in_season <-
%                max(dengue_sj$total_cases[dengue_sj$season == season_val])
%            return(dengue_sj$season_week[dengue_sj$season == season_val &
%                        dengue_sj$total_cases == max_incidence_in_season])
%        })
%)
%
%dengue_peak_week_heights <- data.frame(
%    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
%    peak_height = sapply(unique(dengue_peak_week_results$analysis_time_season),
%        function(season_val) {
%            return(max(dengue_sj$total_cases[dengue_sj$season == season_val]))
%        })
%)
%
%## Contrasts with SARIMA for peak week timing
%dengue_peak_timing_log_score_diffs_from_sarima_wide <- dengue_peak_week_results %>%
%    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_week_log_score) %>%
%    spread(full_model_descriptor, peak_week_log_score)
%
%dengue_peak_timing_log_score_diffs_from_sarima_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
%    dengue_peak_timing_log_score_diffs_from_sarima_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
%    dengue_peak_timing_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
%
%dengue_peak_timing_log_score_diffs_from_sarima_long <- dengue_peak_timing_log_score_diffs_from_sarima_wide %>%
%    gather_("model", "log_score_difference", unique(dengue_peak_week_results$full_model_descriptor)) %>%
%    mutate(
%        seasonal_lag = as.logical(grepl("seasonal_lag_1", model)),
%        differencing = as.logical(grepl("differencing_TRUE", model)),
%        periodic = as.logical(grepl("periodic_TRUE", model)),
%        bw_full = as.logical(grepl("bw_full", model))
%    )
%dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
%dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    dengue_peak_timing_log_score_diffs_from_sarima_long$periodic & !dengue_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel"
%dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    !dengue_peak_timing_log_score_diffs_from_sarima_long$periodic & dengue_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Full Bandwidth"
%dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    dengue_peak_timing_log_score_diffs_from_sarima_long$periodic & dengue_peak_timing_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel,\nFull Bandwidth"
%dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
%    factor(dengue_peak_timing_log_score_diffs_from_sarima_long$reduced_model_descriptor,
%        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
%
%dengue_peak_timing_log_score_diffs_from_sarima_long$leq_peak_week <-
%    dengue_peak_timing_log_score_diffs_from_sarima_long$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_peak_timing_log_score_diffs_from_sarima_long$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%models_used <- unique(dengue_peak_week_results$full_model_descriptor[
%        !as.logical(dengue_peak_week_results$differencing) & !(dengue_peak_week_results$max_seasonal_lag == 1)])
%
%boxplot_timing_sarima_contrasts <- ggplot() +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor, colour = leq_peak_week),
%        data = dengue_peak_timing_log_score_diffs_from_sarima_long[
%            dengue_peak_timing_log_score_diffs_from_sarima_long$model %in% models_used & 
%                dengue_peak_timing_log_score_diffs_from_sarima_long$log_score_difference < 40, ]) +
%#    ylim(c(-4, 4)) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score KCDE -\nLog Score SARIMA") +
%    theme_bw(base_size = 11)# +
%
%
%
%## Contrasts with SARIMA for peak week height
%dengue_peak_height_log_score_diffs_from_sarima_wide <- dengue_peak_week_results %>%
%    select(full_model_descriptor, analysis_time_season, analysis_time_season_week, peak_height_log_score) %>%
%    spread(full_model_descriptor, peak_height_log_score)
%
%dengue_peak_height_log_score_diffs_from_sarima_wide[, unique(dengue_peak_week_results$full_model_descriptor)] <-
%    dengue_peak_height_log_score_diffs_from_sarima_wide[, unique(dengue_peak_week_results$full_model_descriptor)] -
%    dengue_peak_height_log_score_diffs_from_sarima_wide[, "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"]
%
%dengue_peak_height_log_score_diffs_from_sarima_long <- dengue_peak_height_log_score_diffs_from_sarima_wide %>%
%    gather_("model", "log_score_difference", unique(dengue_peak_week_results$full_model_descriptor)) %>%
%    mutate(
%        seasonal_lag = as.logical(grepl("seasonal_lag_1", model)),
%        differencing = as.logical(grepl("differencing_TRUE", model)),
%        periodic = as.logical(grepl("periodic_TRUE", model)),
%        bw_full = as.logical(grepl("bw_full", model))
%    )
%dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor <- "Null Model"
%dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    dengue_peak_height_log_score_diffs_from_sarima_long$periodic & !dengue_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel"
%dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    !dengue_peak_height_log_score_diffs_from_sarima_long$periodic & dengue_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Full Bandwidth"
%dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor[
%    dengue_peak_height_log_score_diffs_from_sarima_long$periodic & dengue_peak_height_log_score_diffs_from_sarima_long$bw_full] <-
%    "Periodic Kernel,\nFull Bandwidth"
%dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor <-
%    factor(dengue_peak_height_log_score_diffs_from_sarima_long$reduced_model_descriptor,
%        levels = c("Null Model", "Full Bandwidth", "Periodic Kernel", "Periodic Kernel,\nFull Bandwidth"))
%
%dengue_peak_height_log_score_diffs_from_sarima_long$leq_peak_week <-
%    dengue_peak_height_log_score_diffs_from_sarima_long$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_peak_height_log_score_diffs_from_sarima_long$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%models_used <- unique(dengue_peak_week_results$full_model_descriptor[
%        !as.logical(dengue_peak_week_results$differencing) & !(dengue_peak_week_results$max_seasonal_lag == 1)])
%
%boxplot_height_sarima_contrasts <- ggplot() +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(y = log_score_difference, x = reduced_model_descriptor, colour = leq_peak_week),
%        data = dengue_peak_height_log_score_diffs_from_sarima_long[dengue_peak_height_log_score_diffs_from_sarima_long$model %in% models_used, ]) +
%#    ylim(c(-4, 4)) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score KCDE -\nLog Score SARIMA") +
%    theme_bw(base_size = 11)# +
%
%
%
%dengue_timing_contrast_periodic_kernel <- dengue_peak_timing_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("periodic",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_timing_contrast_periodic_kernel$fixed_values <- "Null Model"
%dengue_timing_contrast_periodic_kernel$fixed_values[dengue_timing_contrast_periodic_kernel$bw_full] <-
%    "Full Bandwidth"
%dengue_timing_contrast_periodic_kernel$fixed_values <-
%    factor(dengue_timing_contrast_periodic_kernel$fixed_values,
%        levels = c("Null Model", "Full Bandwidth"))
%
%dengue_timing_contrast_periodic_kernel$leq_peak_week <-
%    dengue_timing_contrast_periodic_kernel$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_timing_contrast_periodic_kernel$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%
%dengue_height_contrast_periodic_kernel <- dengue_peak_height_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("periodic",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_height_contrast_periodic_kernel$fixed_values <- "Null Model"
%dengue_height_contrast_periodic_kernel$fixed_values[dengue_height_contrast_periodic_kernel$bw_full] <-
%    "Full Bandwidth"
%dengue_height_contrast_periodic_kernel$fixed_values <-
%    factor(dengue_height_contrast_periodic_kernel$fixed_values,
%        levels = c("Null Model", "Full Bandwidth"))
%
%dengue_height_contrast_periodic_kernel$leq_peak_week <-
%    dengue_height_contrast_periodic_kernel$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_height_contrast_periodic_kernel$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%
%boxplot_timing_periodic_kernel_contrasts <- ggplot(dengue_timing_contrast_periodic_kernel) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
%    theme_bw(base_size = 11)
%
%
%boxplot_height_periodic_kernel_contrasts <- ggplot(dengue_height_contrast_periodic_kernel) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score Model With Periodic Kernel -\nLog Score Model Without Periodic Kernel") +
%    theme_bw(base_size = 11)
%
%
%dengue_timing_contrast_bw_full <- dengue_peak_timing_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("bw_full",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_timing_contrast_bw_full$fixed_values <- "Null Model"
%dengue_timing_contrast_bw_full$fixed_values[dengue_timing_contrast_bw_full$periodic] <-
%    "Periodic Kernel"
%dengue_timing_contrast_bw_full$fixed_values <-
%    factor(dengue_timing_contrast_bw_full$fixed_values,
%        levels = c("Null Model", "Periodic Kernel"))
%
%dengue_timing_contrast_bw_full$leq_peak_week <-
%    dengue_timing_contrast_bw_full$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_timing_contrast_bw_full$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%
%dengue_height_contrast_bw_full <- dengue_peak_height_log_score_diffs_from_sarima_long %>%
%    filter(model %in% models_used) %>%
%    select_("analysis_time_season", "analysis_time_season_week", "log_score_difference",
%        "seasonal_lag", "differencing", "periodic", "bw_full") %>%
%    spread_("bw_full",
%        "log_score_difference") %>%
%    mutate(
%        contrast_value = `TRUE` - `FALSE`)
%dengue_height_contrast_bw_full$fixed_values <- "Null Model"
%dengue_height_contrast_bw_full$fixed_values[dengue_height_contrast_bw_full$periodic] <-
%    "Periodic Kernel"
%dengue_height_contrast_bw_full$fixed_values <-
%    factor(dengue_height_contrast_bw_full$fixed_values,
%        levels = c("Null Model", "Periodic Kernel"))
%
%dengue_height_contrast_bw_full$leq_peak_week <-
%    dengue_height_contrast_bw_full$analysis_time_season_week <=
%    dengue_peak_week_times$peak_week[
%        sapply(dengue_height_contrast_bw_full$analysis_time_season,
%            function(season_val) which(dengue_peak_week_times$analysis_time_season == season_val))]
%
%
%boxplot_timing_bw_full_contrasts <- ggplot(dengue_timing_contrast_bw_full) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Timing") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("Log Score Difference") +
%#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
%    theme_bw(base_size = 11)
%
%boxplot_height_bw_full_contrasts <- ggplot(dengue_height_contrast_bw_full) +
%    geom_hline(yintercept = 0) +
%    geom_boxplot(aes(x = factor(fixed_values), y = contrast_value, colour = leq_peak_week)) +
%#    facet_wrap( ~ fixed_values, ncol = 1) +
%    ggtitle("Peak Week Incidence") +
%#    xlab("Base Model") +
%    xlab("") +
%    ylab("") +
%#    ylab("Log Score Model With Full Bandwidth -\nLog Score Model With Diagonal Bandwidth") +
%    theme_bw(base_size = 11)
%
%
%grid.newpage()
%pushViewport(viewport(layout =
%    grid.layout(nrow = 6, ncol = 2, heights = unit(rep(1, 6), rep(c("lines", "null"), times = 3)))))
%grid.text("(a) Comparison of KCDE with SARIMA",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2))
%print(boxplot_timing_sarima_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
%print(boxplot_height_sarima_contrasts, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))
%grid.text("(b) Effect of Adding Periodic Kernel to Model",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 3, layout.pos.col = 1:2))
%print(boxplot_timing_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 4, layout.pos.col = 1))
%print(boxplot_height_periodic_kernel_contrasts, vp = viewport(layout.pos.row = 4, layout.pos.col = 2))
%grid.text("(c) Effect of Adding Fully Parameterized BW to Model",
%    gp = gpar(fontsize = 12),
%    vp = viewport(layout.pos.row = 5, layout.pos.col = 1:2))
%print(boxplot_timing_bw_full_contrasts, vp = viewport(layout.pos.row = 6, layout.pos.col = 1))
%print(boxplot_height_bw_full_contrasts, vp = viewport(layout.pos.row = 6, layout.pos.col = 2))
%@
%\end{figure}




<<DengueDataMergePeakWeekPredictionResults, echo = FALSE>>=
data_set <- "dengue_sj"
    
prediction_save_path <- file.path("/media/evan/data/Reich/infectious-disease-prediction-with-kcde/inst/results",
    data_set,
    "prediction-results")

all_max_lags <- as.character(c(1L))
#all_max_seasonal_lags <- as.character(c(0L, 1L))
all_max_seasonal_lags <- as.character(c(0L))
all_filtering_values <- c("FALSE")
#all_differencing_values <- c("FALSE", "TRUE")
all_differencing_values <- "FALSE"
all_seasonality_values <- c("FALSE", "TRUE")
all_bw_parameterizations <- c("diagonal", "full")

case_definitions <- expand.grid(
        data_set,
        all_max_lags,
        all_max_seasonal_lags,
        all_filtering_values,
        all_differencing_values,
        all_seasonality_values,
        all_bw_parameterizations,
        stringsAsFactors = FALSE) %>%
    `colnames<-`(c("data_set",
            "max_lag",
            "max_seasonal_lag",
            "filtering",
            "differencing",
            "seasonality",
            "bw_parameterization"))
 
dengue_peak_week_results <- rbind.fill(
    c(
        list(
            readRDS(file.path(prediction_save_path,
                        paste0("peak-week-sarima-", data_set, ".rds"))) %>%
                mutate(model = "SARIMA")
        ),
        lapply(seq_len(nrow(case_definitions)), function(case_row_ind) {
                max_lag <- case_definitions$max_lag[case_row_ind]
                max_seasonal_lag <- case_definitions$max_seasonal_lag[case_row_ind]
                filtering <- case_definitions$filtering[case_row_ind]
                differencing <- case_definitions$differencing[case_row_ind]
                seasonality <- case_definitions$seasonality[case_row_ind]
                bw_parameterization <- case_definitions$bw_parameterization[case_row_ind]
                
                case_descriptor <- paste0(
                    data_set,
                    "-max_lag_", max_lag,
                    "-max_seasonal_lag_", max_seasonal_lag,
                    "-filtering_", filtering,
                    "-differencing_", differencing,
                    "-seasonality_", seasonality,
                    "-bw_parameterization_", bw_parameterization
                )
                
                readRDS(file.path(prediction_save_path,
                            paste0("peak-week-", case_descriptor, ".rds"))) %>%
                    mutate(model = "KCDE",
                        max_lag = max_lag,
                        max_seasonal_lag = max_seasonal_lag,
                        filtering = filtering,
                        differencing = differencing,
                        seasonality = seasonality,
                        bw_parameterization = bw_parameterization)
            })
    )
)

dengue_peak_week_results$full_model_descriptor <- paste0(dengue_peak_week_results$model,
    "-seasonal_lag_", dengue_peak_week_results$max_seasonal_lag,
#    "-filtering_", dengue_prediction_results$filtering,
    "-differencing_", dengue_peak_week_results$differencing,
    "-periodic_", dengue_peak_week_results$seasonality,
    "-bw_", dengue_peak_week_results$bw_parameterization)

dengue_peak_week_results$reduced_model_descriptor <- "Null KCDE Model"
dengue_peak_week_results$reduced_model_descriptor[
    as.logical(dengue_peak_week_results$seasonality) & !(dengue_peak_week_results$bw_parameterization == "full")] <-
    "Periodic Kernel"
dengue_peak_week_results$reduced_model_descriptor[
    !as.logical(dengue_peak_week_results$seasonality) & (dengue_peak_week_results$bw_parameterization == "full")] <-
    "Full Bandwidth"
dengue_peak_week_results$reduced_model_descriptor[
    as.logical(dengue_peak_week_results$seasonality) & (dengue_peak_week_results$bw_parameterization == "full")] <-
    "Periodic Kernel,\nFull Bandwidth"
dengue_peak_week_results$reduced_model_descriptor[
    dengue_peak_week_results$model == "SARIMA"] <-
    "SARIMA"

num_analysis_time_season_values <- length(unique(dengue_peak_week_results$analysis_time_season))
num_analysis_time_season_week_values <- length(unique(dengue_peak_week_results$analysis_time_season_week))
dengue_peak_week_results <- rbind.fill(dengue_peak_week_results,
    data.frame(
        full_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        reduced_model_descriptor = rep("Equal Bin Probabilities", num_analysis_time_season_week_values * num_analysis_time_season_values),
        analysis_time_season = rep(unique(dengue_peak_week_results$analysis_time_season), each = num_analysis_time_season_week_values),
        analysis_time_season_week = rep(unique(dengue_peak_week_results$analysis_time_season_week), times = num_analysis_time_season_values),
        peak_week_log_score = rep(log(1/52), num_analysis_time_season_week_values * num_analysis_time_season_values),
        peak_height_log_score = rep(log(1/27), num_analysis_time_season_week_values * num_analysis_time_season_values)
    ))


dengue_peak_week_results$reduced_model_descriptor <-
    factor(dengue_peak_week_results$reduced_model_descriptor,
        levels = c("SARIMA", "Null KCDE Model", "Full Bandwidth", "Periodic Kernel",
            "Periodic Kernel,\nFull Bandwidth",
            "Equal Bin Probabilities"
        ))

#geom_hline(yintercept = log(1/31), colour = "grey", linetype = 2)

dengue_peak_week_times <- data.frame(
    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
    peak_week = sapply(unique(dengue_peak_week_results$analysis_time_season),
        function(season_val) {
            max_incidence_in_season <-
                max(dengue_sj$total_cases[dengue_sj$season == season_val])
            return(dengue_sj$season_week[dengue_sj$season == season_val &
                        dengue_sj$total_cases == max_incidence_in_season])
        })
)

dengue_peak_week_heights <- data.frame(
    analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
    peak_height = sapply(unique(dengue_peak_week_results$analysis_time_season),
        function(season_val) {
            return(max(dengue_sj$total_cases[dengue_sj$season == season_val]))
        })
)

dengue_peak_week_results$peak_week_log_score[dengue_peak_week_results$peak_week_log_score < -50] <- -50
dengue_peak_week_results$peak_height_log_score[dengue_peak_week_results$peak_height_log_score < -50] <- -50
dengue_peak_week_results$peak_week_log_score[dengue_peak_week_results$peak_week_log_score == -50] <- NA
dengue_peak_week_results$peak_height_log_score[dengue_peak_week_results$peak_height_log_score == -50] <- NA
@

%, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")
\begin{figure}
\caption{Log scores for predictions of peak week timing by predictive
model and analysis time.  The vertical gray line is placed at the peak week for
each season.}
\label{fig:CombinedPeakWeekTimingPredictionLogScores}
<<DenguePeakWeekTimingLogScoreByAnalysisTime, echo = FALSE, fig.keep = "last", fig.height = 8.5>>=
## Add season and season week columns to data so that we can get from
## analysis_time_season and analysis_time_season_week to analysis_time
#dengue_sj$season <- ifelse(
#    dengue_sj$week <= 30,
#    paste0(dengue_sj$year - 1, "/", dengue_sj$year),
#    paste0(dengue_sj$year, "/", dengue_sj$year + 1)
#)
#
## Season week column: week number within season
#dengue_sj$season_week <- sapply(seq_len(nrow(dengue_sj)), function(row_ind) {
#    sum(dengue_sj$season == dengue_sj$season[row_ind] & dengue_sj$time_index <= dengue_sj$time_index[row_ind])
#})
#
#
#dengue_peak_week_results$analysis_time <- dengue_peak_week_results$analysis_time_season_week

#dengue_peak_week_results_for_plot

models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "Equal Bin Probabilities")
reduced_models_used <- c(
    "SARIMA",
    "Null KCDE Model",
    "Full Bandwidth",
    "Periodic Kernel",
    "Periodic Kernel,\nFull Bandwidth",
    "Equal Bin Probabilities"
)

p <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used, ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
    facet_wrap( ~ analysis_time_season, ncol = 1) +
    xlab("Season Week at Analysis Time") +
    ylab("Log Score") +
    theme_bw()

suppressWarnings(print(p))
grid_list <- capture.output(grid.ls())
legend_grob <- grid.get(str_trim(grid_list[grep("guide-box", grid_list)]))

p_d_1 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2009/2010", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2009/2010", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_2 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2010/2011", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2010/2011", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_3 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2011/2012", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2011/2012", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_4 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2012/2013", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2012/2013", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("Season Week at Analysis Time") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")



p_i_1 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2011/2012", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2011/2012", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_i_2 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2012/2013", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2012/2013", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_i_3 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2013/2014", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2013/2014", ]) +
    ylim(c(-10, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("Season Week at Analysis Time") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

grid.newpage()
#grid.layout(nrow = 2, ncol = 2, heights = unit(rep(1, 2), c("null", "lines")))
pushViewport(viewport(layout = grid.layout(nrow = 4, ncol = 3,
            heights = unit(rep(1, 4), c("null", "null", "null", "null")),
            widths = unit(c(1, 1, 1), c("lines", "null", "null")))))
grid.text("Log Score",
    gp = gpar(fontsize = 12),
    rot = 90,
    vp = viewport(layout.pos.row = 1:4, layout.pos.col = 1))
        
pushViewport(viewport(layout.pos.row = 4, layout.pos.col = 3))
grid.draw(legend_grob)
upViewport()
suppressWarnings(print(p_d_1, vp = viewport(layout.pos.row = 1, layout.pos.col = 2)))
suppressWarnings(print(p_d_2, vp = viewport(layout.pos.row = 2, layout.pos.col = 2)))
suppressWarnings(print(p_d_3, vp = viewport(layout.pos.row = 3, layout.pos.col = 2)))
suppressWarnings(print(p_d_4, vp = viewport(layout.pos.row = 4, layout.pos.col = 2)))
suppressWarnings(print(p_i_1, vp = viewport(layout.pos.row = 1, layout.pos.col = 3)))
suppressWarnings(print(p_i_2, vp = viewport(layout.pos.row = 2, layout.pos.col = 3)))
suppressWarnings(print(p_i_3, vp = viewport(layout.pos.row = 3, layout.pos.col = 3)))

@
\end{figure}


Figure~\ref{fig:CombinedPeakWeekIncidencePredictionLogScores} displays log
scores for predictions of incidence in the peak week.  Here there is a slightly
clearer trend in favor of the SARIMA model.  Of the seven seasons in our test sets, SARIMA
consistently outperformed KCDE in four seasons.  The two methods did about as
well as each other in two seasons, and KCDE outperformed SARIMA in the remaining
season.  We believe that the relatively low performance of KCDE in the
application to predicting incidence in the peak week for Dengue can be explained
by the width of the predictive intervals obtained from that method.  Recall from
Figure~\ref{fig:DengueRibbonsPredictions} that the predictive intervals for
incidence in individual weeks were much wider for KCDE than they were for
SARIMA.  This was advantageous for log scores of predictive distributions for
incidence in individual weeks, as the predictive intervals were much more
likely to cover a wide range of possible values for incidence.  However, the
width of the predictive distributions is disadvantageous for
predicting incidence in the peak week: because the intervals for incidence in
individual weeks are very wide, the predicted peak incidence is often much
larger than the true realized peak week.

The large width of the predictive intervals for incidence is determined by
large estimated bandwidth parameters.  We see two possible underlying
reasons for the large bandwidth estimates.  First, our estimation procedure for
KCDE optimized the cross-validated log score of predictions for incidence at
individual weeks.  As we have mentioned, the log score has been criticised as
being sensitive to outliers.
In the Dengue data, our training data contained two outlying years with incidence that was in
the range of 2 to 10 times as large as the incidence in other years. 
The presence of these years in the training data could have led to large
bandwidth estimates which would be beneficial for the log scores of predictive
distributions for incidence in individual weeks, but would lead
to too-high predictions of incidence in the peak week.  Similar large outlying years were not present in the Influenza data,
which would explain why this same phenomenon may not have occurred in that
application.  Another possibility is that the large bandwidth parameter
estimates are a result of bad starting values in inference for KCDE in the
application to Dengue.

\lboxit{Amend above discussion if parameter estimates improve with revised
starting points}

In the application to predicting peak incidence for Influenza, KCDE slightly
underperforms relative to SARIMA in the 2011/2012 season when incidence was
relatively low, but outperforms SARIMA by a wide margin in the 2012/2013 season
when incidence was fairly high and does about as well as SARIMA in the 2013/2014
season when incidence was in between.  Notably, KCDE never did much worse than
a very naive model assigning equal probability to all incidence bins, but in
the early part of the 2012/2013 season SARIMA did much worse than that naive
model.

%, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")
\begin{figure}
\caption{Log scores for predictions of peak week incidence by predictive
model and analysis time.  The vertical gray line is placed at the peak week for
each season.}
\label{fig:CombinedPeakWeekIncidencePredictionLogScores}
<<CombinedPeakWeekHeightLogScoreByAnalysisTime, echo = FALSE, fig.keep = "last", fig.height = 8.5>>=
## Add season and season week columns to data so that we can get from
## analysis_time_season and analysis_time_season_week to analysis_time
#dengue_sj$season <- ifelse(
#    dengue_sj$week <= 30,
#    paste0(dengue_sj$year - 1, "/", dengue_sj$year),
#    paste0(dengue_sj$year, "/", dengue_sj$year + 1)
#)
#
## Season week column: week number within season
#dengue_sj$season_week <- sapply(seq_len(nrow(dengue_sj)), function(row_ind) {
#    sum(dengue_sj$season == dengue_sj$season[row_ind] & dengue_sj$time_index <= dengue_sj$time_index[row_ind])
#})
#
#
#dengue_peak_week_results$analysis_time <- dengue_peak_week_results$analysis_time_season_week

#dengue_peak_week_results_for_plot

models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
    "Equal Bin Probabilities")
reduced_models_used <- c(
    "SARIMA",
    "Null KCDE Model",
    "Full Bandwidth",
    "Periodic Kernel",
    "Periodic Kernel,\nFull Bandwidth",
    "Equal Bin Probabilities"
)

p <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used, ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
    facet_wrap( ~ analysis_time_season, ncol = 1) +
    xlab("Season Week at Analysis Time") +
    ylab("Log Score") +
    theme_bw()

suppressWarnings(print(p))
grid_list <- capture.output(grid.ls())
legend_grob <- grid.get(str_trim(grid_list[grep("guide-box", grid_list)]))

p_d_1 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2009/2010", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2009/2010", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_2 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2010/2011", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2010/2011", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_3 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2011/2012", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2011/2012", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_d_4 <- ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used &
                dengue_peak_week_results$analysis_time_season == "2012/2013", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = dengue_peak_week_times[dengue_peak_week_times$analysis_time_season == "2012/2013", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Dengue Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("Season Week at Analysis Time") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")



p_i_1 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2011/2012", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2011/2012", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_i_2 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2012/2013", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2012/2013", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

p_i_3 <- ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used &
                ili_peak_week_results$analysis_time_season == "2013/2014", ]) +
    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
#    geom_point(aes(x = analysis_time_season_week, y = peak_week_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
    geom_vline(aes(xintercept = peak_week), colour = "#999999", linetype = 1, data = ili_peak_week_times[ili_peak_week_times$analysis_time_season == "2013/2014", ]) +
    ylim(c(-6, 0)) +
    facet_wrap( ~ analysis_time_season, ncol = 1,
        labeller = as_labeller(function(labels, ...) {
                return(paste0("Influenza Data, ", labels))
            })
    ) +
#    xlab("Season Week at Analysis Time") +
#    ylab("Log Score") +
    xlab("Season Week at Analysis Time") +
    ylab("") +
    theme_bw() +
    theme(legend.position = "none")

grid.newpage()
#grid.layout(nrow = 2, ncol = 2, heights = unit(rep(1, 2), c("null", "lines")))
pushViewport(viewport(layout = grid.layout(nrow = 4, ncol = 3,
            heights = unit(rep(1, 4), c("null", "null", "null", "null")),
            widths = unit(c(1, 1, 1), c("lines", "null", "null")))))
grid.text("Log Score",
    gp = gpar(fontsize = 12),
    rot = 90,
    vp = viewport(layout.pos.row = 1:4, layout.pos.col = 1))
        
pushViewport(viewport(layout.pos.row = 4, layout.pos.col = 3))
grid.draw(legend_grob)
upViewport()
suppressWarnings(print(p_d_1, vp = viewport(layout.pos.row = 1, layout.pos.col = 2)))
suppressWarnings(print(p_d_2, vp = viewport(layout.pos.row = 2, layout.pos.col = 2)))
suppressWarnings(print(p_d_3, vp = viewport(layout.pos.row = 3, layout.pos.col = 2)))
suppressWarnings(print(p_d_4, vp = viewport(layout.pos.row = 4, layout.pos.col = 2)))
suppressWarnings(print(p_i_1, vp = viewport(layout.pos.row = 1, layout.pos.col = 3)))
suppressWarnings(print(p_i_2, vp = viewport(layout.pos.row = 2, layout.pos.col = 3)))
suppressWarnings(print(p_i_3, vp = viewport(layout.pos.row = 3, layout.pos.col = 3)))

@
\end{figure}


%\begin{figure}
%\caption{Log scores for predictions of incidence in the peak week for Dengue by
%predictive model and analysis time.  The vertical gray line is placed at the peak week for
%each season.}
%\label{fig:DenguePeakWeekIncidencePredictionLogScores}
%<<DenguePeakWeekIncidenceLogScoreByAnalysisTime, echo = FALSE, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")>>=
%## Add season and season week columns to data so that we can get from
%## analysis_time_season and analysis_time_season_week to analysis_time
%#dengue_sj$season <- ifelse(
%#    dengue_sj$week <= 30,
%#    paste0(dengue_sj$year - 1, "/", dengue_sj$year),
%#    paste0(dengue_sj$year, "/", dengue_sj$year + 1)
%#)
%#
%## Season week column: week number within season
%#dengue_sj$season_week <- sapply(seq_len(nrow(dengue_sj)), function(row_ind) {
%#    sum(dengue_sj$season == dengue_sj$season[row_ind] & dengue_sj$time_index <= dengue_sj$time_index[row_ind])
%#})
%#
%#
%#dengue_peak_week_results$analysis_time <- dengue_peak_week_results$analysis_time_season_week
%
%#dengue_peak_week_results_for_plot
% 
%models_used <- c(
%    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "Equal Bin Probabilities")
%reduced_models_used <- c(
%    "SARIMA",
%    "Null KCDE Model",
%    "Full Bandwidth",
%    "Periodic Kernel",
%    "Periodic Kernel,\nFull Bandwidth",
%    "Equal Bin Probabilities")
%
%dengue_peak_week_results$peak_height_log_score[dengue_peak_week_results$peak_height_log_score == -50] <- NA
%ggplot(dengue_peak_week_results[dengue_peak_week_results$full_model_descriptor %in% models_used, ]) +
%    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
%#    geom_point(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
%    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
%    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
%    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
%    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
%    facet_wrap( ~ analysis_time_season, ncol = 1) +
%#    geom_raster(aes(x = analysis_time_season_week, y = log_score),
%#        data = dengue_peak_week_results) +
%    xlab("Season Week at Analysis Time") +
%    ylab("Log Score") +
%    theme_bw()
%@
%\end{figure}
%
%\begin{figure}
%\caption{Log scores for predictions of incidence in the peak week by predictive
%model and analysis time.  The vertical gray line is placed at the peak week for
%each season.}
%\label{fig:FluPeakWeekIncidencePredictionLogScores}
%<<FluPeakWeekIncidenceLogScoreByAnalysisTime, echo = FALSE>>=
%## Add season and season week columns to data so that we can get from
%## analysis_time_season and analysis_time_season_week to analysis_time
%#ili_national$season <- ifelse(
%#    ili_national$week <= 30,
%#    paste0(ili_national$year - 1, "/", ili_national$year),
%#    paste0(ili_national$year, "/", ili_national$year + 1)
%#)
%#
%## Season week column: week number within season
%#ili_national$season_week <- sapply(seq_len(nrow(ili_national)), function(row_ind) {
%#    sum(ili_national$season == ili_national$season[row_ind] & ili_national$time_index <= ili_national$time_index[row_ind])
%#})
%#
%#
%#ili_peak_week_results$analysis_time <- ili_peak_week_results$analysis_time_season_week
%
%#ili_peak_week_results_for_plot
%
%models_used <- c(
%    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
%    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
%    "Equal Bin Probabilities")
%reduced_models_used <- c(
%    "SARIMA",
%    "Null KCDE Model",
%    "Full Bandwidth",
%    "Periodic Kernel",
%    "Periodic Kernel,\nFull Bandwidth",
%    "Equal Bin Probabilities")
%
%ggplot(ili_peak_week_results[ili_peak_week_results$full_model_descriptor %in% models_used, ]) +
%    geom_line(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, linetype = reduced_model_descriptor)) +
%#    geom_point(aes(x = analysis_time_season_week, y = peak_height_log_score, colour = reduced_model_descriptor, shape = reduced_model_descriptor)) +
%    scale_colour_manual("Model", breaks = reduced_models_used, values = c("#E69F00", "#56B4E9", "#009E73", "#D55E00", "#0072B2", "#999999")) +
%    scale_linetype_manual("Model", breaks = reduced_models_used, values = c(1:5, 1)) +
%    scale_shape_manual("Model", breaks = reduced_models_used, values = c(0:4, 45)) +
%    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = ili_peak_week_times) +
%    facet_wrap( ~ analysis_time_season, ncol = 1) +
%#    geom_raster(aes(x = analysis_time_season_week, y = log_score),
%#        data = ili_peak_week_results) +
%    xlab("Season Week at Analysis Time") +
%    ylab("Log Score") +
%    theme_bw()
%@
%\end{figure}


<<FluObtainPeakWeekTimingPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
ili_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 13, by = 0.5),
    upper = c(seq(from = 0.5, to = 13, by = 0.5), Inf))

for(bin_num in seq(from = 9, to = 41)) {
    ili_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_timing_pred_dist_by_analysis_time <- ili_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq(from = 9, to = 41)))
peak_timing_pred_dist_by_analysis_time$bin <-
    as.integer(substr(peak_timing_pred_dist_by_analysis_time$bin, 14, 15))


#junkjunkjunk <-    peak_timing_pred_dist_by_analysis_time[
#        peak_timing_pred_dist_by_analysis_time$full_model_descriptor == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
#        peak_timing_pred_dist_by_analysis_time$bin ==
#            peak_week_times$peak_week[
#                sapply(peak_timing_pred_dist_by_analysis_time$analysis_time_season,
#                    function(season_val) {
#                        which(peak_week_times$analysis_time_season == season_val)
#                    })
#        ],
#        c("est_prob", "analysis_time_season", "analysis_time_season_week")
#    ] %>%
#    mutate(log_score = log(est_prob))
#
#junkjunkjunk <- 
#    junkjunkjunk[
#        order(junkjunkjunk$analysis_time_season,
#            junkjunkjunk$analysis_time_season_week), ]
#
#junkjunkjunkjunk <- ili_peak_week_results[
#    ili_peak_week_results$full_model_descriptor == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
#    c("peak_week_log_score", "analysis_time_season", "analysis_time_season_week")
#]
#
#junkjunkjunkjunk <- 
#    junkjunkjunkjunk[
#        order(junkjunkjunkjunk$analysis_time_season,
#            junkjunkjunkjunk$analysis_time_season_week), ]
#
#tapply(peak_timing_pred_dist_by_analysis_time$est_prob,
#    peak_timing_pred_dist_by_analysis_time[,
#        c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week")],
#    sum)
 
peak_timing_and_height_pred_dist_means_by_analysis_time <- 
    ili_peak_week_results %>%
    select(full_model_descriptor,
        analysis_time_season,
        analysis_time_season_week,
        starts_with("est_prob_bin_")) %>%
    mutate(
        mean_peak_week = apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            mean),
        median_peak_week = apply(ili_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            median),
        mean_peak_height = apply(ili_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            mean),
        median_peak_height = apply(ili_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            median)
    )

#peak_timing_pred_dist_by_analysis_time <- ili_peak_week_results %>%
#    mutate(count_)
#    select_(c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week"))
#
#
#
#peak_timing_pred_dist_by_analysis_time <-
#    as.data.frame(expand.grid(
#            model = c(
#                "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
#                "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"),
#            analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
#            analysis_time_season_week = seq(from = 10, to = 40),
#            bin_number = seq(from = 10, to = 40),
##        incidence_bin = seq_len(nrow(ili_incidence_bins)),
#            stringsAsFactors = FALSE
#        ))
#peak_timing_pred_dist_by_analysis_time$est_bin_prob <- sapply(
#    seq_len(nrow(peak_timing_pred_dist_by_analysis_time)),
#    function(row_ind) {
#        sum(ili_peak_week_results[
#                    ili_peak_week_results$full_model_descriptor == peak_timing_pred_dist_by_analysis_time$model[row_ind] &
#                        ili_peak_week_results$analysis_time_season == peak_timing_pred_dist_by_analysis_time$analysis_time_season[row_ind] &
#                        ili_peak_week_results$analysis_time_season_week == peak_timing_pred_dist_by_analysis_time$analysis_time_season_week[row_ind],
#                    paste0("peak_week_", seq_len(10000))] ==
#                peak_timing_pred_dist_by_analysis_time$bin_number[row_ind]) / 10000
#    })
#
#
#
@

\begin{figure}
\caption{Predictive distributions for predictions of peak week timing.  The
horizontal and vertical dashed lines are at the observed peak week for the
season.}
\label{fig:FluPeakWeekTimingPredictiveDistributions}
<<FluPlotPeakWeekTimingPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")

#peak_timing_pred_dist_by_analysis_time$est_prob[
#    peak_timing_pred_dist_by_analysis_time$est_prob == 0] <- 10^{-20}
#peak_timing_pred_dist_by_analysis_time$est_prob[
#    peak_timing_pred_dist_by_analysis_time$est_prob == 10^{-20}] <- 0

#min(
#    peak_timing_pred_dist_by_analysis_time$est_prob[
#        peak_timing_pred_dist_by_analysis_time$est_prob != 0 &
#            peak_timing_pred_dist_by_analysis_time$full_model_descriptor %in% models_used]
#)

ggplot() +
    geom_raster(aes(x = analysis_time_season_week, y = bin, fill = est_prob),
        data = peak_timing_pred_dist_by_analysis_time[peak_timing_pred_dist_by_analysis_time$full_model_descriptor %in% models_used, ]) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = ili_peak_week_times) +
    geom_hline(aes(yintercept = peak_week), colour = "red", linetype = 2, data = ili_peak_week_times) +
    geom_point(aes(x = analysis_time_season_week, y = median_peak_week),
        colour = "red",
        data = peak_timing_and_height_pred_dist_means_by_analysis_time[peak_timing_and_height_pred_dist_means_by_analysis_time$full_model_descriptor %in% models_used, ]) +
    scale_fill_gradientn("Predictive\nDistribution\nProbability",
        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
#        labels = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    facet_grid(analysis_time_season ~ full_model_descriptor,
        labeller = as_labeller(function(labels, ...) {
            labels[labels == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"] <- "SARIMA"
            labels[labels == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"] <- "KCDE"
            return(labels)
        })) +
    xlab("Season Week at Analysis Time") +
    ylab("Season Week at Peak Incidence") +
    theme_bw()
@
\end{figure}




<<FluObtainPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE, cache = TRUE>>=
ili_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 13, by = 0.5),
    upper = c(seq(from = 0.5, to = 13, by = 0.5), Inf),
    center = seq(from = 0.25, to = 13.25, by = 0.5))
#ili_peak_week_results <-  
for(bin_num in seq_len(nrow(ili_incidence_bins))) {
    ili_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(ili_peak_week_results[, paste0("peak_height_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_height_pred_dist_by_analysis_time <- ili_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq_len(nrow(ili_incidence_bins))))
peak_height_pred_dist_by_analysis_time$bin <-
    as.integer(substr(peak_height_pred_dist_by_analysis_time$bin, 14, 15))
peak_height_pred_dist_by_analysis_time$bin_center <-
    ili_incidence_bins$center[peak_height_pred_dist_by_analysis_time$bin]

junk <- "junk"
#peak_timing_pred_dist_by_analysis_time <- ili_peak_week_results %>%
#    mutate(count_)
#    select_(c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week"))
#
#
#
#peak_timing_pred_dist_by_analysis_time <-
#    as.data.frame(expand.grid(
#            model = c(
#                "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
#                "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"),
#            analysis_time_season = unique(ili_peak_week_results$analysis_time_season),
#            analysis_time_season_week = seq(from = 10, to = 40),
#            bin_number = seq(from = 10, to = 40),
##        incidence_bin = seq_len(nrow(ili_incidence_bins)),
#            stringsAsFactors = FALSE
#        ))
#peak_timing_pred_dist_by_analysis_time$est_bin_prob <- sapply(
#    seq_len(nrow(peak_timing_pred_dist_by_analysis_time)),
#    function(row_ind) {
#        sum(ili_peak_week_results[
#                    ili_peak_week_results$full_model_descriptor == peak_timing_pred_dist_by_analysis_time$model[row_ind] &
#                        ili_peak_week_results$analysis_time_season == peak_timing_pred_dist_by_analysis_time$analysis_time_season[row_ind] &
#                        ili_peak_week_results$analysis_time_season_week == peak_timing_pred_dist_by_analysis_time$analysis_time_season_week[row_ind],
#                    paste0("peak_week_", seq_len(10000))] ==
#                peak_timing_pred_dist_by_analysis_time$bin_number[row_ind]) / 10000
#    })
#
#
#
@

\begin{figure}
\caption{Predictive distributions for predictions of peak week incidence.  The
horizontal dashed line is at the observed peak incidence for the season.  The
vertical dashed line is at the observed peak week for the season.}
\label{fig:FluPeakWeekHeightPredictiveDistributions}
<<FluPlotPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
junk <- "junk2"

ggplot() +
    geom_raster(aes(x = analysis_time_season_week, y = bin_center, fill = est_prob),
        data = peak_height_pred_dist_by_analysis_time[peak_height_pred_dist_by_analysis_time$full_model_descriptor %in% models_used, ]) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = ili_peak_week_times) +
    geom_hline(aes(yintercept = peak_height), colour = "red", linetype = 2, data = ili_peak_week_heights) +
    geom_point(aes(x = analysis_time_season_week, y = median_peak_height),
        colour = "red",
        data = peak_timing_and_height_pred_dist_means_by_analysis_time[peak_timing_and_height_pred_dist_means_by_analysis_time$full_model_descriptor %in% models_used, ]) +
    scale_fill_gradientn("Predictive\nDistribution\nProbability",
        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    facet_grid(analysis_time_season ~ full_model_descriptor,
        labeller = as_labeller(function(labels, ...) {
                labels[labels == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"] <- "SARIMA"
                labels[labels == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"] <- "KCDE"
                return(labels)
            })) +
    ylab("Peak Incidence") +
    xlab("Season Week at Analysis Time") +
    theme_bw()
@
\end{figure}

















% , dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")
<<DengueObtainPeakWeekTimingPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
dengue_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 500, by = 50),
    upper = c(seq(from = 50, to = 500, by = 50), Inf))
  
for(bin_num in seq(from = 1, to = 52)) {
    dengue_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(dengue_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_timing_pred_dist_by_analysis_time_dengue <- dengue_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq(from = 1, to = 52)))
peak_timing_pred_dist_by_analysis_time_dengue$bin <-
    as.integer(substr(peak_timing_pred_dist_by_analysis_time_dengue$bin, 14, 15))


#junkjunkjunk <-    peak_timing_pred_dist_by_analysis_time[
#        peak_timing_pred_dist_by_analysis_time$full_model_descriptor == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full" &
#        peak_timing_pred_dist_by_analysis_time$bin ==
#            peak_week_times$peak_week[
#                sapply(peak_timing_pred_dist_by_analysis_time$analysis_time_season,
#                    function(season_val) {
#                        which(peak_week_times$analysis_time_season == season_val)
#                    })
#        ],
#        c("est_prob", "analysis_time_season", "analysis_time_season_week")
#    ] %>%
#    mutate(log_score = log(est_prob))
#
#junkjunkjunk <- 
#    junkjunkjunk[
#        order(junkjunkjunk$analysis_time_season,
#            junkjunkjunk$analysis_time_season_week), ]
#
#junkjunkjunkjunk <- dengue_peak_week_results[
#    dengue_peak_week_results$full_model_descriptor == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full",
#    c("peak_week_log_score", "analysis_time_season", "analysis_time_season_week")
#]
#
#junkjunkjunkjunk <- 
#    junkjunkjunkjunk[
#        order(junkjunkjunkjunk$analysis_time_season,
#            junkjunkjunkjunk$analysis_time_season_week), ]
#
#tapply(peak_timing_pred_dist_by_analysis_time$est_prob,
#    peak_timing_pred_dist_by_analysis_time[,
#        c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week")],
#    sum)
 
peak_timing_and_height_pred_dist_means_by_analysis_time_dengue <- 
    dengue_peak_week_results %>%
    select(full_model_descriptor,
        analysis_time_season,
        analysis_time_season_week,
        starts_with("est_prob_bin_")) %>%
    mutate(
        mean_peak_week = apply(dengue_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            mean),
        median_peak_week = apply(dengue_peak_week_results[, paste0("peak_week_", seq_len(10000))],
            1,
            median),
        mean_peak_height = apply(dengue_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            mean),
        median_peak_height = apply(dengue_peak_week_results[, paste0("unbinned_peak_height_", seq_len(10000))],
            1,
            median)
    )

#peak_timing_pred_dist_by_analysis_time <- dengue_peak_week_results %>%
#    mutate(count_)
#    select_(c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week"))
#
#
#
#peak_timing_pred_dist_by_analysis_time <-
#    as.data.frame(expand.grid(
#            model = c(
#                "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
#                "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"),
#            analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
#            analysis_time_season_week = seq(from = 10, to = 40),
#            bin_number = seq(from = 10, to = 40),
##        incidence_bin = seq_len(nrow(dengue_incidence_bins)),
#            stringsAsFactors = FALSE
#        ))
#peak_timing_pred_dist_by_analysis_time$est_bin_prob <- sapply(
#    seq_len(nrow(peak_timing_pred_dist_by_analysis_time)),
#    function(row_ind) {
#        sum(dengue_peak_week_results[
#                    dengue_peak_week_results$full_model_descriptor == peak_timing_pred_dist_by_analysis_time$model[row_ind] &
#                        dengue_peak_week_results$analysis_time_season == peak_timing_pred_dist_by_analysis_time$analysis_time_season[row_ind] &
#                        dengue_peak_week_results$analysis_time_season_week == peak_timing_pred_dist_by_analysis_time$analysis_time_season_week[row_ind],
#                    paste0("peak_week_", seq_len(10000))] ==
#                peak_timing_pred_dist_by_analysis_time$bin_number[row_ind]) / 10000
#    })
#
#
#
@

%, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")
\begin{figure}
\caption{Predictive distributions for predictions of peak week timing for
Dengue.  The horizontal and vertical dashed lines are at the observed peak week
for the season.}
\label{fig:DenguePeakWeekTimingPredictiveDistributions}
<<DenguePlotPeakWeekTimingPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")

#peak_timing_pred_dist_by_analysis_time$est_prob[
#    peak_timing_pred_dist_by_analysis_time$est_prob == 0] <- 10^{-20}
#peak_timing_pred_dist_by_analysis_time$est_prob[
#    peak_timing_pred_dist_by_analysis_time$est_prob == 10^{-20}] <- 0

#min(
#    peak_timing_pred_dist_by_analysis_time$est_prob[
#        peak_timing_pred_dist_by_analysis_time$est_prob != 0 &
#            peak_timing_pred_dist_by_analysis_time$full_model_descriptor %in% models_used]
#)

ggplot() +
    geom_raster(aes(x = analysis_time_season_week, y = bin, fill = est_prob),
        data = peak_timing_pred_dist_by_analysis_time_dengue[peak_timing_pred_dist_by_analysis_time_dengue$full_model_descriptor %in% models_used, ]) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
    geom_hline(aes(yintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
    geom_point(aes(x = analysis_time_season_week, y = median_peak_week),
        colour = "red",
        data = peak_timing_and_height_pred_dist_means_by_analysis_time_dengue[peak_timing_and_height_pred_dist_means_by_analysis_time_dengue$full_model_descriptor %in% models_used, ]) +
    scale_fill_gradientn("Predictive\nDistribution\nProbability",
        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
#        labels = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    facet_grid(analysis_time_season ~ full_model_descriptor,
        labeller = as_labeller(function(labels, ...) {
            labels[labels == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"] <- "SARIMA"
            labels[labels == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"] <- "KCDE"
            return(labels)
        })) +
    xlab("Season Week at Analysis Time") +
    ylab("Season Week at Peak Incidence") +
    theme_bw()
@
\end{figure}



%, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots")
<<DengueObtainPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE>>=
dengue_incidence_bins <- data.frame(
    lower = seq(from = 0, to = 500, by = 50),
    upper = c(seq(from = 50, to = 500, by = 50), Inf),
    center = seq(from = 25, to = 525, by = 50))

for(bin_num in seq_len(nrow(dengue_incidence_bins))) {
    dengue_peak_week_results[, paste0("est_prob_bin_", bin_num)] <-
        apply(dengue_peak_week_results[, paste0("peak_height_", seq_len(10000))],
            1,
            function(x) {sum(x == bin_num) / length(x)})
}

peak_height_pred_dist_by_analysis_time_dengue <- dengue_peak_week_results %>%
    select(full_model_descriptor,
            analysis_time_season,
            analysis_time_season_week,
            starts_with("est_prob_bin_")) %>%
    gather_("bin", "est_prob", paste0("est_prob_bin_", seq_len(nrow(dengue_incidence_bins))))
peak_height_pred_dist_by_analysis_time_dengue$bin <-
    as.integer(substr(peak_height_pred_dist_by_analysis_time_dengue$bin, 14, 15))
peak_height_pred_dist_by_analysis_time_dengue$bin_center <-
    dengue_incidence_bins$center[peak_height_pred_dist_by_analysis_time_dengue$bin]


#peak_timing_pred_dist_by_analysis_time <- dengue_peak_week_results %>%
#    mutate(count_)
#    select_(c("full_model_descriptor", "analysis_time_season", "analysis_time_season_week"))
#
#
#
#peak_timing_pred_dist_by_analysis_time <-
#    as.data.frame(expand.grid(
#            model = c(
#                "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
#                "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"),
#            analysis_time_season = unique(dengue_peak_week_results$analysis_time_season),
#            analysis_time_season_week = seq(from = 10, to = 40),
#            bin_number = seq(from = 10, to = 40),
##        incidence_bin = seq_len(nrow(dengue_incidence_bins)),
#            stringsAsFactors = FALSE
#        ))
#peak_timing_pred_dist_by_analysis_time$est_bin_prob <- sapply(
#    seq_len(nrow(peak_timing_pred_dist_by_analysis_time)),
#    function(row_ind) {
#        sum(dengue_peak_week_results[
#                    dengue_peak_week_results$full_model_descriptor == peak_timing_pred_dist_by_analysis_time$model[row_ind] &
#                        dengue_peak_week_results$analysis_time_season == peak_timing_pred_dist_by_analysis_time$analysis_time_season[row_ind] &
#                        dengue_peak_week_results$analysis_time_season_week == peak_timing_pred_dist_by_analysis_time$analysis_time_season_week[row_ind],
#                    paste0("peak_week_", seq_len(10000))] ==
#                peak_timing_pred_dist_by_analysis_time$bin_number[row_ind]) / 10000
#    })
#
#
#
@

\begin{figure}
\caption{Predictive distributions for predictions of peak week incidence for
Dengue.  The horizontal dashed line is at the observed peak incidence for the
season.  The vertical dashed line is at the observed peak week for the season.}
\label{fig:DenguePeakWeekHeightPredictiveDistributions} %, dependson = c("DengueDataMergePeakWeekPredictionResults", "DengueDataPeakWeekPredictionBoxPlots", "DengueObtainPeakWeekHeightPredictiveDistributionsByAnalysisTime")
<<DenguePlotPeakWeekHeightPredictiveDistributionsByAnalysisTime, echo = FALSE, cache = FALSE>>=
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_FALSE-bw_full",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_diagonal",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
models_used <- c(
    "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA",
    "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full")
 
ggplot() +
    geom_raster(aes(x = analysis_time_season_week, y = bin_center, fill = est_prob),
        data = peak_height_pred_dist_by_analysis_time_dengue[peak_height_pred_dist_by_analysis_time_dengue$full_model_descriptor %in% models_used, ]) +
    geom_vline(aes(xintercept = peak_week), colour = "red", linetype = 2, data = dengue_peak_week_times) +
    geom_hline(aes(yintercept = peak_height), colour = "red", linetype = 2, data = dengue_peak_week_heights) +
    geom_point(aes(x = analysis_time_season_week, y = median_peak_height),
        colour = "red",
        data = peak_timing_and_height_pred_dist_means_by_analysis_time_dengue[peak_timing_and_height_pred_dist_means_by_analysis_time_dengue$full_model_descriptor %in% models_used, ]) +
    scale_fill_gradientn("Predictive\nDistribution\nProbability",
        colours = rev(c("#000000", "#111111", "#222222", "#333333", "#444444", "#555555", "#666666", "#777777", "#888888", "#999999", "#AAAAAA", "#BBBBBB", "#CCCCCC", "#DDDDDD", "#EEEEEE", "#FFFFFF")),
#        limits = c(10^{-10}, 1),
        trans = "log",
#        values = c(0, seq(from = exp(-10), to = 1, length = 15))) +
#        values = c(0, exp(seq(from = log(10^-3), to = log(1), length = 15)))
#        values = c(0, exp(seq(from = log(5 * 10^-4), to = log(1), length = 15)))
        breaks = c(0.0001, 0.001, 0.01, 0.1, 1),
        labels = c(expression(10^{-4}), expression(10^{-3}), expression(10^{-2}), expression(10^{-1}), "1   "),
        na.value = "white"
    ) +
    facet_grid(analysis_time_season ~ full_model_descriptor,
        labeller = as_labeller(function(labels, ...) {
                labels[labels == "SARIMA-seasonal_lag_NA-differencing_NA-periodic_NA-bw_NA"] <- "SARIMA"
                labels[labels == "KCDE-seasonal_lag_0-differencing_FALSE-periodic_TRUE-bw_full"] <- "KCDE"
                return(labels)
            })) +
    ylab("Peak Incidence") +
    xlab("Season Week at Analysis Time") +
    theme_bw()
@
\end{figure}


\lboxit{To do: Put figures 8 - 11 in supplemental materials, possibly pull out
1 to 4 sub-panels for further discussion?}





\section{Conclusions}

Prediction of infectious disease incidence at horizons of more than a few weeks
is a challenging task.  We have presented a non-parametric approach to doing
this based on KCDE and found that it is a viable method that yields improved
predictions relative to commonly employed methods in some applications.  In an application to
predicting Dengue fever, we saw that our approach offered consistent
and large performance gains relative to a SARIMA model for predicting incidence
in individual weeks.  For predicting influenza-like illness, our method did
about as well as SARIMA when predicting incidence in individual weeks.

For predicting the timing of the peak week in a season and incidence in that
peak week, our method performed reasonably well in the application to Influenza,
but struggled in the application to Dengue fever.  We believe that many of the
problems in the application to Dengue fever were related to parameter
estimation.  A major factor here is that we based parameter estimation for KCDE
on the log score of predictive distributions for incidence in individual weeks. 
Because of the sensitivity of log scores to outliers, this may have led to large
bandwidth estimates in the application to Dengue fever that are beneficial for
predicting incidence in individual weeks, but throw off predictions of peak
incidence.  We also observed some problems with convergence in the process of
parameter estimation process that may have contributed to these difficulties.

An advantage of our approach relative to the SARIMA model is that the predictive
distributions obtained from KCDE are generally not over-confident.  This shows
up most strongly in the predictive distributions for incidence in individual
weeks in the application to Dengue fever.  There, the SARIMA model concentrated
much too much of the mass of its predictive distribution in the wrong area. 
In that application, there were several cases where the predictive distributions
from KCDE were centered closer to the realized value than the distributions from
SARIMA -- but at least as importantly, the predictive distributions were much
wider so that they assigned more mass to the eventual realized outcome.  This is
a particularly important feature of a predictive distribution in the context of
reporting to public health decision makers.  We would much rather acknowledge
uncertainty than make over-confident predictions of incidence that are much
lower than the eventually realized outcome.
%These broader predictive
%distributions also showed up in the predictions of peak week timing in the
%application to Influenza.  There, 

There is a great deal of room for extensions and improvements to the methods we
have outlined in this article.  One major limitation of our work lies in the
selection of conditioning variables for the predictive model.  We have simply
used incidence at the two most recent time points, and possibly the periodic
function of time, as conditioning variables.  We considered using a stepwise
variable selection approach as in  (cite ***), but we found this to be too
computationally expensive to be practical.

Another possibility for addressing this problem would be to replace
variable selection with shrinkage.  Hall, Racine, and
Li\cite{hall2004crossvalidationKCDE} show that when cross-validation is used to
select the bandwidth parameters in KCDE using product kernels, the estimated
bandwidths corresponding to irrelevant conditioning variables tend to infinity
asymptotically as the sample size increases.  They discuss the fact that similar
results could be obtained for linear combinations of continuous variables if a
full bandwidth matrix were used.  A difficulty with relying on cross-validation
to eliminate irrelevant conditioning variables is that we may not have a large
enough sample size for this asymptotic argument to be relevant.  We conjecture
that by introducing an appropriate penalty on the elements bandwidth
matrix, we could include more (possibly
irrelevant) conditioning variables in the model without requiring
an dramatically larger sample size.  In particular, we suggest that a
penalty on the inverse of the bandwidth matrix encouraging it to have small
eigenvalues could be helpful.  A similar effect could be achieved in
a Bayesian framework by using Dirichlet process mixtures with informative priors
on the mixture component covariances.
%Our approach for obtaining kernels that can be
%used with mixed discrete and continuous variables opens up an opportunity to
%extend this analysis to that case; we have not pursued this mathematical
%analysis here.

Although it is technically possible, we also have not explored the idea of
including other predictive covariates such as weather data in the KCDE specification.  An
approach to variable selection or shrinkage would also enable further
exploration of using other predictive variables along these lines in the model.

Another aspect of our method that should be explored further is the use of log
score in estimation.  We used log scores in this work in order to match the use
of log scores in evaluating and comparing the performance of different models. 
The use of this statistic for model comparison was set by the contest
administrators.  The log score has the advantage of defining a proper scoring
rule, but it has the disadvantage of being sensitive to outlying values.  We
would encourage the infectious disease prediction community to continue
exploring appropriate metrics for predictive model quality in the context of
infectious disease prediction.  We would also encourage future users of KCDE to
consider alternative loss functions in estimation, such as variations on
integrated squared error that were used by **** cite cite cite.

We could also make some tweaks to our implementation of KCDE.  One
limitation of our current implementation is its sensitivity to edge
effects.  A possibility for addressing this would be to adopt locally linear
or polynomial mean functions.  Approaches along these lines have been explored
by Cite Hyndman, Bashtannyk, Grunwald - "Estimating and Visualizing Conditional Densities", maybe also Fan and Yim - "A crossvaildation
method for estimating conditional densities" and Fan et al. 1996 "Estimation of
conditional densities and sensitivity measures in nonlinear dynamical systems."

Our approach could also be extended by incorporating it in an ensemble
model.  This could be implemented using only variants on KCDE specifications,
for example with different explanatory variables or sets of prediction horizons
incorporated in each component model.  However, it would likely also be
beneficial to construct a predictive ensemble with multiple different types of
models.  For example, in our application to Influenza, we saw that the SARIMA
model captured some features of the data generating process, such as the
Christmas-week effect, that KCDE did not capture.  On the other hand, the KCDE
approach offered the advantage of more conservative predictive distributions
that captured realized values of incidence more often, and rarely performed
worse than a very naive baseline of equal bin probabilities; the same cannot be said for the SARIMA
model.  We believe that an appropriately constructed ensemble incorporating
predictions from both SARIMA and KCDE might perform better than either model on
its own, and might offer more consistent performance across a variety of data
sets.

\bibliographystyle{plainnat}
\bibliography{kde-bib}


\end{document}